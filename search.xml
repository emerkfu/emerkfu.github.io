<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ClickHouse Kafka 数据接入 —— 实时数据导入 Task 创建</title>
    <url>/2021/07/26/ClickHouseKafka%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%85%A501%E2%80%94%E2%80%94%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5Task%E5%88%9B%E5%BB%BA/</url>
    <content><![CDATA[<p>Kafka 接入 ClickHouse，数据实时进入 ClickHouse 表</p>
<p>这个功能和 Doris 和 Routine Load 功能类似，只是有些操作更复杂一些（Offset 操作）</p>
<p>相比于 Doris Routine Load，ClickHouse 的 Kafka 接入更灵活一些</p>
<a id="more"></a>
<hr>
<h4 id="创建-kafka-source"><a href="#创建-kafka-source" class="headerlink" title="创建 kafka_source"></a>创建 kafka_source</h4><p>kafka_source 表直接对接 Kafka Topic，是 Kafka 消息订阅引擎 </p>
<p>消费的消息会被自动追踪，因此每个消息在不同的消费组里<strong>只会记录一次</strong>，这句话非常重要</p>
<p>所以我们不能随便对 kafka_source 表进行查询操作，</p>
<p>因为每条消息在一个消费组内只记录一次，所以查询操作会导致下游数据的丢失</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_source (</span><br><span class="line">  ts UInt64,</span><br><span class="line">  <span class="keyword">level</span> <span class="keyword">String</span>,</span><br><span class="line">  message <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ENGINE</span> = Kafka</span><br><span class="line"><span class="keyword">SETTINGS</span> kafka_broker_list = <span class="string">'10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092'</span>,</span><br><span class="line">         kafka_topic_list = <span class="string">'ck_test'</span>,</span><br><span class="line">         kafka_group_name = <span class="string">'test01'</span>,</span><br><span class="line">         kafka_format = <span class="string">'JSONEachRow'</span>,</span><br><span class="line">         kafka_num_consumers = <span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<h4 id="创建-target-表"><a href="#创建-target-表" class="headerlink" title="创建 target 表"></a>创建 target 表</h4><p>target 表为 Kafka -&gt; ClickHouse 的目标表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE target</span><br><span class="line">(</span><br><span class="line">  ts DateTime,</span><br><span class="line">  level String,</span><br><span class="line">  message String</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; MergeTree()</span><br><span class="line">PARTITION BY toYYYYMM(ts)</span><br><span class="line">ORDER BY level;</span><br></pre></td></tr></table></figure>

<h4 id="创建物化视图"><a href="#创建物化视图" class="headerlink" title="创建物化视图"></a>创建物化视图</h4><p>source_target_mv 物化视图为 kafka_source 和 target 之间的连接器</p>
<p>控制数据从哪来、到哪去</p>
<p>还可以进行数据落地前的逻辑加工</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE MATERIALIZED VIEW source_target_mv TO target AS</span><br><span class="line">SELECT</span><br><span class="line">  toDateTime(ts) AS ts,</span><br><span class="line">  level,</span><br><span class="line">  message</span><br><span class="line">FROM kafka_source;</span><br></pre></td></tr></table></figure>

<h4 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h4><p>1.不要手动对 kafka_source 表进行任何查询操作！！</p>
<p>2.Kafka 数据接入时，会对 Json 中的 Key 字段进行赋值，</p>
<p>   因为 Value 为空时，Json 的 Key 会进行缺省，缺省的 Key 对应的字段落表时值为空。</p>
]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse Kafka 数据接入 —— Schema 修改流程</title>
    <url>/2021/07/27/ClickHouseKafka%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%85%A502%E2%80%94%E2%80%94Schema%E4%BF%AE%E6%94%B9%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>这个流程针对 Kafka 数据实时接入 ClickHouse 的情况下，</p>
<p>Schema 增加字段的同时，能够保证实时数据进入 ClickHouse 的 EOS。</p>
<a id="more"></a>
<hr>
<h4 id="DETACH-物化视图"><a href="#DETACH-物化视图" class="headerlink" title="DETACH 物化视图"></a>DETACH 物化视图</h4><p>停止 Kakfa 数据的消费</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">DETACH TABLE source_target_mv;</span><br></pre></td></tr></table></figure>

<h4 id="删除-kafka-source-表"><a href="#删除-kafka-source-表" class="headerlink" title="删除 kafka_source 表"></a>删除 kafka_source 表</h4><p>因为后续要 ATTACH 物化视图，所以先删掉 kafka_source</p>
<p>防止消费 kafka_source 的数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">table</span> kafka_source;</span><br></pre></td></tr></table></figure>

<h4 id="修改-target-表"><a href="#修改-target-表" class="headerlink" title="修改 target 表"></a>修改 target 表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 增加字段</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> target <span class="keyword">add</span> <span class="keyword">column</span> jg <span class="keyword">String</span>;</span><br></pre></td></tr></table></figure>

<h4 id="ATTACH-物化视图"><a href="#ATTACH-物化视图" class="headerlink" title="ATTACH 物化视图"></a>ATTACH 物化视图</h4><p>这里 ATTACH 物化视图看起来很奇怪</p>
<p>不直接删除物化视图，而是先 ATTACH 物化视图</p>
<p>因为在 DETACH 状态的物化视图是不能执行删除操作的</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ATTACH TABLE source_target_mv;</span><br></pre></td></tr></table></figure>

<h4 id="删除物化视图"><a href="#删除物化视图" class="headerlink" title="删除物化视图"></a>删除物化视图</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> source_target_mv;</span><br></pre></td></tr></table></figure>

<h4 id="重建-kafka-source"><a href="#重建-kafka-source" class="headerlink" title="重建 kafka_source"></a>重建 kafka_source</h4><p>在重建物化视图前先重建 kafka_source，因为创建物化视图会检查 kafka_source 是否存在</p>
<p>如果 kafka_source 不存在，物化视图创建会报错</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_source (</span><br><span class="line">  ts UInt64,</span><br><span class="line">  <span class="keyword">level</span> <span class="keyword">String</span>,</span><br><span class="line">  message <span class="keyword">String</span>,</span><br><span class="line">  jg <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ENGINE</span> = Kafka</span><br><span class="line"><span class="keyword">SETTINGS</span> kafka_broker_list = <span class="string">'10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092'</span>,</span><br><span class="line">         kafka_topic_list = <span class="string">'ck_test'</span>,</span><br><span class="line">         kafka_group_name = <span class="string">'test01'</span>,</span><br><span class="line">         kafka_format = <span class="string">'JSONEachRow'</span>,</span><br><span class="line">         kafka_num_consumers = <span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<h4 id="重建物化视图"><a href="#重建物化视图" class="headerlink" title="重建物化视图"></a>重建物化视图</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">MATERIALIZED</span> <span class="keyword">VIEW</span> source_target_mv <span class="keyword">TO</span> target <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  toDateTime(ts) <span class="keyword">AS</span> ts,</span><br><span class="line">  <span class="keyword">level</span>,</span><br><span class="line">  message,</span><br><span class="line">  jg</span><br><span class="line"><span class="keyword">FROM</span> kafka_source;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse Kafka 数据接入 —— offset 指定位置开始消费</title>
    <url>/2021/07/27/ClickHouseKafka%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%85%A503%E2%80%94%E2%80%94offset%E6%8C%87%E5%AE%9A%E4%BD%8D%E7%BD%AE%E5%BC%80%E5%A7%8B%E6%B6%88%E8%B4%B9/</url>
    <content><![CDATA[<p>这个流程针对 Kafka 数据实时接入 ClickHouse 的情况下，</p>
<p>某些场景需要从指定 Offset 位置开始消费，可以参考以下流程进行操作。</p>
<a id="more"></a>
<hr>
<h4 id="DETACH-物化视图"><a href="#DETACH-物化视图" class="headerlink" title="DETACH 物化视图"></a>DETACH 物化视图</h4><p>停止 Kakfa 数据的消费</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">DETACH TABLE source_target_mv;</span><br></pre></td></tr></table></figure>

<h4 id="删除-kafka-source-表"><a href="#删除-kafka-source-表" class="headerlink" title="删除 kafka_source 表"></a>删除 kafka_source 表</h4><p>kafka_source 一定要删除，否则执行重置 Kafka Offset 命令时会报被占用的错误，不能进行 Offset 重置</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">table</span> kafka_source;</span><br></pre></td></tr></table></figure>

<h4 id="重置-Kafka-Offset"><a href="#重置-Kafka-Offset" class="headerlink" title="重置 Kafka Offset"></a>重置 Kafka Offset</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kafka-consumer-groups --bootstrap-server 10.0.15.130:9092 --group test01 --topic ck_test:0,1 --reset-offsets --to-offset 5,5 --execute</span><br></pre></td></tr></table></figure>

<h4 id="重建-kafka-source-表"><a href="#重建-kafka-source-表" class="headerlink" title="重建 kafka_source 表"></a>重建 kafka_source 表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_source (</span><br><span class="line">  ts UInt64,</span><br><span class="line">  <span class="keyword">level</span> <span class="keyword">String</span>,</span><br><span class="line">  message <span class="keyword">String</span>,</span><br><span class="line">  jg <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ENGINE</span> = Kafka</span><br><span class="line"><span class="keyword">SETTINGS</span> kafka_broker_list = <span class="string">'10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092'</span>,</span><br><span class="line">         kafka_topic_list = <span class="string">'ck_test'</span>,</span><br><span class="line">         kafka_group_name = <span class="string">'test01'</span>,</span><br><span class="line">         kafka_format = <span class="string">'JSONEachRow'</span>,</span><br><span class="line">         kafka_num_consumers = <span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<h4 id="ATTACH-物化视图"><a href="#ATTACH-物化视图" class="headerlink" title="ATTACH 物化视图"></a>ATTACH 物化视图</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ATTACH TABLE source_target_mv;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse Kafka 数据接入 —— offset 指定位置开始消费 补充</title>
    <url>/2021/07/29/ClickHouseKafka%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%85%A504%E2%80%94%E2%80%94offset%E6%8C%87%E5%AE%9A%E4%BD%8D%E7%BD%AE%E5%BC%80%E5%A7%8B%E6%B6%88%E8%B4%B9%E8%A1%A5%E5%85%85/</url>
    <content><![CDATA[<p>这个流程针对 Kafka 数据实时接入 ClickHouse 的情况下，</p>
<p>某些场景需要从指定 Offset 位置开始消费，可能会出现修改了 Offset 后，Kafka 引擎不会继续消费的情况，</p>
<p>使用以下方式，解决了 Offset 修改后不能继续消费的问题。</p>
<a id="more"></a>
<hr>
<h4 id="Offset-修改后不能继续消费解决方法"><a href="#Offset-修改后不能继续消费解决方法" class="headerlink" title="Offset 修改后不能继续消费解决方法"></a>Offset 修改后不能继续消费解决方法</h4><h5 id="step-1"><a href="#step-1" class="headerlink" title="step 1"></a>step 1</h5><p>kafka_source 表更换表名重建</p>
<h5 id="step-2"><a href="#step-2" class="headerlink" title="step 2"></a>step 2</h5><p>物化视图变更视图名重建</p>
<p>其他操作流程不变，按照以下流程进行。</p>
<h4 id="DETACH-物化视图"><a href="#DETACH-物化视图" class="headerlink" title="DETACH 物化视图"></a>DETACH 物化视图</h4><p>停止 Kakfa 数据的消费</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">DETACH TABLE source_target_mv;</span><br></pre></td></tr></table></figure>

<h4 id="删除-kafka-source-表"><a href="#删除-kafka-source-表" class="headerlink" title="删除 kafka_source 表"></a>删除 kafka_source 表</h4><p>kafka_source 一定要删除，否则执行重置 Kafka Offset 命令时会报被占用的错误，不能进行 Offset 重置</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">table</span> kafka_source;</span><br></pre></td></tr></table></figure>

<h4 id="重置-Kafka-Offset"><a href="#重置-Kafka-Offset" class="headerlink" title="重置 Kafka Offset"></a>重置 Kafka Offset</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kafka-consumer-groups --bootstrap-server 10.0.15.130:9092 --group test01 --topic ck_test:0 --reset-offsets --to-offset 5 --execute</span><br></pre></td></tr></table></figure>

<h4 id="重建-kafka-source-表"><a href="#重建-kafka-source-表" class="headerlink" title="重建 kafka_source 表"></a>重建 kafka_source 表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_source (</span><br><span class="line">  ts UInt64,</span><br><span class="line">  <span class="keyword">level</span> <span class="keyword">String</span>,</span><br><span class="line">  message <span class="keyword">String</span>,</span><br><span class="line">  jg <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ENGINE</span> = Kafka</span><br><span class="line"><span class="keyword">SETTINGS</span> kafka_broker_list = <span class="string">'10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092'</span>,</span><br><span class="line">         kafka_topic_list = <span class="string">'ck_test'</span>,</span><br><span class="line">         kafka_group_name = <span class="string">'test01'</span>,</span><br><span class="line">         kafka_format = <span class="string">'JSONEachRow'</span>,</span><br><span class="line">         kafka_num_consumers = <span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<h4 id="ATTACH-物化视图"><a href="#ATTACH-物化视图" class="headerlink" title="ATTACH 物化视图"></a>ATTACH 物化视图</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ATTACH TABLE source_target_mv;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse UBT 包含细分维度的 windowFunnel 计算</title>
    <url>/2021/07/22/ClickHouseUBT%E5%8C%85%E5%90%AB%E7%BB%86%E5%88%86%E7%BB%B4%E5%BA%A6%E7%9A%84windowFunnel%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>Funnel 计算（包含细分维度）</p>
<p>查询条件:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 漏斗流程</span><br><span class="line">   浏览详情页 -&gt; 加入进货单或立即购买 -&gt; 提交订单 -&gt; 支付商品订单</span><br><span class="line">2. 对第一层 &quot;浏览详情页&quot; 进行了 &quot;是否注册用户&quot; &#x3D; &quot;是&quot; 的筛选</span><br><span class="line">3. 时间范围 2021-07-01 ~ 2021-07-09</span><br><span class="line">4. 转化周期 1 天</span><br><span class="line">5. 细分维度 &quot;一级分类名称&quot;</span><br></pre></td></tr></table></figure>

<a id="more"></a>
<hr>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span></span><br><span class="line">    t0 <span class="keyword">as</span> (</span><br><span class="line">            <span class="keyword">select</span> distinct_id, xwhat, toDateTime(toUInt64(<span class="keyword">round</span>(xwhen/<span class="number">1000</span>))) <span class="keyword">as</span> <span class="built_in">time</span>, first_category <span class="keyword">as</span> jg</span><br><span class="line">            <span class="keyword">from</span> test.event</span><br><span class="line">            <span class="keyword">where</span> toDate(<span class="built_in">time</span>) &gt;= <span class="string">'2021-07-01'</span> <span class="keyword">and</span> toDate(<span class="built_in">time</span>) &lt;= <span class="string">'2021-07-09'</span></span><br><span class="line">                <span class="keyword">and</span> xwhat = <span class="string">'view_detail'</span></span><br><span class="line">                <span class="keyword">and</span> is_login = <span class="string">'1'</span>),</span><br><span class="line">    t1 <span class="keyword">as</span> (</span><br><span class="line">        <span class="keyword">select</span> distinct_id, xwhat, <span class="built_in">time</span>, jg <span class="keyword">from</span></span><br><span class="line">            t0</span><br><span class="line">        <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">            (<span class="keyword">select</span> t2.distinct_id, t2.what <span class="keyword">as</span> xwhat, t2.time, t0.jg</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">            t0</span><br><span class="line">            <span class="keyword">INNER</span> <span class="keyword">JOIN</span></span><br><span class="line">            (<span class="keyword">select</span> distinct_id, <span class="string">'add_item_cart_or_click_buy_now'</span> what, toDateTime(toUInt64(<span class="keyword">round</span>(xwhen/<span class="number">1000</span>))) <span class="keyword">as</span> <span class="built_in">time</span></span><br><span class="line">            <span class="keyword">from</span> test.event</span><br><span class="line">            <span class="keyword">where</span> toDate(<span class="built_in">time</span>) &gt;= <span class="string">'2021-07-01'</span> <span class="keyword">and</span> toDate(<span class="built_in">time</span>) &lt;= <span class="string">'2021-07-10'</span> <span class="keyword">and</span> (xwhat = <span class="string">'add_item_cart'</span> <span class="keyword">or</span> xwhat = <span class="string">'click_buy_now'</span>)) t2</span><br><span class="line">            <span class="keyword">on</span> t0.distinct_id = t2.distinct_id)</span><br><span class="line">        <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">            (<span class="keyword">select</span> t2.distinct_id, t2.xwhat, t2.time, t0.jg</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">            t0</span><br><span class="line">            <span class="keyword">INNER</span> <span class="keyword">JOIN</span></span><br><span class="line">            (<span class="keyword">select</span> distinct_id, xwhat, toDateTime(toUInt64(<span class="keyword">round</span>(xwhen/<span class="number">1000</span>))) <span class="keyword">as</span> <span class="built_in">time</span></span><br><span class="line">            <span class="keyword">from</span> test.event</span><br><span class="line">            <span class="keyword">where</span> toDate(<span class="built_in">time</span>) &gt;= <span class="string">'2021-07-01'</span> <span class="keyword">and</span> toDate(<span class="built_in">time</span>) &lt;= <span class="string">'2021-07-10'</span> <span class="keyword">and</span> xwhat = <span class="string">'submit_item_order'</span>) t2</span><br><span class="line">            <span class="keyword">on</span> t0.distinct_id = t2.distinct_id)</span><br><span class="line">        <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">            (<span class="keyword">select</span> t2.distinct_id, t2.xwhat, t2.time, t0.jg</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">            t0</span><br><span class="line">            <span class="keyword">INNER</span> <span class="keyword">JOIN</span></span><br><span class="line">            (<span class="keyword">select</span> distinct_id, xwhat, toDateTime(toUInt64(<span class="keyword">round</span>(xwhen/<span class="number">1000</span>))) <span class="keyword">as</span> <span class="built_in">time</span></span><br><span class="line">            <span class="keyword">from</span> test.event</span><br><span class="line">            <span class="keyword">where</span> toDate(<span class="built_in">time</span>) &gt;= <span class="string">'2021-07-01'</span> <span class="keyword">and</span> toDate(<span class="built_in">time</span>) &lt;= <span class="string">'2021-07-10'</span> <span class="keyword">and</span> xwhat = <span class="string">'pay_item_order'</span>) t2</span><br><span class="line">            <span class="keyword">on</span> t0.distinct_id = t2.distinct_id)),</span><br><span class="line">    t2 <span class="keyword">as</span> (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">          jg,</span><br><span class="line">          distinct_id,</span><br><span class="line">          <span class="keyword">min</span>(<span class="built_in">time</span>) <span class="keyword">AS</span> <span class="built_in">TIME</span>,</span><br><span class="line">          windowFunnel(<span class="number">86400</span>)(</span><br><span class="line">            <span class="built_in">time</span>,</span><br><span class="line">            xwhat = <span class="string">'view_detail'</span>,</span><br><span class="line">            xwhat = <span class="string">'add_item_cart_or_click_buy_now'</span>,</span><br><span class="line">            xwhat = <span class="string">'submit_item_order'</span>,</span><br><span class="line">            xwhat = <span class="string">'pay_item_order'</span></span><br><span class="line">          ) <span class="keyword">AS</span> <span class="keyword">level</span></span><br><span class="line">        <span class="keyword">FROM</span> (</span><br><span class="line">          <span class="keyword">SELECT</span></span><br><span class="line">          distinct_id</span><br><span class="line">          , xwhat</span><br><span class="line">          , <span class="built_in">time</span></span><br><span class="line">          , jg</span><br><span class="line">          <span class="keyword">FROM</span> t1</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> distinct_id,jg)</span><br><span class="line">    ,t3 <span class="keyword">as</span> (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">            jg,</span><br><span class="line">            distinct_id,</span><br><span class="line">            <span class="built_in">TIME</span>,</span><br><span class="line">            <span class="keyword">level</span>,</span><br><span class="line">            row_number()<span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> distinct_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">level</span> <span class="keyword">desc</span>, <span class="built_in">TIME</span> <span class="keyword">asc</span>) rn</span><br><span class="line">        <span class="keyword">from</span> t2</span><br><span class="line">        <span class="keyword">SETTINGS</span> allow_experimental_window_functions = <span class="number">1</span>)</span><br><span class="line">    ,t4 <span class="keyword">as</span> (</span><br><span class="line">        <span class="keyword">select</span> </span><br><span class="line">            distinct_id,</span><br><span class="line">            jg,</span><br><span class="line">            arrayWithConstant(<span class="keyword">level</span>, <span class="number">1</span>) <span class="keyword">levels</span>,</span><br><span class="line">            arrayEnumerate( <span class="keyword">levels</span> ) b,</span><br><span class="line">            arrayJoin(arrayEnumerate( <span class="keyword">levels</span> )) level_index,</span><br><span class="line">            rn</span><br><span class="line">        <span class="keyword">from</span> t3</span><br><span class="line">        <span class="keyword">where</span> rn = <span class="number">1</span>)</span><br><span class="line"><span class="keyword">select</span> jg, level_index, cnt</span><br><span class="line"><span class="keyword">from</span>(<span class="keyword">select</span> jg, level_index, <span class="keyword">count</span>(<span class="number">1</span>) cnt</span><br><span class="line">    <span class="keyword">from</span> t4</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> jg,level_index</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="keyword">select</span> <span class="string">'总体'</span> jg, level_index, <span class="keyword">count</span>(<span class="number">1</span>) cnt</span><br><span class="line">    <span class="keyword">from</span> t4</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> level_index)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> level_index <span class="keyword">asc</span>, cnt <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse UBT EVENT 表创建</title>
    <url>/2021/07/20/ClickHouseUBTEVENT%E8%A1%A8%E5%88%9B%E5%BB%BA/</url>
    <content><![CDATA[<p>ClickHouse 建表语句，该表为用户行为分析平台的事件表。</p>
<a id="more"></a>
<hr>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> <span class="string">`test.event`</span>(</span><br><span class="line"> <span class="string">`distinct_id`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`xwho`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`xwhen`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`xwhat`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`os`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`utm_campaign_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`utm_campaign`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`utm_medium`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_utm_medium`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`utm_source`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_utm_source`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`utm_content`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`utm_term`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`channel`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`referrer`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`referrer_domain`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`traffic_source_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_traffic_source_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`search_engine`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_search_engine`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`search_keyword`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`social_share_from`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`scene`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_scene`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`scene_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_scene_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`platform`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_platform`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`session_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`url`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`social_media`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_social_media`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`url_domain`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`track_xwhen`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`fingerprint`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`campaign_shortlink`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`offset`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`xwhat_id`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`need_update`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`network`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`os_version`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`is_first_day`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`is_from_background`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`model`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`screen_width`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`is_first_time`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`brand`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`ip`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`screen_height`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`language`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`app_version`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`lib`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`is_time_calibrated`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`is_login`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`lib_version`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`platform_extra`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`time_zone`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`manufacturer`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`country`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`province`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`city`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`title`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`duration`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`element_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`element_path`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`start_source`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`element_content`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`element_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`carrier_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`element_position`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`parent_url`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`web_crawler`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`user_agent`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`browser_version`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`device_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_device_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`browser`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`startup_time`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`original_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`url_path`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`element_class_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`element_target_url`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`btn_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`content_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`nav_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`desc`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`is_success`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`method`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`user_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`source_position_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`source_module`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`rank`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`module_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`link_page_url`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`ad_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`ad_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`location`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`element_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`third_category`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`news_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`news_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`event_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`second_category`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`first_category`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`price`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`item_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`item_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`item_count`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`keyword_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`min_buy_unit`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`review_num`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`list_price`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`event_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`reason`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`brand_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`shop_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`shop_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`brand_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`video_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`video_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`is_filter`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`item_code`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`item_specific_module`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`shop_num`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`recommend_item_num`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`page_num`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`filter_detail`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`filter_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`crash_data`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_price`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`second_category_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`stock_status_pro`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`first_category_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`stock_status`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`shelf_status`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`item_u8_code`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_price_pro`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`third_category_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`min_buy_num`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`String_stp_price2`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_price1`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_price3`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_num3`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_num2`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_num1`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_price4`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_num4`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_price5`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_stp_num5`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`pic_num`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`integral_use_amount`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`shipping_cost`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`orgcode`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`credit_period_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`is_wait_deliver`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`insert_time`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`actual_pay_amount`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`coupon_freight_use_amount`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`is_negotiation`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`package_use_amount`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`discount_use_amount`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`channel_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`order_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`category_code`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`payment_time`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`is_status`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`payment_method`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`per_pay_amount`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`is_first_order`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`status`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`smb_pay_time`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`order_amount`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`confirm_time`</span> Nullable(Int64),</span><br><span class="line"> <span class="string">`action`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`viewport_position`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`viewport_width`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`viewport_height`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`event_duration`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`deal_amount`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`live_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`live_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`debug`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`news_channel`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`news_subject`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`radio_num`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`plan_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`plan_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`click_content_rank`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`activity_tag`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`String_list_price`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`is_delete`</span> Nullable(Int32),</span><br><span class="line"> <span class="string">`device_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`oaid`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`imei`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`android_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`element_clickable`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`click_x`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`click_y`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`page_height`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`element_y`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`element_x`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`page_width`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`push_title`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`activityid`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`activityname`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`activitytype`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`virtualurl`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`prop_0`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`prop_1`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`prop_2`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`prop_3`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`prop_4`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dict_platform_extra`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`page_name`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`sms_channel`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`message_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`ea_cohort_code`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`ad_space_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`ad_space_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`grade1`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`ad_space_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`dese`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`item_type`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`package_num`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`package_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`pagestaytime`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`is_ad_sku`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`sku_rank`</span> Nullable(decimal64(<span class="number">3</span>)),</span><br><span class="line"> <span class="string">`package_name`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`url_domain2`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`search_scene`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`share_source_id`</span> Nullable(<span class="keyword">String</span>),</span><br><span class="line"> <span class="string">`static_type`</span> Nullable(<span class="keyword">String</span>)</span><br><span class="line"> ) <span class="keyword">engine</span> = TinyLog;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse 单机部署、HDFS配置</title>
    <url>/2021/07/19/ClickHouse%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2%E3%80%81HDFS%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>两部分内容：</p>
<ol>
<li>ClickHouse 单机部署</li>
<li>ClickHouse 与 HDFS 数据传输配置</li>
</ol>
<a id="more"></a>
<hr>
<h3 id="ClickHouse-单机部署"><a href="#ClickHouse-单机部署" class="headerlink" title="ClickHouse 单机部署"></a>ClickHouse 单机部署</h3><h4 id="确认-SSE-4-2-supported"><a href="#确认-SSE-4-2-supported" class="headerlink" title="确认 SSE 4.2 supported"></a>确认 SSE 4.2 supported</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> grep -q sse4_2 /proc/cpuinfo &amp;&amp; <span class="built_in">echo</span> <span class="string">"SSE 4.2 supported"</span> || <span class="built_in">echo</span> <span class="string">"SSE 4.2 not supported"</span></span></span><br></pre></td></tr></table></figure>

<h4 id="yum-配置"><a href="#yum-配置" class="headerlink" title="yum 配置"></a>yum 配置</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install yum-utils</span><br><span class="line">rpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPG</span><br><span class="line">yum-config-manager --add-repo https://repo.clickhouse.tech/rpm/stable/x86_64</span><br></pre></td></tr></table></figure>

<h4 id="安装-ClickHouse"><a href="#安装-ClickHouse" class="headerlink" title="安装 ClickHouse"></a>安装 ClickHouse</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install clickhouse-server clickhouse-client</span><br></pre></td></tr></table></figure>

<h4 id="启动-ClickHouse"><a href="#启动-ClickHouse" class="headerlink" title="启动 ClickHouse"></a>启动 ClickHouse</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl start clickhouse-server.service</span><br><span class="line">systemctl status clickhouse-server.service</span><br><span class="line">systemctl stop clickhouse-server.service</span><br></pre></td></tr></table></figure>

<h4 id="日志文件将输出"><a href="#日志文件将输出" class="headerlink" title="日志文件将输出"></a>日志文件将输出</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /var/log/clickhouse-server</span><br><span class="line">ps -aux | grep clickhouse</span><br><span class="line">netstat -nltp | grep clickhouse</span><br></pre></td></tr></table></figure>

<h4 id="修改配置文件config-xml"><a href="#修改配置文件config-xml" class="headerlink" title="修改配置文件config.xml"></a>修改配置文件config.xml</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/clickhouse-server/config.xml</span><br></pre></td></tr></table></figure>

<p>先修改 tcp_port 为 9977，因为这个端口和 HDFS 的端口冲突了</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tcp_port</span>&gt;</span>9977<span class="tag">&lt;/<span class="name">tcp_port</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>配置外网访问，修改后 DBeaver 才能访问<br>listen_host表示能监听的主机，::表示任意主机都可以访问</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">listen_host</span>&gt;</span>::<span class="tag">&lt;/<span class="name">listen_host</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="设置用户名和密码"><a href="#设置用户名和密码" class="headerlink" title="设置用户名和密码"></a>设置用户名和密码</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/clickhouse-server/users.xml</span><br></pre></td></tr></table></figure>

<p>在 <users> 下增加以下配置</users></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">root</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">password</span>&gt;</span>root<span class="tag">&lt;/<span class="name">password</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">networks</span> <span class="attr">incl</span>=<span class="string">"networks"</span> <span class="attr">replace</span>=<span class="string">"replace"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ip</span>&gt;</span>::/0<span class="tag">&lt;/<span class="name">ip</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">networks</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">profile</span>&gt;</span>default<span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">quota</span>&gt;</span>default<span class="tag">&lt;/<span class="name">quota</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>修改完成后重启服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart clickhouse-server.service</span><br><span class="line">systemctl status clickhouse-server.service</span><br></pre></td></tr></table></figure>

<h4 id="客户端登录"><a href="#客户端登录" class="headerlink" title="客户端登录"></a>客户端登录</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">clickhouse-client --host=localhost --port=9977 --user=root --password=root -m</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> tablename;</span><br></pre></td></tr></table></figure>



<h3 id="ClickHouse-与-HDFS-数据传输配置"><a href="#ClickHouse-与-HDFS-数据传输配置" class="headerlink" title="ClickHouse 与 HDFS 数据传输配置"></a>ClickHouse 与 HDFS 数据传输配置</h3><h4 id="Hadoop-集群-hdfs-site-xml-拷贝"><a href="#Hadoop-集群-hdfs-site-xml-拷贝" class="headerlink" title="Hadoop 集群 hdfs-site.xml 拷贝"></a>Hadoop 集群 hdfs-site.xml 拷贝</h4><p>Hadoop 集群 hdfs-site.xml 拷贝到 /etc/clickhouse-server</p>
<p>hdfs-site.xml 重命名为 hdfs-client.xml</p>
<h4 id="修改-ClickHouse-Server-启动文件，添加环境变量"><a href="#修改-ClickHouse-Server-启动文件，添加环境变量" class="headerlink" title="修改 ClickHouse Server 启动文件，添加环境变量"></a>修改 ClickHouse Server 启动文件，添加环境变量</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/systemd/system/clickhouse-server.service</span><br><span class="line">Environment="LIBHDFS3_CONF=/etc/clickhouse-server/hdfs-client.xml"</span><br></pre></td></tr></table></figure>

<p>加载并重启 clickhouse-server</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart clickhouse-server.service</span><br><span class="line">systemctl status clickhouse-server.service</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse UBT 不包含细分维度的 windowFunnel 计算</title>
    <url>/2021/07/21/ClickHouseUBT%E4%B8%8D%E5%8C%85%E5%90%AB%E7%BB%86%E5%88%86%E7%BB%B4%E5%BA%A6%E7%9A%84windowFunnel%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>Funnel 计算（不包含细分维度）</p>
<p>查询条件:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 漏斗流程</span><br><span class="line">   login_in -&gt; $pageview -&gt; $end -&gt; $startup -&gt; $end -&gt; $startup</span><br><span class="line">2. 对第一层 login_in 进行了 screen_width &gt; 500 的筛选</span><br><span class="line">3. 时间范围 2021-07-01 ~ 2021-07-09</span><br><span class="line">4. 转化周期 1 天</span><br></pre></td></tr></table></figure>

<a id="more"></a>
<hr>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> level_index,<span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">FROM</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">SELECT</span>  distinct_id,</span><br><span class="line">        arrayWithConstant(<span class="keyword">level</span>, <span class="number">1</span>) <span class="keyword">levels</span>,</span><br><span class="line">        arrayEnumerate( <span class="keyword">levels</span> ) b, <span class="comment">-- for_test_col</span></span><br><span class="line">        arrayJoin(arrayEnumerate( <span class="keyword">levels</span> )) level_index</span><br><span class="line">      <span class="keyword">FROM</span> (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">          distinct_id,</span><br><span class="line">          windowFunnel(<span class="number">86400</span>)(</span><br><span class="line">            <span class="built_in">time</span>,</span><br><span class="line">            xwhat = <span class="string">'login_in'</span>,</span><br><span class="line">            xwhat = <span class="string">'$pageview'</span>,</span><br><span class="line">            xwhat = <span class="string">'$end'</span>,</span><br><span class="line">            xwhat = <span class="string">'$startup'</span>,</span><br><span class="line">            xwhat = <span class="string">'$end'</span>,</span><br><span class="line">            xwhat = <span class="string">'$startup'</span></span><br><span class="line">          ) <span class="keyword">AS</span> <span class="keyword">level</span></span><br><span class="line">        <span class="keyword">FROM</span> (</span><br><span class="line">          <span class="keyword">SELECT</span></span><br><span class="line">          distinct_id</span><br><span class="line">          , xwhat</span><br><span class="line">          , toDateTime(toUInt64(<span class="keyword">round</span>(xwhen/<span class="number">1000</span>))) <span class="keyword">as</span> <span class="built_in">time</span></span><br><span class="line">          <span class="keyword">FROM</span> test.event</span><br><span class="line">          <span class="keyword">WHERE</span> (toDate(<span class="built_in">time</span>) &gt;= <span class="string">'2021-07-01'</span> <span class="keyword">and</span> toDate(<span class="built_in">time</span>) &lt;= <span class="string">'2021-07-09'</span> <span class="keyword">and</span> xwhat = <span class="string">'login_in'</span> <span class="keyword">and</span> screen_width &gt; <span class="number">500</span> )</span><br><span class="line">          <span class="keyword">or</span> (toDate(<span class="built_in">time</span>) &gt;= <span class="string">'2021-07-01'</span> <span class="keyword">and</span> toDate(<span class="built_in">time</span>) &lt;= <span class="string">'2021-07-10'</span> <span class="keyword">and</span> xwhat <span class="keyword">in</span> (<span class="string">'$pageview'</span>,<span class="string">'$end'</span>,<span class="string">'$startup'</span>))</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> distinct_id</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> level_index</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> level_index;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse 数据导入 HDFS -&gt; ClickHouse</title>
    <url>/2021/07/20/ClickHouse%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5HDFS-CK/</url>
    <content><![CDATA[<p>ClickHouse 数据导入。</p>
<p>HDFS 数据导入 ClickHouse，HDFS 文件格式为 ORC。</p>
<a id="more"></a>
<hr>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span></span><br><span class="line">test.<span class="string">`event`</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">*</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">hdfs(<span class="string">'hdfs://hmdservice/opt/user/HomedoDB/warehouse/ods_cdp/event_vd/d=20210710/*'</span>,<span class="string">'ORC'</span>,</span><br><span class="line"><span class="string">'`distinct_id` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `xwho` Nullable(String),</span></span><br><span class="line"><span class="string"> `xwhen` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `xwhat` Nullable(String),</span></span><br><span class="line"><span class="string"> `os` Nullable(String),</span></span><br><span class="line"><span class="string"> `utm_campaign_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `utm_campaign` Nullable(String),</span></span><br><span class="line"><span class="string"> `utm_medium` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_utm_medium` Nullable(String),</span></span><br><span class="line"><span class="string"> `utm_source` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_utm_source` Nullable(String),</span></span><br><span class="line"><span class="string"> `utm_content` Nullable(String),</span></span><br><span class="line"><span class="string"> `utm_term` Nullable(String),</span></span><br><span class="line"><span class="string"> `channel` Nullable(String),</span></span><br><span class="line"><span class="string"> `referrer` Nullable(String),</span></span><br><span class="line"><span class="string"> `referrer_domain` Nullable(String),</span></span><br><span class="line"><span class="string"> `traffic_source_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_traffic_source_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `search_engine` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_search_engine` Nullable(String),</span></span><br><span class="line"><span class="string"> `search_keyword` Nullable(String),</span></span><br><span class="line"><span class="string"> `social_share_from` Nullable(String),</span></span><br><span class="line"><span class="string"> `scene` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_scene` Nullable(String),</span></span><br><span class="line"><span class="string"> `scene_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_scene_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `platform` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_platform` Nullable(String),</span></span><br><span class="line"><span class="string"> `session_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `url` Nullable(String),</span></span><br><span class="line"><span class="string"> `social_media` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_social_media` Nullable(String),</span></span><br><span class="line"><span class="string"> `url_domain` Nullable(String),</span></span><br><span class="line"><span class="string"> `track_xwhen` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `fingerprint` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `campaign_shortlink` Nullable(String),</span></span><br><span class="line"><span class="string"> `offset` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `xwhat_id` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `need_update` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `network` Nullable(String),</span></span><br><span class="line"><span class="string"> `os_version` Nullable(String),</span></span><br><span class="line"><span class="string"> `is_first_day` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `is_from_background` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `model` Nullable(String),</span></span><br><span class="line"><span class="string"> `screen_width` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `is_first_time` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `brand` Nullable(String),</span></span><br><span class="line"><span class="string"> `ip` Nullable(String),</span></span><br><span class="line"><span class="string"> `screen_height` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `language` Nullable(String),</span></span><br><span class="line"><span class="string"> `app_version` Nullable(String),</span></span><br><span class="line"><span class="string"> `lib` Nullable(String),</span></span><br><span class="line"><span class="string"> `is_time_calibrated` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `is_login` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `lib_version` Nullable(String),</span></span><br><span class="line"><span class="string"> `platform_extra` Nullable(String),</span></span><br><span class="line"><span class="string"> `time_zone` Nullable(String),</span></span><br><span class="line"><span class="string"> `manufacturer` Nullable(String),</span></span><br><span class="line"><span class="string"> `country` Nullable(String),</span></span><br><span class="line"><span class="string"> `province` Nullable(String),</span></span><br><span class="line"><span class="string"> `city` Nullable(String),</span></span><br><span class="line"><span class="string"> `title` Nullable(String),</span></span><br><span class="line"><span class="string"> `duration` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `element_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `element_path` Nullable(String),</span></span><br><span class="line"><span class="string"> `start_source` Nullable(String),</span></span><br><span class="line"><span class="string"> `element_content` Nullable(String),</span></span><br><span class="line"><span class="string"> `element_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `carrier_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `element_position` Nullable(String),</span></span><br><span class="line"><span class="string"> `parent_url` Nullable(String),</span></span><br><span class="line"><span class="string"> `web_crawler` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `user_agent` Nullable(String),</span></span><br><span class="line"><span class="string"> `browser_version` Nullable(String),</span></span><br><span class="line"><span class="string"> `device_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_device_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `browser` Nullable(String),</span></span><br><span class="line"><span class="string"> `startup_time` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `original_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `url_path` Nullable(String),</span></span><br><span class="line"><span class="string"> `element_class_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `element_target_url` Nullable(String),</span></span><br><span class="line"><span class="string"> `btn_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `type` Nullable(String),</span></span><br><span class="line"><span class="string"> `content_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `nav_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `desc` Nullable(String),</span></span><br><span class="line"><span class="string"> `is_success` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `method` Nullable(String),</span></span><br><span class="line"><span class="string"> `user_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `source_position_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `source_module` Nullable(String),</span></span><br><span class="line"><span class="string"> `rank` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `name` Nullable(String),</span></span><br><span class="line"><span class="string"> `module_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `link_page_url` Nullable(String),</span></span><br><span class="line"><span class="string"> `ad_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `ad_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `location` Nullable(String),</span></span><br><span class="line"><span class="string"> `element_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `third_category` Nullable(String),</span></span><br><span class="line"><span class="string"> `news_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `news_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `event_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `second_category` Nullable(String),</span></span><br><span class="line"><span class="string"> `first_category` Nullable(String),</span></span><br><span class="line"><span class="string"> `price` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `item_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `item_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `item_count` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `keyword_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `min_buy_unit` Nullable(String),</span></span><br><span class="line"><span class="string"> `review_num` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `list_price` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `event_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `reason` Nullable(String),</span></span><br><span class="line"><span class="string"> `brand_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `shop_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `shop_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `brand_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `video_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `video_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `is_filter` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `item_code` Nullable(String),</span></span><br><span class="line"><span class="string"> `item_specific_module` Nullable(String),</span></span><br><span class="line"><span class="string"> `shop_num` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `recommend_item_num` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `page_num` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `filter_detail` Nullable(String),</span></span><br><span class="line"><span class="string"> `filter_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `crash_data` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_price` Nullable(String),</span></span><br><span class="line"><span class="string"> `second_category_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `stock_status_pro` Nullable(String),</span></span><br><span class="line"><span class="string"> `first_category_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `stock_status` Nullable(String),</span></span><br><span class="line"><span class="string"> `shelf_status` Nullable(String),</span></span><br><span class="line"><span class="string"> `item_u8_code` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_price_pro` Nullable(String),</span></span><br><span class="line"><span class="string"> `third_category_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `min_buy_num` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `string_stp_price2` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_price1` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_price3` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_num3` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_num2` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_num1` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_price4` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_num4` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_price5` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_stp_num5` Nullable(String),</span></span><br><span class="line"><span class="string"> `pic_num` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `integral_use_amount` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `shipping_cost` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `orgcode` Nullable(String),</span></span><br><span class="line"><span class="string"> `credit_period_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `is_wait_deliver` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `insert_time` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `actual_pay_amount` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `coupon_freight_use_amount` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `is_negotiation` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `package_use_amount` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `discount_use_amount` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `channel_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `order_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `category_code` Nullable(String),</span></span><br><span class="line"><span class="string"> `payment_time` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `is_status` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `payment_method` Nullable(String),</span></span><br><span class="line"><span class="string"> `per_pay_amount` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `is_first_order` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `status` Nullable(String),</span></span><br><span class="line"><span class="string"> `smb_pay_time` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `order_amount` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `confirm_time` Nullable(Int64),</span></span><br><span class="line"><span class="string"> `action` Nullable(String),</span></span><br><span class="line"><span class="string"> `viewport_position` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `viewport_width` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `viewport_height` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `event_duration` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `deal_amount` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `live_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `live_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `debug` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `news_channel` Nullable(String),</span></span><br><span class="line"><span class="string"> `news_subject` Nullable(String),</span></span><br><span class="line"><span class="string"> `radio_num` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `plan_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `plan_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `click_content_rank` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `activity_tag` Nullable(String),</span></span><br><span class="line"><span class="string"> `string_list_price` Nullable(String),</span></span><br><span class="line"><span class="string"> `is_delete` Nullable(Int32),</span></span><br><span class="line"><span class="string"> `device_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `oaid` Nullable(String),</span></span><br><span class="line"><span class="string"> `imei` Nullable(String),</span></span><br><span class="line"><span class="string"> `android_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `element_clickable` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `click_x` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `click_y` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `page_height` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `element_y` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `element_x` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `page_width` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `push_title` Nullable(String),</span></span><br><span class="line"><span class="string"> `activityid` Nullable(String),</span></span><br><span class="line"><span class="string"> `activityname` Nullable(String),</span></span><br><span class="line"><span class="string"> `activitytype` Nullable(String),</span></span><br><span class="line"><span class="string"> `virtualurl` Nullable(String),</span></span><br><span class="line"><span class="string"> `prop_0` Nullable(String),</span></span><br><span class="line"><span class="string"> `prop_1` Nullable(String),</span></span><br><span class="line"><span class="string"> `prop_2` Nullable(String),</span></span><br><span class="line"><span class="string"> `prop_3` Nullable(String),</span></span><br><span class="line"><span class="string"> `prop_4` Nullable(String),</span></span><br><span class="line"><span class="string"> `dict_platform_extra` Nullable(String),</span></span><br><span class="line"><span class="string"> `page_name` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `sms_channel` Nullable(String),</span></span><br><span class="line"><span class="string"> `message_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `ea_cohort_code` Nullable(String),</span></span><br><span class="line"><span class="string"> `ad_space_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `ad_space_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `grade1` Nullable(String),</span></span><br><span class="line"><span class="string"> `ad_space_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `dese` Nullable(String),</span></span><br><span class="line"><span class="string"> `item_type` Nullable(String),</span></span><br><span class="line"><span class="string"> `package_num` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `package_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `pagestaytime` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `is_ad_sku` Nullable(String),</span></span><br><span class="line"><span class="string"> `sku_rank` Nullable(decimal64(3)),</span></span><br><span class="line"><span class="string"> `package_name` Nullable(String),</span></span><br><span class="line"><span class="string"> `url_domain2` Nullable(String),</span></span><br><span class="line"><span class="string"> `search_scene` Nullable(String),</span></span><br><span class="line"><span class="string"> `share_source_id` Nullable(String),</span></span><br><span class="line"><span class="string"> `static_type` Nullable(String)'</span>)</span><br><span class="line">;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris BE 节点迁移、节点下线</title>
    <url>/2021/07/06/DorisBE%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<p>Doris 动态分区使用方法说明。</p>
<p>Doris 动态分区使用方法说明。</p>
<p>在我们的生产环境，Doris 部署在 CDH 集群，每当早批进行时，都会出现资源争夺的问题。</p>
<p>因为 Doris BE 单个进程占用的内存最大，所以总是会被 Kill 掉，导致任务失败。</p>
<p>基于以上原因需要将 Doris BE 独立部署，因为时生产环境，数据的迁移需要平滑无感，不影响成产使用。</p>
<a id="more"></a>
<hr>
<h3 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h3><p><a href="http://doris.incubator.apache.org/master/zh-CN/installing/install-deploy.html#%E6%89%A9%E5%AE%B9%E7%BC%A9%E5%AE%B9" target="_blank" rel="noopener">http://doris.incubator.apache.org/master/zh-CN/installing/install-deploy.html#%E6%89%A9%E5%AE%B9%E7%BC%A9%E5%AE%B9</a></p>
<h3 id="BE-下线流程"><a href="#BE-下线流程" class="headerlink" title="BE 下线流程"></a>BE 下线流程</h3><p>在新节点部署完成后，开始进行 BE 节点的下线操作。</p>
<h4 id="下线操作命令："><a href="#下线操作命令：" class="headerlink" title="下线操作命令："></a>下线操作命令：</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ALTER SYSTEM DECOMMISSION BACKEND "be_host:be_heartbeat_service_port";</span><br></pre></td></tr></table></figure>

<h4 id="下线操作确认："><a href="#下线操作确认：" class="headerlink" title="下线操作确认："></a>下线操作确认：</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> PROC <span class="string">'/backends'</span> \G</span><br></pre></td></tr></table></figure>

<h4 id="下线操作状态确认："><a href="#下线操作状态确认：" class="headerlink" title="下线操作状态确认："></a>下线操作状态确认：</h4><p>可以看到 SystemDecommissioned: true，证明正在进行 BE 的下线操作。</p>
<p>TabletNum 会持续的减少，转移到其他 BE 节点。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">*************************** 4. row ***************************</span><br><span class="line">            BackendId: 10002</span><br><span class="line">              Cluster: default_cluster</span><br><span class="line">                   IP: 10.0.15.131</span><br><span class="line">             HostName: cdh9</span><br><span class="line">        HeartbeatPort: 9050</span><br><span class="line">               BePort: 9060</span><br><span class="line">             HttpPort: 18040</span><br><span class="line">             BrpcPort: 8060</span><br><span class="line">        LastStartTime: 2021-06-29 17:34:50</span><br><span class="line">        LastHeartbeat: 2021-07-06 08:07:27</span><br><span class="line">                Alive: true</span><br><span class="line"> SystemDecommissioned: true</span><br><span class="line">ClusterDecommissioned: false</span><br><span class="line">            TabletNum: 463</span><br><span class="line">     DataUsedCapacity: 6.894 GB</span><br><span class="line">        AvailCapacity: 1.528 TB</span><br><span class="line">        TotalCapacity: 2.884 TB</span><br><span class="line">              UsedPct: 47.02 %</span><br><span class="line">       MaxDiskUsedPct: 47.02 %</span><br><span class="line">               ErrMsg: </span><br><span class="line">              Version: </span><br><span class="line">               Status: &#123;"lastSuccessReportTabletsTime":"N/A"&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><p>BE 迁移时 Tablet 的数量减少到0或者很少的数量后会停止不动，始终不能结束，</p>
<p>这时候需要耐心等待，我们的经验是，1天下线完成一个节点。</p>
<p>所以要等待 BE 自动迁移完成，不要强行删除，强行删除可能会导致数据丢失。</p>
<h4 id="取消下线命令："><a href="#取消下线命令：" class="headerlink" title="取消下线命令："></a>取消下线命令：</h4><p>取消下线后，会立即停止 Tablet 的转移，维持当前的 Tablet 数量，</p>
<p>一段时间后，其他节点的 Tablet 会逐渐平衡到这个节点。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">CANCEL DECOMMISSION BACKEND "be_host:be_heartbeat_service_port";</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris BE 状态监控</title>
    <url>/2021/07/23/DorisBE%E7%8A%B6%E6%80%81%E7%9B%91%E6%8E%A7/</url>
    <content><![CDATA[<p>Doris BE 监控，</p>
<p>当 BE 挂掉，Azkaban 定时任务会监控到，同时将 BE 服务重启，</p>
<p>重启完成后，Azkaban 失败、告警。</p>
<a id="more"></a>
<hr>
<h4 id="Azkaban-Zip-脚本"><a href="#Azkaban-Zip-脚本" class="headerlink" title="Azkaban Zip 脚本"></a>Azkaban Zip 脚本</h4><p>azkaban.project</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure>

<p>doris_check.flow</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">doris_node170_be_was_dead_and_restart_complited_now</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/opt/sync/sync_script/sink_doris/az_doris_be_check.sh</span> <span class="number">10.0</span><span class="number">.14</span><span class="number">.170</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">doris_node171_be_was_dead_and_restart_complited_now</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/opt/sync/sync_script/sink_doris/az_doris_be_check.sh</span> <span class="number">10.0</span><span class="number">.14</span><span class="number">.171</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">doris_node172_be_was_dead_and_restart_complited_now</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/opt/sync/sync_script/sink_doris/az_doris_be_check.sh</span> <span class="number">10.0</span><span class="number">.14</span><span class="number">.172</span></span><br></pre></td></tr></table></figure>

<p>az_doris_be_check.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">host=$1</span><br><span class="line">doris_check_script_path=/opt/doris/be/bin</span><br><span class="line"></span><br><span class="line">ssh root@$host &lt;&lt; eeooff</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash">doris_check_script_path/doris_be_check.sh</span></span><br><span class="line"></span><br><span class="line">eeooff</span><br></pre></td></tr></table></figure>

<h4 id="BE-节点脚本"><a href="#BE-节点脚本" class="headerlink" title="BE 节点脚本"></a>BE 节点脚本</h4><p>$DORIS_HOME/be/bin/doris_be_check.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">check="$(ps -ef | grep palo_be  | grep -v grep | awk '&#123;print $8&#125;' | awk -F '[/]' '&#123;print $NF&#125;')"</span><br><span class="line">doris_path="/opt/doris"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">start()&#123;</span><br><span class="line"> now=`date "+%Y-%m-%d %H:%M:%S"`</span><br><span class="line"> echo "BE重启中... 重启时间:$now..."</span><br><span class="line"><span class="meta"> $</span><span class="bash">doris_path/be/bin/start_be.sh --daemon</span></span><br><span class="line"> sleep 10s</span><br><span class="line"> test_after_restart="$(ps -ef | grep palo_be  | grep -v grep | awk '&#123;print $8&#125;' | awk -F '[/]' '&#123;print $NF&#125;')"</span><br><span class="line"> if [[ $test_after_restart = "palo_be" ]];</span><br><span class="line"> then</span><br><span class="line">  echo "be重启成功..."</span><br><span class="line"> else</span><br><span class="line">  echo "be启动失败..."</span><br><span class="line"> fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [[ $check = "palo_be" ]];</span><br><span class="line">then</span><br><span class="line">  echo "BE 运行正常..."</span><br><span class="line">  exit 0</span><br><span class="line">else</span><br><span class="line">  echo "BE 挂了..."</span><br><span class="line">  start</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris 使用 AGGREGATE 模型进行数据局部字段更新</title>
    <url>/2021/05/24/Doris%E4%BD%BF%E7%94%A8AGGREGATE%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%B1%80%E9%83%A8%E5%AD%97%E6%AE%B5%E6%9B%B4%E6%96%B0/</url>
    <content><![CDATA[<p>使用 AGGREGATE 模型进行数据局部字段更新DEMO。</p>
<a id="more"></a>

<hr>
<h4 id="创建测试库"><a href="#创建测试库" class="headerlink" title="创建测试库"></a>创建测试库</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure>

<h4 id="创建测试表"><a href="#创建测试表" class="headerlink" title="创建测试表"></a>创建测试表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test.<span class="string">`REPLACE_TEST`</span> (</span><br><span class="line">  a <span class="built_in">varchar</span>(<span class="number">1000</span>),</span><br><span class="line">  b <span class="built_in">varchar</span>(<span class="number">1000</span>) REPLACE_IF_NOT_NULL,</span><br><span class="line">  c <span class="built_in">varchar</span>(<span class="number">1000</span>) REPLACE_IF_NOT_NULL,</span><br><span class="line">  d <span class="built_in">varchar</span>(<span class="number">1000</span>) REPLACE_IF_NOT_NULL</span><br><span class="line">) <span class="keyword">ENGINE</span>=OLAP</span><br><span class="line"><span class="keyword">AGGREGATE</span> <span class="keyword">KEY</span>(<span class="string">`a`</span>)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">"OLAP"</span></span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(<span class="string">`a`</span>) BUCKETS <span class="number">3</span></span><br><span class="line">PROPERTIES (</span><br><span class="line"><span class="string">"replication_num"</span> = <span class="string">"3"</span>,</span><br><span class="line"><span class="string">"in_memory"</span> = <span class="string">"false"</span>,</span><br><span class="line"><span class="string">"storage_format"</span> = <span class="string">"V2"</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="向测试表插入数据"><a href="#向测试表插入数据" class="headerlink" title="向测试表插入数据"></a>向测试表插入数据</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test.<span class="string">`REPLACE_TEST`</span></span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">"A"</span>, <span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>),</span><br><span class="line">(<span class="string">"B"</span>, <span class="string">"4"</span>, <span class="string">"5"</span>, <span class="string">"6"</span>);</span><br><span class="line"> </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test.<span class="string">`REPLACE_TEST`</span>;</span><br></pre></td></tr></table></figure>

<h4 id="创建-Kafka-Routine-Load-Task"><a href="#创建-Kafka-Routine-Load-Task" class="headerlink" title="创建 Kafka Routine Load Task"></a>创建 Kafka Routine Load Task</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> ROUTINE <span class="keyword">LOAD</span> test.REPLACE_TEST <span class="keyword">ON</span> REPLACE_TEST</span><br><span class="line"><span class="keyword">COLUMNS</span>(a, b, c, d)</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line"><span class="string">"desired_concurrent_number"</span>=<span class="string">"3"</span>,</span><br><span class="line"><span class="string">"max_batch_interval"</span>=<span class="string">"20"</span>,</span><br><span class="line"><span class="string">"max_batch_rows"</span>=<span class="string">"300000"</span>,</span><br><span class="line"><span class="string">"max_batch_size"</span>=<span class="string">"209715200"</span>,</span><br><span class="line"><span class="string">"strict_mode"</span>=<span class="string">"false"</span>,</span><br><span class="line"><span class="string">"format"</span>=<span class="string">"json"</span>,</span><br><span class="line"><span class="string">"jsonpaths"</span> = <span class="string">"[\"</span>$.a\<span class="string">",\"</span>$.b\<span class="string">",\"</span>$.c\<span class="string">",\"</span>$.d\<span class="string">"]"</span>,</span><br><span class="line"><span class="string">"strip_outer_array"</span> = <span class="string">"false"</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">FROM</span> KAFKA</span><br><span class="line">(</span><br><span class="line"><span class="string">"kafka_broker_list"</span>=<span class="string">"10.10.110.235:9092"</span>,</span><br><span class="line"><span class="string">"kafka_topic"</span>=<span class="string">"doris_replace_test"</span>,</span><br><span class="line"><span class="string">"property.kafka_default_offsets"</span> = <span class="string">"OFFSET_BEGINNING"</span>,</span><br><span class="line"><span class="string">"property.auto.offset.reset"</span> = <span class="string">"earliest"</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="确认-Kafka-Routine-Load-Task-作业状态"><a href="#确认-Kafka-Routine-Load-Task-作业状态" class="headerlink" title="确认 Kafka Routine Load Task 作业状态"></a>确认 Kafka Routine Load Task 作业状态</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="keyword">SHOW</span> ROUTINE <span class="keyword">FOR</span> test.REPLACE_TEST;</span><br></pre></td></tr></table></figure>

<h4 id="向-Kafka-Topic-插入数据"><a href="#向-Kafka-Topic-插入数据" class="headerlink" title="向 Kafka Topic 插入数据"></a>向 Kafka Topic 插入数据</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">kafka-console-producer <span class="comment">--topic doris_replace_test --broker-list 10.10.110.235:9092</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">-- Json 数据</span></span><br><span class="line">&#123;"a":"C","b":"7","c":"8","d":"9"&#125;</span><br><span class="line">&#123;"a":"C","c":"10","d":"11"&#125;</span><br><span class="line">&#123;"a":"C","d":"20"&#125;</span><br></pre></td></tr></table></figure>

<p>Kafka Routine Load Task 会自动识别Json数据中存在的字段，不存在的字段不会被更新。</p>
]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris Unique 模型使用 Sequence 功能解决数据更新乱序问题</title>
    <url>/2021/06/07/Doris%E4%BD%BF%E7%94%A8Unique%E6%A8%A1%E5%9E%8BSequence%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E4%B9%B1%E5%BA%8F%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>Unique模型有按照Key取最新值的去重功能，</p>
<p>因为Routine Load Task 是累计一定数据量/到达时间周期批量导入，</p>
<p>但是不保证每个批量的数据的顺序，</p>
<p>所以当一批数据中有同Key的数据时，会出现乱序覆盖的问题。</p>
<a id="more"></a>

<hr>
<p>以下是这个场景使用 sequence 功能的解决方案：</p>
<h4 id="创建测试表"><a href="#创建测试表" class="headerlink" title="创建测试表"></a>创建测试表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- dwd_homedo_real.dwd_agent_agent_test1 definition</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dwd_homedo_real.<span class="string">`dwd_agent_agent_test1`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`accountid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`typeid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`legalperson`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`telephone`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`registeraddress`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`address`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`postal`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`buslicensefilename`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`taxregcertificatefilename`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`orgcodefilename`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`taxnumber`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`bankname`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`remark`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`bidcount`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`packagecount`</span> <span class="built_in">decimal</span>(<span class="number">18</span>, <span class="number">2</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`serviceid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`statusflag`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`submitid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`auditid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`audittime`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`inserttime`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`updatetime`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`deletetime`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`mark`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`version`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`tag`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`firstsuccessaudittime`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`isthreeinone`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`bankaccountnumber`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`financepayamount`</span> <span class="built_in">decimal</span>(<span class="number">18</span>, <span class="number">2</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`audittype`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`systemremark`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`sysmodifydate`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`provinceid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`cityid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`areaid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`platform`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`managementprovinceid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`managementcityid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`managementareaid`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`managementaddress`</span> <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span>,</span><br><span class="line">  <span class="string">`eventtime`</span> datetime <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">""</span></span><br><span class="line">) <span class="keyword">ENGINE</span>=OLAP</span><br><span class="line"><span class="keyword">UNIQUE</span> <span class="keyword">KEY</span>(<span class="string">`id`</span>)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">"OLAP"</span></span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(<span class="string">`id`</span>) BUCKETS <span class="number">3</span></span><br><span class="line">PROPERTIES (</span><br><span class="line"><span class="string">"replication_num"</span> = <span class="string">"3"</span>,</span><br><span class="line"><span class="string">"in_memory"</span> = <span class="string">"false"</span>,</span><br><span class="line"><span class="string">"storage_format"</span> = <span class="string">"V2"</span>,</span><br><span class="line"><span class="string">"function_column.sequence_type"</span> = <span class="string">'Datetime'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="创建Routine-Load-Task"><a href="#创建Routine-Load-Task" class="headerlink" title="创建Routine Load Task"></a>创建Routine Load Task</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> ROUTINE <span class="keyword">LOAD</span> dwd_homedo_real.dwd_agent_agent_test1 <span class="keyword">ON</span> dwd_agent_agent_test1</span><br><span class="line"><span class="keyword">COLUMNS</span>(</span><br><span class="line"><span class="keyword">id</span>,</span><br><span class="line">accountid,</span><br><span class="line">typeid,</span><br><span class="line"><span class="keyword">name</span>,</span><br><span class="line">legalperson,</span><br><span class="line">telephone,</span><br><span class="line">registeraddress,</span><br><span class="line">address,</span><br><span class="line">postal,</span><br><span class="line">buslicensefilename,</span><br><span class="line">taxregcertificatefilename,</span><br><span class="line">orgcodefilename,</span><br><span class="line">taxnumber,</span><br><span class="line">bankname,</span><br><span class="line">remark,</span><br><span class="line">bidcount,</span><br><span class="line">packagecount,</span><br><span class="line">serviceid,</span><br><span class="line">statusflag,</span><br><span class="line">submitid,</span><br><span class="line">auditid,</span><br><span class="line">audittime,</span><br><span class="line">inserttime,</span><br><span class="line">updatetime,</span><br><span class="line">deletetime,</span><br><span class="line">mark,</span><br><span class="line"><span class="keyword">version</span>,</span><br><span class="line">tag,</span><br><span class="line">firstsuccessaudittime,</span><br><span class="line">isthreeinone,</span><br><span class="line">bankaccountnumber,</span><br><span class="line">financepayamount,</span><br><span class="line">audittype,</span><br><span class="line">systemremark,</span><br><span class="line">sysmodifydate,</span><br><span class="line">provinceid,</span><br><span class="line">cityid,</span><br><span class="line">areaid,</span><br><span class="line">platform,</span><br><span class="line">managementprovinceid,</span><br><span class="line">managementcityid,</span><br><span class="line">managementareaid,</span><br><span class="line">managementaddress,</span><br><span class="line">eventtime</span><br><span class="line">),</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> eventtime</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line"><span class="string">"desired_concurrent_number"</span>=<span class="string">"3"</span>,</span><br><span class="line"><span class="string">"max_batch_interval"</span>=<span class="string">"20"</span>,</span><br><span class="line"><span class="string">"max_batch_rows"</span>=<span class="string">"300000"</span>,</span><br><span class="line"><span class="string">"max_batch_size"</span>=<span class="string">"209715200"</span>,</span><br><span class="line"><span class="string">"strict_mode"</span>=<span class="string">"false"</span>,</span><br><span class="line"><span class="string">"format"</span>=<span class="string">"json"</span>,</span><br><span class="line"><span class="string">"jsonpaths"</span> = <span class="string">"[\"</span>$.id\<span class="string">",\"</span>$.accountId\<span class="string">",\"</span>$.typeId\<span class="string">",\"</span>$.name\<span class="string">",\"</span>$.legalPerson\<span class="string">",\"</span>$.telephone\<span class="string">",\"</span>$.registerAddress\<span class="string">",\"</span>$.address\<span class="string">",\"</span>$.postal\<span class="string">",\"</span>$.busLicenseFileName\<span class="string">",\"</span>$.taxRegCertificateFileName\<span class="string">",\"</span>$.orgCodeFileName\<span class="string">",\"</span>$.taxNumber\<span class="string">",\"</span>$.bankName\<span class="string">",\"</span>$.remark\<span class="string">",\"</span>$.bidCount\<span class="string">",\"</span>$.packageCount\<span class="string">",\"</span>$.serviceId\<span class="string">",\"</span>$.statusFlag\<span class="string">",\"</span>$.submitId\<span class="string">",\"</span>$.auditId\<span class="string">",\"</span>$.auditTime\<span class="string">",\"</span>$.insertTime\<span class="string">",\"</span>$.updateTime\<span class="string">",\"</span>$.deleteTime\<span class="string">",\"</span>$.mark\<span class="string">",\"</span>$.version\<span class="string">",\"</span>$.tag\<span class="string">",\"</span>$.firstSuccessAuditTime\<span class="string">",\"</span>$.isThreeInOne\<span class="string">",\"</span>$.bankAccountNumber\<span class="string">",\"</span>$.financePayAmount\<span class="string">",\"</span>$.auditType\<span class="string">",\"</span>$.systemRemark\<span class="string">",\"</span>$.sysModifyDate\<span class="string">",\"</span>$.provinceId\<span class="string">",\"</span>$.cityId\<span class="string">",\"</span>$.areaId\<span class="string">",\"</span>$.platform\<span class="string">",\"</span>$.managementProvinceId\<span class="string">",\"</span>$.managementCityId\<span class="string">",\"</span>$.managementAreaId\<span class="string">",\"</span>$.managementAddress\<span class="string">",\"</span>$.eventTime\<span class="string">"]"</span>,</span><br><span class="line"><span class="string">"strip_outer_array"</span> = <span class="string">"false"</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">FROM</span> KAFKA</span><br><span class="line">(</span><br><span class="line"><span class="string">"kafka_broker_list"</span>=<span class="string">"10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092"</span>,</span><br><span class="line"><span class="string">"kafka_topic"</span>=<span class="string">"ods_Homedo_t_Agent_Agent"</span>,</span><br><span class="line"><span class="string">"kafka_partitions"</span> = <span class="string">"0,1"</span>,</span><br><span class="line"><span class="string">"kafka_offsets"</span> = <span class="string">"3158,3160"</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h4 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h4><p>两个要点：</p>
<p>1.建表时要增加以下配置，这个DEMO使用的是Datetime，即使用时间来进行排序。</p>
<p>​    “function_column.sequence_type” = ‘Datetime’</p>
<p>​    指定了上述配置后，建表会自动创建一个隐藏字段用于排序</p>
<p>​    可以通过下面的命令来查看隐藏字段</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 当前session开启查看可以隐藏字段</span></span><br><span class="line">mysql&gt; SET show_hidden_columns=true;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看表结构</span></span><br><span class="line"><span class="comment">-- 可以看到隐藏字段  __DORIS_SEQUENCE_COL__ ，类型为 DATETIME</span></span><br><span class="line">mysql&gt; desc dwd_homedo_real.`dwd_agent_agent_test1` all;</span><br><span class="line">+<span class="comment">-----------------------+---------------+---------------------------+---------------+------+-------+---------+---------+---------+</span></span><br><span class="line">| IndexName             | IndexKeysType | Field                     | Type          | Null | Key   | Default | Extra   | Visible |</span><br><span class="line">+<span class="comment">-----------------------+---------------+---------------------------+---------------+------+-------+---------+---------+---------+</span></span><br><span class="line">| dwd_agent_agent_test1 | UNIQUE_KEYS   | id                        | BIGINT        | Yes  | true  | NULL    |         | true    |</span><br><span class="line">|                       |               | accountid                 | BIGINT        | Yes  | false | NULL    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | typeid                    | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | <span class="keyword">name</span>                      | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | legalperson               | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | telephone                 | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | registeraddress           | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | address                   | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | postal                    | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | buslicensefilename        | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | taxregcertificatefilename | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | orgcodefilename           | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | taxnumber                 | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | bankname                  | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | remark                    | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | bidcount                  | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | packagecount              | <span class="built_in">DECIMAL</span>(<span class="number">18</span>,<span class="number">2</span>) | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | serviceid                 | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | statusflag                | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | submitid                  | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | auditid                   | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | audittime                 | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | inserttime                | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | updatetime                | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | deletetime                | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | mark                      | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | <span class="keyword">version</span>                   | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | tag                       | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | firstsuccessaudittime     | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | isthreeinone              | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | bankaccountnumber         | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | financepayamount          | <span class="built_in">DECIMAL</span>(<span class="number">18</span>,<span class="number">2</span>) | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | audittype                 | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | systemremark              | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | sysmodifydate             | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | provinceid                | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | cityid                    | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | areaid                    | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | platform                  | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | managementprovinceid      | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | managementcityid          | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | managementareaid          | <span class="built_in">BIGINT</span>        | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | managementaddress         | <span class="built_in">VARCHAR</span>(<span class="number">500</span>)  | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | eventtime                 | DATETIME      | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">true</span>    |</span><br><span class="line">|                       |               | __DORIS_SEQUENCE_COL__    | DATETIME      | Yes  | <span class="literal">false</span> | <span class="literal">NULL</span>    | <span class="keyword">REPLACE</span> | <span class="literal">false</span>   |</span><br><span class="line">+<span class="comment">-----------------------+---------------+---------------------------+---------------+------+-------+---------+---------+---------+</span></span><br><span class="line"><span class="number">45</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure>

<p>2.Routine Load Task 需要指定 ORDER BY 的字段映射关系，这里使用的是ORDER BY eventtime。</p>
]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris 使用 sqoop 将 Doris 数据导入到 HDFS</title>
    <url>/2021/07/29/Doris%E4%BD%BF%E7%94%A8sqoop%E5%B0%86Doris%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0HDFS/</url>
    <content><![CDATA[<p>使用 sqoop 将 Doris 数据导入到 HDFS，</p>
<p>HDFS 数据更新到 Hive 表分区。</p>
<a id="more"></a>

<hr>
<h4 id="Doris-gt-HDFS-临时路径"><a href="#Doris-gt-HDFS-临时路径" class="headerlink" title="Doris -&gt; HDFS 临时路径"></a>Doris -&gt; HDFS 临时路径</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/cloudera/parcels/CDH/bin/sqoop-import \</span><br><span class="line">  --connect "jdbc:mysql://10.0.15.131:9030/dwd_homedo_ubt" \</span><br><span class="line">  --username root \</span><br><span class="line">  --password "root" \</span><br><span class="line">  --target-dir /opt/user/HomedoDB/warehouse/ods_cdp/mid_event_vd/d=20210729 \</span><br><span class="line">  --query "select distinct_id, xwho, xwhen, xwhat, os, utm_campaign_id, utm_campaign, utm_medium, dict_utm_medium, utm_source, dict_utm_source, </span><br><span class="line">               utm_content, utm_term, channel, referrer, referrer_domain, traffic_source_type, dict_traffic_source_type, search_engine, dict_search_engine, </span><br><span class="line">               search_keyword, social_share_from, scene, dict_scene, scene_type, dict_scene_type, platform, dict_platform, session_id, url, social_media, </span><br><span class="line">               dict_social_media, url_domain, track_xwhen, fingerprint, campaign_shortlink, offset, xwhat_id, need_update, network, os_version, is_first_day, </span><br><span class="line">               is_from_background, model, screen_width, is_first_time, brand, ip, screen_height, language, app_version, lib, is_time_calibrated, is_login, </span><br><span class="line">               lib_version, platform_extra, time_zone, manufacturer, country, province, city, title, duration, element_type, element_path, start_source, </span><br><span class="line">               element_content, element_id, carrier_name, element_position, parent_url, web_crawler, user_agent, browser_version, device_type, </span><br><span class="line">               dict_device_type, browser, startup_time, original_id, url_path, element_class_name, element_target_url, btn_name, type, content_type, </span><br><span class="line">               nav_name, \`desc\`, is_success, method, user_id, source_position_name, source_module, rank, name, module_name, link_page_url, ad_name, </span><br><span class="line">               ad_id, location, element_name, third_category, news_id, news_name, event_name, second_category, first_category, price, item_id, item_name, </span><br><span class="line">               item_count, keyword_type, min_buy_unit, review_num, list_price, event_id, reason, brand_id, shop_name, shop_id, brand_name, video_name, </span><br><span class="line">               video_id, is_filter, item_code, item_specific_module, shop_num, recommend_item_num, page_num, filter_detail, filter_type, crash_data, </span><br><span class="line">               string_price, second_category_id, stock_status_pro, first_category_id, stock_status, shelf_status, item_u8_code, string_price_pro, </span><br><span class="line">               third_category_id, min_buy_num, string_stp_price2, string_stp_price1, string_stp_price3, string_stp_num3, string_stp_num2, string_stp_num1, </span><br><span class="line">               string_stp_price4, string_stp_num4, string_stp_price5, string_stp_num5, pic_num, integral_use_amount, shipping_cost, orgcode, credit_period_type, </span><br><span class="line">               is_wait_deliver, insert_time, actual_pay_amount, coupon_freight_use_amount, is_negotiation, package_use_amount, discount_use_amount, </span><br><span class="line">               channel_type, order_id, category_code, payment_time, is_status, payment_method, per_pay_amount, is_first_order, status, smb_pay_time, order_amount, </span><br><span class="line">               confirm_time, action, viewport_position, viewport_width, viewport_height, event_duration, deal_amount, live_name, live_id, debug, news_channel, </span><br><span class="line">               news_subject, radio_num, plan_name, plan_id, click_content_rank, activity_tag, string_list_price, is_delete, device_id, oaid, imei, android_id, </span><br><span class="line">               element_clickable, click_x, click_y, page_height, element_y, element_x, page_width, push_title, activityid, activityname, activitytype, virtualurl, </span><br><span class="line">               prop_0, prop_1, prop_2, prop_3, prop_4, dict_platform_extra, page_name, sms_channel, message_id, ea_cohort_code, ad_space_type, ad_space_name, </span><br><span class="line">               grade1, ad_space_id, dese, item_type, package_num, package_id, pagestaytime, is_ad_sku, sku_rank, package_name, url_domain2, search_scene,</span><br><span class="line">               share_source_id, static_type</span><br><span class="line">           from dwd_homedo_ubt.dwd_log_event</span><br><span class="line">           where date_d = '2021-07-29'</span><br><span class="line">           and \$CONDITIONS" \</span><br><span class="line">  --delete-target-dir \</span><br><span class="line">  --fields-terminated-by '\001' \</span><br><span class="line">  --hive-delims-replacement '' \</span><br><span class="line">  --null-string '\\N' \</span><br><span class="line">  --null-non-string '\\N' \</span><br><span class="line">  --m 1</span><br></pre></td></tr></table></figure>

<h4 id="Hive-临时表-gt-Hive-正式表"><a href="#Hive-临时表-gt-Hive-正式表" class="headerlink" title="Hive 临时表 -&gt; Hive 正式表"></a>Hive 临时表 -&gt; Hive 正式表</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive -e "set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">         set hive.execution.engine=mr;</span><br><span class="line">         msck repair table ods_cdp.mid_event_vd;</span><br><span class="line">         insert overwrite table ods_cdp.event_vd</span><br><span class="line">         partition(d)</span><br><span class="line">         select distinct_id, xwho, xwhen, xwhat, os, utm_campaign_id, utm_campaign, utm_medium, dict_utm_medium, utm_source, dict_utm_source, </span><br><span class="line">               utm_content, utm_term, channel, referrer, referrer_domain, traffic_source_type, dict_traffic_source_type, search_engine, dict_search_engine, </span><br><span class="line">               search_keyword, social_share_from, scene, dict_scene, scene_type, dict_scene_type, platform, dict_platform, session_id, url, social_media, </span><br><span class="line">               dict_social_media, url_domain, track_xwhen, fingerprint, campaign_shortlink, offset, xwhat_id, need_update, network, os_version, is_first_day, </span><br><span class="line">               is_from_background, model, screen_width, is_first_time, brand, ip, screen_height, language, app_version, lib, is_time_calibrated, is_login, </span><br><span class="line">               lib_version, platform_extra, time_zone, manufacturer, country, province, city, title, duration, element_type, element_path, start_source, </span><br><span class="line">               element_content, element_id, carrier_name, element_position, parent_url, web_crawler, user_agent, browser_version, device_type, </span><br><span class="line">               dict_device_type, browser, startup_time, original_id, url_path, element_class_name, element_target_url, btn_name, type, content_type, </span><br><span class="line">               nav_name, desc, is_success, method, user_id, source_position_name, source_module, rank, name, module_name, link_page_url, ad_name, </span><br><span class="line">               ad_id, location, element_name, third_category, news_id, news_name, event_name, second_category, first_category, price, item_id, item_name, </span><br><span class="line">               item_count, keyword_type, min_buy_unit, review_num, list_price, event_id, reason, brand_id, shop_name, shop_id, brand_name, video_name, </span><br><span class="line">               video_id, is_filter, item_code, item_specific_module, shop_num, recommend_item_num, page_num, filter_detail, filter_type, crash_data, </span><br><span class="line">               string_price, second_category_id, stock_status_pro, first_category_id, stock_status, shelf_status, item_u8_code, string_price_pro, </span><br><span class="line">               third_category_id, min_buy_num, string_stp_price2, string_stp_price1, string_stp_price3, string_stp_num3, string_stp_num2, string_stp_num1, </span><br><span class="line">               string_stp_price4, string_stp_num4, string_stp_price5, string_stp_num5, pic_num, integral_use_amount, shipping_cost, orgcode, credit_period_type, </span><br><span class="line">               is_wait_deliver, insert_time, actual_pay_amount, coupon_freight_use_amount, is_negotiation, package_use_amount, discount_use_amount, </span><br><span class="line">               channel_type, order_id, category_code, payment_time, is_status, payment_method, per_pay_amount, is_first_order, status, smb_pay_time, order_amount, </span><br><span class="line">               confirm_time, action, viewport_position, viewport_width, viewport_height, event_duration, deal_amount, live_name, live_id, debug, news_channel, </span><br><span class="line">               news_subject, radio_num, plan_name, plan_id, click_content_rank, activity_tag, string_list_price, is_delete, device_id, oaid, imei, android_id, </span><br><span class="line">               element_clickable, click_x, click_y, page_height, element_y, element_x, page_width, push_title, activityid, activityname, activitytype, virtualurl, </span><br><span class="line">               prop_0, prop_1, prop_2, prop_3, prop_4, dict_platform_extra, page_name, sms_channel, message_id, ea_cohort_code, ad_space_type, ad_space_name, </span><br><span class="line">               grade1, ad_space_id, dese, item_type, package_num, package_id, pagestaytime, is_ad_sku, sku_rank, package_name, url_domain2, search_scene,</span><br><span class="line">               share_source_id, static_type, d</span><br><span class="line">           from ods_cdp.mid_event_vd</span><br><span class="line">           where d = '20210729';"</span><br></pre></td></tr></table></figure>

<h4 id="删除临时表分区数据"><a href="#删除临时表分区数据" class="headerlink" title="删除临时表分区数据"></a>删除临时表分区数据</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive -e "<span class="keyword">alter</span> <span class="keyword">table</span> ods_cdp.mid_event_vd <span class="keyword">drop</span> <span class="keyword">partition</span> (d = <span class="string">'20210729'</span>);"</span><br></pre></td></tr></table></figure>



<h4 id="自动化调度脚本"><a href="#自动化调度脚本" class="headerlink" title="自动化调度脚本"></a>自动化调度脚本</h4><h5 id="azkaban调度任务Zip内容"><a href="#azkaban调度任务Zip内容" class="headerlink" title="azkaban调度任务Zip内容"></a>azkaban调度任务Zip内容</h5><p>azkaban.project</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure>

<p>sync_ubt.flow</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sqoop_event_vd</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">      <span class="attr">config:</span></span><br><span class="line">        <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/opt/sync/sync_script/ods_cdp/sqoop_event_vd.sh</span></span><br></pre></td></tr></table></figure>

<p>sqoop_event_vd.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">deleteDate=`date -d "1 day ago" +%Y%m%d`</span><br><span class="line"></span><br><span class="line">for ((i=1; i&lt;=11; i++))</span><br><span class="line"></span><br><span class="line">do</span><br><span class="line"></span><br><span class="line">  dorisDate=`date -d "$i day ago" +%Y-%m-%d`</span><br><span class="line"></span><br><span class="line">  hiveDate=`date -d "$i day ago" +%Y%m%d`</span><br><span class="line">  </span><br><span class="line">  /opt/cloudera/parcels/CDH/bin/sqoop-import \</span><br><span class="line">  --connect "jdbc:mysql://10.0.15.131:9030/dwd_homedo_ubt" \</span><br><span class="line">  --username root \</span><br><span class="line">  --password "root" \</span><br><span class="line">  --target-dir /opt/user/HomedoDB/warehouse/ods_cdp/mid_event_vd/d=$hiveDate \</span><br><span class="line">  --query "select distinct_id, xwho, xwhen, xwhat, os, utm_campaign_id, utm_campaign, utm_medium, dict_utm_medium, utm_source, dict_utm_source, </span><br><span class="line">               utm_content, utm_term, channel, referrer, referrer_domain, traffic_source_type, dict_traffic_source_type, search_engine, dict_search_engine, </span><br><span class="line">               search_keyword, social_share_from, scene, dict_scene, scene_type, dict_scene_type, platform, dict_platform, session_id, url, social_media, </span><br><span class="line">               dict_social_media, url_domain, track_xwhen, fingerprint, campaign_shortlink, offset, xwhat_id, need_update, network, os_version, is_first_day, </span><br><span class="line">               is_from_background, model, screen_width, is_first_time, brand, ip, screen_height, language, app_version, lib, is_time_calibrated, is_login, </span><br><span class="line">               lib_version, platform_extra, time_zone, manufacturer, country, province, city, title, duration, element_type, element_path, start_source, </span><br><span class="line">               element_content, element_id, carrier_name, element_position, parent_url, web_crawler, user_agent, browser_version, device_type, </span><br><span class="line">               dict_device_type, browser, startup_time, original_id, url_path, element_class_name, element_target_url, btn_name, type, content_type, </span><br><span class="line">               nav_name, \`desc\`, is_success, method, user_id, source_position_name, source_module, rank, name, module_name, link_page_url, ad_name, </span><br><span class="line">               ad_id, location, element_name, third_category, news_id, news_name, event_name, second_category, first_category, price, item_id, item_name, </span><br><span class="line">               item_count, keyword_type, min_buy_unit, review_num, list_price, event_id, reason, brand_id, shop_name, shop_id, brand_name, video_name, </span><br><span class="line">               video_id, is_filter, item_code, item_specific_module, shop_num, recommend_item_num, page_num, filter_detail, filter_type, crash_data, </span><br><span class="line">               string_price, second_category_id, stock_status_pro, first_category_id, stock_status, shelf_status, item_u8_code, string_price_pro, </span><br><span class="line">               third_category_id, min_buy_num, string_stp_price2, string_stp_price1, string_stp_price3, string_stp_num3, string_stp_num2, string_stp_num1, </span><br><span class="line">               string_stp_price4, string_stp_num4, string_stp_price5, string_stp_num5, pic_num, integral_use_amount, shipping_cost, orgcode, credit_period_type, </span><br><span class="line">               is_wait_deliver, insert_time, actual_pay_amount, coupon_freight_use_amount, is_negotiation, package_use_amount, discount_use_amount, </span><br><span class="line">               channel_type, order_id, category_code, payment_time, is_status, payment_method, per_pay_amount, is_first_order, status, smb_pay_time, order_amount, </span><br><span class="line">               confirm_time, action, viewport_position, viewport_width, viewport_height, event_duration, deal_amount, live_name, live_id, debug, news_channel, </span><br><span class="line">               news_subject, radio_num, plan_name, plan_id, click_content_rank, activity_tag, string_list_price, is_delete, device_id, oaid, imei, android_id, </span><br><span class="line">               element_clickable, click_x, click_y, page_height, element_y, element_x, page_width, push_title, activityid, activityname, activitytype, virtualurl, </span><br><span class="line">               prop_0, prop_1, prop_2, prop_3, prop_4, dict_platform_extra, page_name, sms_channel, message_id, ea_cohort_code, ad_space_type, ad_space_name, </span><br><span class="line">               grade1, ad_space_id, dese, item_type, package_num, package_id, pagestaytime, is_ad_sku, sku_rank, package_name, url_domain2, search_scene,</span><br><span class="line">               share_source_id, static_type</span><br><span class="line">           from dwd_homedo_ubt.dwd_log_event</span><br><span class="line">           where date_d = '$dorisDate'</span><br><span class="line">           and \$CONDITIONS" \</span><br><span class="line">  --delete-target-dir \</span><br><span class="line">  --fields-terminated-by '\001' \</span><br><span class="line">  --hive-delims-replacement '' \</span><br><span class="line">  --null-string '\\N' \</span><br><span class="line">  --null-non-string '\\N' \</span><br><span class="line">  --m 1</span><br><span class="line">  </span><br><span class="line">  hive -e "set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">           set hive.execution.engine=mr;</span><br><span class="line">           msck repair table ods_cdp.mid_event_vd;</span><br><span class="line">           insert overwrite table ods_cdp.event_vd</span><br><span class="line">           partition(d)</span><br><span class="line">           select distinct_id, xwho, xwhen, xwhat, os, utm_campaign_id, utm_campaign, utm_medium, dict_utm_medium, utm_source, dict_utm_source, </span><br><span class="line">               utm_content, utm_term, channel, referrer, referrer_domain, traffic_source_type, dict_traffic_source_type, search_engine, dict_search_engine, </span><br><span class="line">               search_keyword, social_share_from, scene, dict_scene, scene_type, dict_scene_type, platform, dict_platform, session_id, url, social_media, </span><br><span class="line">               dict_social_media, url_domain, track_xwhen, fingerprint, campaign_shortlink, offset, xwhat_id, need_update, network, os_version, is_first_day, </span><br><span class="line">               is_from_background, model, screen_width, is_first_time, brand, ip, screen_height, language, app_version, lib, is_time_calibrated, is_login, </span><br><span class="line">               lib_version, platform_extra, time_zone, manufacturer, country, province, city, title, duration, element_type, element_path, start_source, </span><br><span class="line">               element_content, element_id, carrier_name, element_position, parent_url, web_crawler, user_agent, browser_version, device_type, </span><br><span class="line">               dict_device_type, browser, startup_time, original_id, url_path, element_class_name, element_target_url, btn_name, type, content_type, </span><br><span class="line">               nav_name, desc, is_success, method, user_id, source_position_name, source_module, rank, name, module_name, link_page_url, ad_name, </span><br><span class="line">               ad_id, location, element_name, third_category, news_id, news_name, event_name, second_category, first_category, price, item_id, item_name, </span><br><span class="line">               item_count, keyword_type, min_buy_unit, review_num, list_price, event_id, reason, brand_id, shop_name, shop_id, brand_name, video_name, </span><br><span class="line">               video_id, is_filter, item_code, item_specific_module, shop_num, recommend_item_num, page_num, filter_detail, filter_type, crash_data, </span><br><span class="line">               string_price, second_category_id, stock_status_pro, first_category_id, stock_status, shelf_status, item_u8_code, string_price_pro, </span><br><span class="line">               third_category_id, min_buy_num, string_stp_price2, string_stp_price1, string_stp_price3, string_stp_num3, string_stp_num2, string_stp_num1, </span><br><span class="line">               string_stp_price4, string_stp_num4, string_stp_price5, string_stp_num5, pic_num, integral_use_amount, shipping_cost, orgcode, credit_period_type, </span><br><span class="line">               is_wait_deliver, insert_time, actual_pay_amount, coupon_freight_use_amount, is_negotiation, package_use_amount, discount_use_amount, </span><br><span class="line">               channel_type, order_id, category_code, payment_time, is_status, payment_method, per_pay_amount, is_first_order, status, smb_pay_time, order_amount, </span><br><span class="line">               confirm_time, action, viewport_position, viewport_width, viewport_height, event_duration, deal_amount, live_name, live_id, debug, news_channel, </span><br><span class="line">               news_subject, radio_num, plan_name, plan_id, click_content_rank, activity_tag, string_list_price, is_delete, device_id, oaid, imei, android_id, </span><br><span class="line">               element_clickable, click_x, click_y, page_height, element_y, element_x, page_width, push_title, activityid, activityname, activitytype, virtualurl, </span><br><span class="line">               prop_0, prop_1, prop_2, prop_3, prop_4, dict_platform_extra, page_name, sms_channel, message_id, ea_cohort_code, ad_space_type, ad_space_name, </span><br><span class="line">               grade1, ad_space_id, dese, item_type, package_num, package_id, pagestaytime, is_ad_sku, sku_rank, package_name, url_domain2, search_scene,</span><br><span class="line">               share_source_id, static_type, d</span><br><span class="line">           from ods_cdp.mid_event_vd</span><br><span class="line">           where d = '$hiveDate';"</span><br><span class="line"></span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">hive -e "alter table ods_cdp.mid_event_vd drop partition (d &lt; '$deleteDate');"</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris Kafka Load</title>
    <url>/2021/05/17/DorisKafkaLoad/</url>
    <content><![CDATA[<p>实时消费 Kafka 某 Topic 数据到 Doris 表 Task 创建方法及案例。</p>
<a id="more"></a>
<hr>
<h2 id="Kafka-Topic创建"><a href="#Kafka-Topic创建" class="headerlink" title="Kafka Topic创建"></a>Kafka Topic创建</h2><p>Kafka Topic : dorisDemo</p>
<h3 id="创建Kafka-Topic"><a href="#创建Kafka-Topic" class="headerlink" title="创建Kafka Topic"></a>创建Kafka Topic</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kafka-topics --create --zookeeper cdh9:2181&#x2F;kafka --partitions 3 --replication-factor 2 --topic dorisDemo</span><br></pre></td></tr></table></figure>

<h3 id="查看Kafka-Topic"><a href="#查看Kafka-Topic" class="headerlink" title="查看Kafka Topic"></a>查看Kafka Topic</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kafka-topics --zookeeper cdh9:2181&#x2F;kafka --list</span><br></pre></td></tr></table></figure>

<h2 id="创建Doris导入任务"><a href="#创建Doris导入任务" class="headerlink" title="创建Doris导入任务"></a>创建Doris导入任务</h2><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE ROUTINE LOAD [database.][job_name] ON [table_name]</span><br><span class="line">[COLUMN TERMINATED BY &quot;column_separator&quot; ,]</span><br><span class="line">[COLUMN (col1, col2, ...) ,]</span><br><span class="line">[WHERE where_condition ,]</span><br><span class="line">[PARTITION (part1, part2, ...)]</span><br><span class="line">[PROPERTIES (&quot;key&quot; &#x3D; &quot;value&quot;, ...)]</span><br><span class="line">FROM [DATA_SOURCE]</span><br><span class="line">[(data_source_properties1 &#x3D; &#39;value1&#39;,</span><br><span class="line">  data_source_properties2 &#x3D; &#39;value2&#39;,</span><br><span class="line">  ...)]</span><br></pre></td></tr></table></figure>

<h3 id="sql案例"><a href="#sql案例" class="headerlink" title="sql案例"></a>sql案例</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE ROUTINE LOAD routine_load_kafka ON demo_kafka_load_detail</span><br><span class="line">COLUMNS TERMINATED BY &quot;,&quot;,</span><br><span class="line">COLUMNS (a, b, c)</span><br><span class="line">PROPERTIES(</span><br><span class="line">    &quot;desired_concurrent_number&quot;&#x3D;&quot;1&quot;,</span><br><span class="line">    &quot;max_error_number&quot;&#x3D;&quot;1000&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA(</span><br><span class="line">    &quot;kafka_broker_list&quot;&#x3D;&quot;10.0.15.131:9092&quot;,</span><br><span class="line">    &quot;kafka_topic&quot;&#x3D;&quot;dorisDemo&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-7-19.png" alt></p>
<h3 id="查看任务状态"><a href="#查看任务状态" class="headerlink" title="查看任务状态"></a>查看任务状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW ALL ROUTINE LOAD \G</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-12-7.png" alt></p>
<h3 id="手动向Kafka生产数据"><a href="#手动向Kafka生产数据" class="headerlink" title="手动向Kafka生产数据"></a>手动向Kafka生产数据</h3><p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-13-7.png" alt></p>
<p>查询Doris load表数据</p>
<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-13-37.png" alt></p>
<p>查看任务状态</p>
<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-15-0.png" alt></p>
<h2 id="Doris-ROUTINE-LOAD-操作"><a href="#Doris-ROUTINE-LOAD-操作" class="headerlink" title="Doris ROUTINE LOAD 操作"></a>Doris ROUTINE LOAD 操作</h2><h3 id="暂停导入任务"><a href="#暂停导入任务" class="headerlink" title="暂停导入任务"></a>暂停导入任务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PAUSE ROUTINE LOAD FOR [job_name];</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-18-53.png" alt></p>
<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-19-33.png" alt></p>
<h3 id="恢复导入任务"><a href="#恢复导入任务" class="headerlink" title="恢复导入任务"></a>恢复导入任务</h3><p>使用RESUME语句后，任务会短暂的进入 NEED_SCHEDULE 状态，表示任务正在重新调度，一段时间</p>
<p>后会重新恢复至 RUNING 状态，继续导入数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RESUME ROUTINE LOAD FOR [job_name];</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-20-12.png" alt></p>
<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-21-32.png" alt></p>
<p><img src="/2021/05/17/DorisKafkaLoad/image2021-4-13_18-21-48.png" alt></p>
<h3 id="停止导入任务"><a href="#停止导入任务" class="headerlink" title="停止导入任务"></a>停止导入任务</h3><p>使用STOP语句后，此时导入任务进入 STOP 状态，数据停止导入，任务消亡，无法恢复数据导入。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">STOP ROUTINE &#96;&#96;LOAD&#96; &#96;FOR&#96; &#96;[job_name];</span><br></pre></td></tr></table></figure>

<h2 id="Doris-Routine-Load-Task-DEMO-案例"><a href="#Doris-Routine-Load-Task-DEMO-案例" class="headerlink" title="Doris Routine Load Task DEMO 案例"></a>Doris Routine Load Task DEMO 案例</h2><h3 id="DEMO1：Json字段和Doris-Table字段需要顺序一致的情况"><a href="#DEMO1：Json字段和Doris-Table字段需要顺序一致的情况" class="headerlink" title="DEMO1：Json字段和Doris Table字段需要顺序一致的情况"></a>DEMO1：Json字段和Doris Table字段需要顺序一致的情况</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE ROUTINE LOAD example_db.kafka_json_to_doris_2 ON demo_ubt_detail</span><br><span class="line">COLUMNS(xwhen,appid,xwho,xwhat,ds,$country,$province,$city,$browser_version,$os_version,$model,$os,$brand,$device_type,$browser,xwhat_id,isChangeXwho,distinct_id,offset,$web_crawler,$is_first_day,$screen_width,$is_first_time,$platform,$ip,$screen_height,$user_agent,$language,$lib,$debug,$is_login,$is_time_calibrated,$lib_version,platform_extra,$time_zone,$session_id)</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">&quot;desired_concurrent_number&quot;&#x3D;&quot;3&quot;,</span><br><span class="line">&quot;max_batch_interval&quot; &#x3D; &quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot; &#x3D; &quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot; &#x3D; &quot;209715200&quot;,</span><br><span class="line">&quot;strict_mode&quot; &#x3D; &quot;false&quot;,</span><br><span class="line">&quot;format&quot; &#x3D; &quot;json&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA</span><br><span class="line">(</span><br><span class="line">&quot;kafka_broker_list&quot; &#x3D; &quot;10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092&quot;,</span><br><span class="line">&quot;kafka_topic&quot; &#x3D; &quot;ubtDoris1&quot;,</span><br><span class="line">&quot;property.kafka_default_offsets&quot; &#x3D; &quot;OFFSET_BEGINNING&quot;,</span><br><span class="line">&quot;property.auto.offset.reset&quot; &#x3D; &quot;earliest&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="DEMO2：Json字段和Doris-Table字段顺序不一致的情况"><a href="#DEMO2：Json字段和Doris-Table字段顺序不一致的情况" class="headerlink" title="DEMO2：Json字段和Doris Table字段顺序不一致的情况"></a>DEMO2：Json字段和Doris Table字段顺序不一致的情况</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE ROUTINE LOAD dwd_homedo_real.dwd_smb_account_info ON dwd_smb_account_info</span><br><span class="line">COLUMNS(user_id, createtime, agentId, login_type, accountName, mobile, login_account, accountId, roleIdAudit, modified_time, roleIds, login_password, accountRealName, disabled, otherPermissions, operatorId, email, fullFileName, memberId)</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">&quot;desired_concurrent_number&quot;&#x3D;&quot;3&quot;,</span><br><span class="line">&quot;max_batch_interval&quot;&#x3D;&quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot;&#x3D;&quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot;&#x3D;&quot;209715200&quot;,</span><br><span class="line">&quot;strict_mode&quot;&#x3D;&quot;false&quot;,</span><br><span class="line">&quot;format&quot;&#x3D;&quot;json&quot;,</span><br><span class="line">&quot;jsonpaths&quot; &#x3D; &quot;[\&quot;$.user_id\&quot;,\&quot;$.createtime\&quot;,\&quot;$.agentId\&quot;,\&quot;$.login_type\&quot;,\&quot;$.accountName\&quot;,\&quot;$.mobile\&quot;,\&quot;$.login_account\&quot;,\&quot;$.accountId\&quot;,\&quot;$.roleIdAudit\&quot;,\&quot;$.modified_time\&quot;,\&quot;$.roleIds\&quot;,\&quot;$.login_password\&quot;,\&quot;$.accountRealName\&quot;,\&quot;$.disabled\&quot;,\&quot;$.otherPermissions\&quot;,\&quot;$.operatorId\&quot;,\&quot;$.email\&quot;,\&quot;$.fullFileName\&quot;,\&quot;$.memberId\&quot;]&quot;,</span><br><span class="line">&quot;strip_outer_array&quot; &#x3D; &quot;false&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA</span><br><span class="line">(</span><br><span class="line">&quot;kafka_broker_list&quot; &#x3D; &quot;10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092&quot;,</span><br><span class="line">&quot;kafka_topic&quot; &#x3D; &quot;ods_homedo_shop_sysuser_account&quot;,</span><br><span class="line">&quot;property.kafka_default_offsets&quot; &#x3D; &quot;OFFSET_BEGINNING&quot;,</span><br><span class="line">&quot;property.auto.offset.reset&quot; &#x3D; &quot;earliest&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="DEMO3：从Json字段取部分字段并做筛选后到Doris"><a href="#DEMO3：从Json字段取部分字段并做筛选后到Doris" class="headerlink" title="DEMO3：从Json字段取部分字段并做筛选后到Doris"></a>DEMO3：从Json字段取部分字段并做筛选后到Doris</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- Json 数据</span><br><span class="line">&#123;</span><br><span class="line">  &quot;id&quot;: 7247316,</span><br><span class="line">  &quot;tagGroupId&quot;: 30,</span><br><span class="line">  &quot;tagId&quot;: 5619893,</span><br><span class="line">  &quot;promotionId&quot;: 27039,</span><br><span class="line">  &quot;insertTime&quot;: &quot;2021-05-17 14:35:03.037&quot;,</span><br><span class="line">  &quot;updateTime&quot;: &quot;2021-05-17 14:35:03.037&quot;,</span><br><span class="line">  &quot;deleteTime&quot;: &quot;1900-01-01 00:00:00.0&quot;,</span><br><span class="line">  &quot;mark&quot;: 1,</span><br><span class="line">  &quot;version&quot;: 0,</span><br><span class="line">  &quot;sysModifyDate&quot;: 5634101229,</span><br><span class="line">  &quot;tagValue&quot;: &quot;明星品&quot;,</span><br><span class="line">  &quot;eventTime&quot;: &quot;2021-05-17 14:35:04.603&quot;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">-- Doris Table DDL</span><br><span class="line">DROP TABLE IF EXISTS dwd_homedo_real.dwd_homedo_promotion_flag;</span><br><span class="line">CREATE TABLE IF NOT EXISTS dwd_homedo_real.dwd_homedo_promotion_flag(</span><br><span class="line">    promotionId   VARCHAR(1000) NULL,</span><br><span class="line">    tagGroupId    VARCHAR(1000) NULL,</span><br><span class="line">    tagId         VARCHAR(1000) NULL,</span><br><span class="line">    tagValue      VARCHAR(1000) NULL,</span><br><span class="line">    mark          VARCHAR(1000) NULL</span><br><span class="line">)</span><br><span class="line">DUPLICATE KEY(promotionId)</span><br><span class="line">DISTRIBUTED BY HASH(promotionId) BUCKETS 3;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">-- Routine Load SQL</span><br><span class="line">CREATE ROUTINE LOAD dwd_homedo_real.dwd_homedo_promotion_flag ON dwd_homedo_promotion_flag</span><br><span class="line">COLUMNS(promotionId, tagGroupId, tagId, tagValue, mark),</span><br><span class="line">WHERE mark &gt; 0</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">&quot;desired_concurrent_number&quot;&#x3D;&quot;3&quot;,</span><br><span class="line">&quot;max_batch_interval&quot;&#x3D;&quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot;&#x3D;&quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot;&#x3D;&quot;209715200&quot;,</span><br><span class="line">&quot;strict_mode&quot;&#x3D;&quot;false&quot;,</span><br><span class="line">&quot;format&quot;&#x3D;&quot;json&quot;,</span><br><span class="line">&quot;jsonpaths&quot; &#x3D; &quot;[\&quot;$.promotionId\&quot;,\&quot;$.tagGroupId\&quot;,\&quot;$.tagId\&quot;,\&quot;$.tagValue\&quot;,\&quot;$.mark\&quot;]&quot;,</span><br><span class="line">&quot;strip_outer_array&quot; &#x3D; &quot;false&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA</span><br><span class="line">(</span><br><span class="line">&quot;kafka_broker_list&quot;&#x3D;&quot;10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092&quot;,</span><br><span class="line">&quot;kafka_topic&quot;&#x3D;&quot;ods_Homedo_t_Tag_Promotion&quot;,</span><br><span class="line">&quot;property.kafka_default_offsets&quot; &#x3D; &quot;OFFSET_BEGINNING&quot;,</span><br><span class="line">&quot;property.auto.offset.reset&quot; &#x3D; &quot;earliest&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h2 id="Doris-Routine-Load-官方帮助文档"><a href="#Doris-Routine-Load-官方帮助文档" class="headerlink" title="Doris Routine Load 官方帮助文档"></a>Doris Routine Load 官方帮助文档</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Name: &#39;ROUTINE LOAD&#39;</span><br><span class="line">Description:</span><br><span class="line">例行导入（Routine Load）功能，支持用户提交一个常驻的导入任务，通过不断的从指定的数据源读取数据，将数据导入到 Doris 中。</span><br><span class="line">目前仅支持通过无认证或者 SSL 认证方式，从 Kakfa 导入文本格式（CSV）的数据。</span><br><span class="line">语法：</span><br><span class="line">CREATE ROUTINE LOAD [db.]job_name ON tbl_name</span><br><span class="line">[load_properties]</span><br><span class="line">[job_properties]</span><br><span class="line">FROM data_source</span><br><span class="line">[data_source_properties]</span><br><span class="line">1. [db.]job_name</span><br><span class="line">导入作业的名称，在同一个 database 内，相同名称只能有一个 job 在运行。</span><br><span class="line">2. tbl_name</span><br><span class="line">指定需要导入的表的名称。</span><br><span class="line">3. load_properties</span><br><span class="line">用于描述导入数据。语法：</span><br><span class="line">[column_separator],</span><br><span class="line">[columns_mapping],</span><br><span class="line">[where_predicates],</span><br><span class="line">[partitions]</span><br><span class="line">1. column_separator:</span><br><span class="line">指定列分隔符，如：</span><br><span class="line">COLUMNS TERMINATED BY &quot;,&quot;</span><br><span class="line">默认为：\t</span><br><span class="line">2. columns_mapping:</span><br><span class="line">指定源数据中列的映射关系，以及定义衍生列的生成方式。</span><br><span class="line">1. 映射列：</span><br><span class="line">按顺序指定，源数据中各个列，对应目的表中的哪些列。对于希望跳过的列，可以指定一个不存在的列名。</span><br><span class="line">假设目的表有三列 k1, k2, v1。源数据有4列，其中第1、2、4列分别对应 k2, k1, v1。则书写如下：</span><br><span class="line">COLUMNS (k2, k1, xxx, v1)</span><br><span class="line">其中 xxx 为不存在的一列，用于跳过源数据中的第三列。</span><br><span class="line">2. 衍生列：</span><br><span class="line">以 col_name &#x3D; expr 的形式表示的列，我们称为衍生列。即支持通过 expr 计算得出目的表中对应列的值。</span><br><span class="line">衍生列通常排列在映射列之后，虽然这不是强制的规定，但是 Doris 总是先解析映射列，再解析衍生列。</span><br><span class="line">接上一个示例，假设目的表还有第4列 v2，v2 由 k1 和 k2 的和产生。则可以书写如下：</span><br><span class="line">COLUMNS (k2, k1, xxx, v1, v2 &#x3D; k1 + k2);</span><br><span class="line">3. where_predicates</span><br><span class="line">用于指定过滤条件，以过滤掉不需要的列。过滤列可以是映射列或衍生列。</span><br><span class="line">例如我们只希望导入 k1 大于 100 并且 k2 等于 1000 的列，则书写如下：</span><br><span class="line">WHERE k1 &gt; 100 and k2 &#x3D; 1000</span><br><span class="line">4. partitions</span><br><span class="line">指定导入目的表的哪些 partition 中。如果不指定，则会自动导入到对应的 partition 中。</span><br><span class="line">示例：</span><br><span class="line">PARTITION(p1, p2, p3)</span><br><span class="line">4. job_properties</span><br><span class="line">用于指定例行导入作业的通用参数。</span><br><span class="line">语法：</span><br><span class="line">PROPERTIES (</span><br><span class="line">&quot;key1&quot; &#x3D; &quot;val1&quot;,</span><br><span class="line">&quot;key2&quot; &#x3D; &quot;val2&quot;</span><br><span class="line">)</span><br><span class="line">目前我们支持以下参数：</span><br><span class="line">1. desired_concurrent_number</span><br><span class="line">期望的并发度。一个例行导入作业会被分成多个子任务执行。这个参数指定一个作业最多有多少任务可以同时执行。必须大于0。默认为3。</span><br><span class="line">这个并发度并不是实际的并发度，实际的并发度，会通过集群的节点数、负载情况，以及数据源的情况综合考虑。</span><br><span class="line">例：</span><br><span class="line">&quot;desired_concurrent_number&quot; &#x3D; &quot;3&quot;</span><br><span class="line">2. max_batch_interval&#x2F;max_batch_rows&#x2F;max_batch_size</span><br><span class="line">这三个参数分别表示：</span><br><span class="line">1）每个子任务最大执行时间，单位是秒。范围为 5 到 60。默认为10。</span><br><span class="line">2）每个子任务最多读取的行数。必须大于等于200000。默认是200000。</span><br><span class="line">3）每个子任务最多读取的字节数。单位是字节，范围是 100MB 到 1GB。默认是 100MB。</span><br><span class="line">这三个参数，用于控制一个子任务的执行时间和处理量。当任意一个达到阈值，则任务结束。</span><br><span class="line">例：</span><br><span class="line">&quot;max_batch_interval&quot; &#x3D; &quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot; &#x3D; &quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot; &#x3D; &quot;209715200&quot;</span><br><span class="line">3. max_error_number</span><br><span class="line">采样窗口内，允许的最大错误行数。必须大于等于0。默认是 0，即不允许有错误行。</span><br><span class="line">采样窗口为 max_batch_rows * 10。即如果在采样窗口内，错误行数大于 max_error_number，则会导致例行作业被暂停，需要人工介入检查数据质量问题。</span><br><span class="line">被 where 条件过滤掉的行不算错误行。</span><br><span class="line">4. strict_mode</span><br><span class="line">是否开启严格模式，默认为关闭。如果开启后，非空原始数据的列类型变换如果结果为 NULL，则会被过滤。指定方式为 &quot;strict_mode&quot; &#x3D; &quot;true&quot;</span><br><span class="line">5. timezone</span><br><span class="line">指定导入作业所使用的时区。默认为使用 Session 的 timezone 参数。该参数会影响所有导入涉及的和时区有关的函数结果。</span><br><span class="line">6. format</span><br><span class="line">指定导入数据格式，默认是csv，支持json格式。</span><br><span class="line">7. jsonpaths</span><br><span class="line">jsonpaths: 导入json方式分为：简单模式和匹配模式。如果设置了jsonpath则为匹配模式导入，否则为简单模式导入，具体可参考示例。</span><br><span class="line">8. strip_outer_array</span><br><span class="line">布尔类型，为true表示json数据以数组对象开始且将数组对象中进行展平，默认值是false。</span><br><span class="line">5. data_source</span><br><span class="line">数据源的类型。当前支持：</span><br><span class="line">KAFKA</span><br><span class="line">6. data_source_properties</span><br><span class="line">指定数据源相关的信息。</span><br><span class="line">语法：</span><br><span class="line">(</span><br><span class="line">&quot;key1&quot; &#x3D; &quot;val1&quot;,</span><br><span class="line">&quot;key2&quot; &#x3D; &quot;val2&quot;</span><br><span class="line">)</span><br><span class="line">1. KAFKA 数据源</span><br><span class="line">1. kafka_broker_list</span><br><span class="line">Kafka 的 broker 连接信息。格式为 ip:host。多个broker之间以逗号分隔。</span><br><span class="line">示例：</span><br><span class="line">&quot;kafka_broker_list&quot; &#x3D; &quot;broker1:9092,broker2:9092&quot;</span><br><span class="line">2. kafka_topic</span><br><span class="line">指定要订阅的 Kafka 的 topic。</span><br><span class="line">示例：</span><br><span class="line">&quot;kafka_topic&quot; &#x3D; &quot;my_topic&quot;</span><br><span class="line">3. kafka_partitions&#x2F;kafka_offsets</span><br><span class="line">指定需要订阅的 kafka partition，以及对应的每个 partition 的起始 offset。</span><br><span class="line">offset 可以指定从大于等于 0 的具体 offset，或者：</span><br><span class="line">1) OFFSET_BEGINNING: 从有数据的位置开始订阅。</span><br><span class="line">2) OFFSET_END: 从末尾开始订阅。</span><br><span class="line">如果没有指定，则默认从 OFFSET_END 开始订阅 topic 下的所有 partition。</span><br><span class="line">示例：</span><br><span class="line">&quot;kafka_partitions&quot; &#x3D; &quot;0,1,2,3&quot;,</span><br><span class="line">&quot;kafka_offsets&quot; &#x3D; &quot;101,0,OFFSET_BEGINNING,OFFSET_END&quot;</span><br><span class="line">4. property</span><br><span class="line">指定自定义kafka参数。</span><br><span class="line">功能等同于kafka shell中 &quot;--property&quot; 参数。</span><br><span class="line">当参数的 value 为一个文件时，需要在 value 前加上关键词：&quot;FILE:&quot;。</span><br><span class="line">关于如何创建文件，请参阅 &quot;HELP CREATE FILE;&quot;</span><br><span class="line">更多支持的自定义参数，请参阅 librdkafka 的官方 CONFIGURATION 文档中，client 端的配置项。</span><br><span class="line">示例:</span><br><span class="line">&quot;property.client.id&quot; &#x3D; &quot;12345&quot;,</span><br><span class="line">&quot;property.ssl.ca.location&quot; &#x3D; &quot;FILE:ca.pem&quot;</span><br><span class="line">1.使用 SSL 连接 Kafka 时，需要指定以下参数：</span><br><span class="line">&quot;property.security.protocol&quot; &#x3D; &quot;ssl&quot;,</span><br><span class="line">&quot;property.ssl.ca.location&quot; &#x3D; &quot;FILE:ca.pem&quot;,</span><br><span class="line">&quot;property.ssl.certificate.location&quot; &#x3D; &quot;FILE:client.pem&quot;,</span><br><span class="line">&quot;property.ssl.key.location&quot; &#x3D; &quot;FILE:client.key&quot;,</span><br><span class="line">&quot;property.ssl.key.password&quot; &#x3D; &quot;abcdefg&quot;</span><br><span class="line">其中：</span><br><span class="line">&quot;property.security.protocol&quot; 和 &quot;property.ssl.ca.location&quot; 为必须，用于指明连接方式为 SSL，以及 CA 证书的位置。</span><br><span class="line">如果 Kafka server 端开启了 client 认证，则还需设置：</span><br><span class="line">&quot;property.ssl.certificate.location&quot;</span><br><span class="line">&quot;property.ssl.key.location&quot;</span><br><span class="line">&quot;property.ssl.key.password&quot;</span><br><span class="line">分别用于指定 client 的 public key，private key 以及 private key 的密码。</span><br><span class="line"> </span><br><span class="line">2.指定kafka partition的默认起始offset</span><br><span class="line">如果没有指定kafka_partitions&#x2F;kafka_offsets,默认消费所有分区,此时可以指定kafka_default_offsets指定起始 offset。默认为 OFFSET_END，即从末尾开始订阅。</span><br><span class="line">值为</span><br><span class="line">1) OFFSET_BEGINNING: 从有数据的位置开始订阅。</span><br><span class="line">2) OFFSET_END: 从末尾开始订阅。</span><br><span class="line">示例：</span><br><span class="line">&quot;property.kafka_default_offsets&quot; &#x3D; &quot;OFFSET_BEGINNING&quot;</span><br><span class="line">7. 导入数据格式样例</span><br><span class="line">整型类（TINYINT&#x2F;SMALLINT&#x2F;INT&#x2F;BIGINT&#x2F;LARGEINT）：1, 1000, 1234</span><br><span class="line">浮点类（FLOAT&#x2F;DOUBLE&#x2F;DECIMAL）：1.1, 0.23, .356</span><br><span class="line">日期类（DATE&#x2F;DATETIME）：2017-10-03, 2017-06-13 12:34:03。</span><br><span class="line">字符串类（CHAR&#x2F;VARCHAR）（无引号）：I am a student, a</span><br><span class="line">NULL值：\N</span><br><span class="line">Examples:</span><br><span class="line">1. 为 example_db 的 example_tbl 创建一个名为 test1 的 Kafka 例行导入任务。指定列分隔符和 group.id 和 client.id，并且自动默认消费所有分区，且从有数据的位置（OFFSET_BEGINNING）开始订阅</span><br><span class="line">CREATE ROUTINE LOAD example_db.test1 ON example_tbl</span><br><span class="line">COLUMNS TERMINATED BY &quot;,&quot;,</span><br><span class="line">COLUMNS(k1, k2, k3, v1, v2, v3 &#x3D; k1 * 100)</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">&quot;desired_concurrent_number&quot;&#x3D;&quot;3&quot;,</span><br><span class="line">&quot;max_batch_interval&quot; &#x3D; &quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot; &#x3D; &quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot; &#x3D; &quot;209715200&quot;,</span><br><span class="line">&quot;strict_mode&quot; &#x3D; &quot;false&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA</span><br><span class="line">(</span><br><span class="line">&quot;kafka_broker_list&quot; &#x3D; &quot;broker1:9092,broker2:9092,broker3:9092&quot;,</span><br><span class="line">&quot;kafka_topic&quot; &#x3D; &quot;my_topic&quot;,</span><br><span class="line">&quot;property.group.id&quot; &#x3D; &quot;xxx&quot;,</span><br><span class="line">&quot;property.client.id&quot; &#x3D; &quot;xxx&quot;,</span><br><span class="line">&quot;property.kafka_default_offsets&quot; &#x3D; &quot;OFFSET_BEGINNING&quot;</span><br><span class="line">);</span><br><span class="line">2. 为 example_db 的 example_tbl 创建一个名为 test1 的 Kafka 例行导入任务。导入任务为严格模式。</span><br><span class="line">CREATE ROUTINE LOAD example_db.test1 ON example_tbl</span><br><span class="line">COLUMNS(k1, k2, k3, v1, v2, v3 &#x3D; k1 * 100),</span><br><span class="line">WHERE k1 &gt; 100 and k2 like &quot;%doris%&quot;</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">&quot;desired_concurrent_number&quot;&#x3D;&quot;3&quot;,</span><br><span class="line">&quot;max_batch_interval&quot; &#x3D; &quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot; &#x3D; &quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot; &#x3D; &quot;209715200&quot;,</span><br><span class="line">&quot;strict_mode&quot; &#x3D; &quot;false&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA</span><br><span class="line">(</span><br><span class="line">&quot;kafka_broker_list&quot; &#x3D; &quot;broker1:9092,broker2:9092,broker3:9092&quot;,</span><br><span class="line">&quot;kafka_topic&quot; &#x3D; &quot;my_topic&quot;,</span><br><span class="line">&quot;kafka_partitions&quot; &#x3D; &quot;0,1,2,3&quot;,</span><br><span class="line">&quot;kafka_offsets&quot; &#x3D; &quot;101,0,0,200&quot;</span><br><span class="line">);</span><br><span class="line">3. 通过 SSL 认证方式，从 Kafka 集群导入数据。同时设置 client.id 参数。导入任务为非严格模式，时区为 Africa&#x2F;Abidjan</span><br><span class="line">CREATE ROUTINE LOAD example_db.test1 ON example_tbl</span><br><span class="line">COLUMNS(k1, k2, k3, v1, v2, v3 &#x3D; k1 * 100),</span><br><span class="line">WHERE k1 &gt; 100 and k2 like &quot;%doris%&quot;</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">&quot;desired_concurrent_number&quot;&#x3D;&quot;3&quot;,</span><br><span class="line">&quot;max_batch_interval&quot; &#x3D; &quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot; &#x3D; &quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot; &#x3D; &quot;209715200&quot;,</span><br><span class="line">&quot;strict_mode&quot; &#x3D; &quot;false&quot;,</span><br><span class="line">&quot;timezone&quot; &#x3D; &quot;Africa&#x2F;Abidjan&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA</span><br><span class="line">(</span><br><span class="line">&quot;kafka_broker_list&quot; &#x3D; &quot;broker1:9092,broker2:9092,broker3:9092&quot;,</span><br><span class="line">&quot;kafka_topic&quot; &#x3D; &quot;my_topic&quot;,</span><br><span class="line">&quot;property.security.protocol&quot; &#x3D; &quot;ssl&quot;,</span><br><span class="line">&quot;property.ssl.ca.location&quot; &#x3D; &quot;FILE:ca.pem&quot;,</span><br><span class="line">&quot;property.ssl.certificate.location&quot; &#x3D; &quot;FILE:client.pem&quot;,</span><br><span class="line">&quot;property.ssl.key.location&quot; &#x3D; &quot;FILE:client.key&quot;,</span><br><span class="line">&quot;property.ssl.key.password&quot; &#x3D; &quot;abcdefg&quot;,</span><br><span class="line">&quot;property.client.id&quot; &#x3D; &quot;my_client_id&quot;</span><br><span class="line">);</span><br><span class="line">4. 简单模式导入json</span><br><span class="line">CREATE ROUTINE LOAD example_db.test_json_label_1 ON table1</span><br><span class="line">COLUMNS(category,price,author)</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">&quot;desired_concurrent_number&quot;&#x3D;&quot;3&quot;,</span><br><span class="line">&quot;max_batch_interval&quot; &#x3D; &quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot; &#x3D; &quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot; &#x3D; &quot;209715200&quot;,</span><br><span class="line">&quot;strict_mode&quot; &#x3D; &quot;false&quot;,</span><br><span class="line">&quot;format&quot; &#x3D; &quot;json&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA</span><br><span class="line">(</span><br><span class="line">&quot;kafka_broker_list&quot; &#x3D; &quot;broker1:9092,broker2:9092,broker3:9092&quot;,</span><br><span class="line">&quot;kafka_topic&quot; &#x3D; &quot;my_topic&quot;,</span><br><span class="line">&quot;kafka_partitions&quot; &#x3D; &quot;0,1,2&quot;,</span><br><span class="line">&quot;kafka_offsets&quot; &#x3D; &quot;0,0,0&quot;</span><br><span class="line">);</span><br><span class="line">支持两种json数据格式：</span><br><span class="line">1）&#123;&quot;category&quot;:&quot;a9jadhx&quot;,&quot;author&quot;:&quot;test&quot;,&quot;price&quot;:895&#125;</span><br><span class="line">2）[</span><br><span class="line">&#123;&quot;category&quot;:&quot;a9jadhx&quot;,&quot;author&quot;:&quot;test&quot;,&quot;price&quot;:895&#125;,</span><br><span class="line">&#123;&quot;category&quot;:&quot;axdfa1&quot;,&quot;author&quot;:&quot;EvelynWaugh&quot;,&quot;price&quot;:1299&#125;</span><br><span class="line">]</span><br><span class="line">5. 精准导入json数据格式</span><br><span class="line">CREATE TABLE &#96;example_tbl&#96; (</span><br><span class="line">&#96;category&#96; varchar(24) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;author&#96; varchar(24) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;timestamp&#96; bigint(20) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;dt&#96; int(11) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;price&#96; double REPLACE</span><br><span class="line">) ENGINE&#x3D;OLAP</span><br><span class="line">AGGREGATE KEY(&#96;category&#96;,&#96;author&#96;,&#96;timestamp&#96;,&#96;dt&#96;)</span><br><span class="line">COMMENT &quot;OLAP&quot;</span><br><span class="line">PARTITION BY RANGE(&#96;dt&#96;)</span><br><span class="line">(PARTITION p0 VALUES [(&quot;-2147483648&quot;), (&quot;20200509&quot;)),</span><br><span class="line">PARTITION p20200509 VALUES [(&quot;20200509&quot;), (&quot;20200510&quot;)),</span><br><span class="line">PARTITION p20200510 VALUES [(&quot;20200510&quot;), (&quot;20200511&quot;)),</span><br><span class="line">PARTITION p20200511 VALUES [(&quot;20200511&quot;), (&quot;20200512&quot;)))</span><br><span class="line">DISTRIBUTED BY HASH(&#96;category&#96;,&#96;author&#96;,&#96;timestamp&#96;) BUCKETS 4</span><br><span class="line">PROPERTIES (</span><br><span class="line">&quot;storage_type&quot; &#x3D; &quot;COLUMN&quot;,</span><br><span class="line">&quot;replication_num&quot; &#x3D; &quot;1&quot;</span><br><span class="line">);</span><br><span class="line">CREATE ROUTINE LOAD example_db.test1 ON example_tbl</span><br><span class="line">COLUMNS(category, author, price, timestamp, dt&#x3D;from_unixtime(timestamp, &#39;%Y%m%d&#39;))</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">&quot;desired_concurrent_number&quot;&#x3D;&quot;3&quot;,</span><br><span class="line">&quot;max_batch_interval&quot; &#x3D; &quot;20&quot;,</span><br><span class="line">&quot;max_batch_rows&quot; &#x3D; &quot;300000&quot;,</span><br><span class="line">&quot;max_batch_size&quot; &#x3D; &quot;209715200&quot;,</span><br><span class="line">&quot;strict_mode&quot; &#x3D; &quot;false&quot;,</span><br><span class="line">&quot;format&quot; &#x3D; &quot;json&quot;,</span><br><span class="line">&quot;jsonpaths&quot; &#x3D; &quot;[\&quot;$.category\&quot;,\&quot;$.author\&quot;,\&quot;$.price\&quot;,\&quot;$.timestamp\&quot;]&quot;,</span><br><span class="line">&quot;strip_outer_array&quot; &#x3D; &quot;true&quot;</span><br><span class="line">)</span><br><span class="line">FROM KAFKA</span><br><span class="line">(</span><br><span class="line">&quot;kafka_broker_list&quot; &#x3D; &quot;broker1:9092,broker2:9092,broker3:9092&quot;,</span><br><span class="line">&quot;kafka_topic&quot; &#x3D; &quot;my_topic&quot;,</span><br><span class="line">&quot;kafka_partitions&quot; &#x3D; &quot;0,1,2&quot;,</span><br><span class="line">&quot;kafka_offsets&quot; &#x3D; &quot;0,0,0&quot;</span><br><span class="line">);</span><br><span class="line">json数据格式:</span><br><span class="line">[</span><br><span class="line">&#123;&quot;category&quot;:&quot;11&quot;,&quot;title&quot;:&quot;SayingsoftheCentury&quot;,&quot;price&quot;:895,&quot;timestamp&quot;:1589191587&#125;,</span><br><span class="line">&#123;&quot;category&quot;:&quot;22&quot;,&quot;author&quot;:&quot;2avc&quot;,&quot;price&quot;:895,&quot;timestamp&quot;:1589191487&#125;,</span><br><span class="line">&#123;&quot;category&quot;:&quot;33&quot;,&quot;author&quot;:&quot;3avc&quot;,&quot;title&quot;:&quot;SayingsoftheCentury&quot;,&quot;timestamp&quot;:1589191387&#125;</span><br><span class="line">]</span><br><span class="line">说明：</span><br><span class="line">1）如果json数据是以数组开始，并且数组中每个对象是一条记录，则需要将strip_outer_array设置成true，表示展平数组。</span><br><span class="line">2）如果json数据是以数组开始，并且数组中每个对象是一条记录，在设置jsonpath时，我们的ROOT节点实际上是数组中对象。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris MySQL Load</title>
    <url>/2021/04/15/DorisMySQLLoad/</url>
    <content><![CDATA[<p>Doris MySQL Load 的操作说明。</p>
<a id="more"></a>
<hr>
<p>Doris 支持创建 Mysql 引擎的表，操作此表相当于操作远程 Mysql 表。可以用来导入Mysql 数据使用或测试。</p>
<p>Tips: Doris 中默认字段都是不能为空的，如果字段可能为空需要加上 null 标记。</p>
<p>例如, <code>grade</code> int(11) NULL DEFAULT NULL</p>
<h1 id="MySql-load-DEMO-案例"><a href="#MySql-load-DEMO-案例" class="headerlink" title="MySql load DEMO 案例"></a>MySql load DEMO 案例</h1><h3 id="创建-Mysql-测试用表及测试数据"><a href="#创建-Mysql-测试用表及测试数据" class="headerlink" title="创建 Mysql 测试用表及测试数据"></a>创建 Mysql 测试用表及测试数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Mysql 测试库 test_udp, 测试表 test_score</span><br><span class="line">CREATE DATABASE IF NOT EXISTS test_udp;</span><br><span class="line">ROP TABLE IF EXISTS &#96;test_score&#96;;</span><br><span class="line">CREATE TABLE &#96;test_score&#96;  (</span><br><span class="line">  &#96;sno&#96; varchar(7) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT &#39;学号&#39;,</span><br><span class="line">  &#96;cno&#96; varchar(10) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT &#39;课程号&#39;,</span><br><span class="line">  &#96;grade&#96; int(11) NULL DEFAULT NULL COMMENT &#39;成绩&#39;,</span><br><span class="line">  PRIMARY KEY (&#96;sno&#96;, &#96;cno&#96;) USING BTREE,</span><br><span class="line">  INDEX &#96;Cno&#96;(&#96;cno&#96;) USING BTREE</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; InnoDB</span><br><span class="line">CHARACTER SET &#x3D; utf8</span><br><span class="line">COLLATE &#x3D; utf8_general_ci</span><br><span class="line">ROW_FORMAT &#x3D; Dynamic;</span><br><span class="line"> </span><br><span class="line"># 测试表 test_score 导入数据</span><br><span class="line">INSERT INTO &#96;test_score&#96; VALUES (&#39;0811101&#39;, &#39;C001&#39;, 96);</span><br><span class="line">INSERT INTO &#96;test_score&#96; VALUES (&#39;0811101&#39;, &#39;C002&#39;, 80);</span><br><span class="line">INSERT INTO &#96;test_score&#96; VALUES (&#39;0811102&#39;, &#39;C002&#39;, 90);</span><br><span class="line">INSERT INTO &#96;test_score&#96; VALUES (&#39;0811102&#39;, &#39;C005&#39;, 73);</span><br><span class="line">INSERT INTO &#96;test_score&#96; VALUES (&#39;0811102&#39;, &#39;C007&#39;, NULL);</span><br><span class="line">INSERT INTO &#96;test_score&#96; VALUES (&#39;0811103&#39;, &#39;C001&#39;, 50);</span><br><span class="line">INSERT INTO &#96;test_score&#96; VALUES (&#39;0811103&#39;, &#39;C004&#39;, 80);</span><br></pre></td></tr></table></figure>

<h3 id="创建-Doris-表映射-Mysql-表"><a href="#创建-Doris-表映射-Mysql-表" class="headerlink" title="创建 Doris 表映射 Mysql 表"></a>创建 Doris 表映射 Mysql 表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在 Doris test 库下创建映射 Mysql 的测试表 score</span><br><span class="line">USE test;</span><br><span class="line">CREATE TABLE &#96;score&#96;(</span><br><span class="line">  &#96;sno&#96; varchar(7) NOT NULL,</span><br><span class="line">  &#96;cno&#96; varchar(10) NOT NULL,</span><br><span class="line">  &#96;grade&#96; int(11) NULL DEFAULT NULL</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; mysql</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">  &quot;host&quot; &#x3D; &quot;10.0.15.134&quot;,</span><br><span class="line">  &quot;port&quot; &#x3D; &quot;3306&quot;,</span><br><span class="line">  &quot;user&quot; &#x3D; &quot;root&quot;,</span><br><span class="line">  &quot;password&quot; &#x3D; &quot;hmd@_#admin&quot;,</span><br><span class="line">  &quot;database&quot; &#x3D; &quot;test_udp&quot;,</span><br><span class="line">  &quot;table&quot; &#x3D; &quot;test_score&quot;</span><br><span class="line">);</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># 查询 Doris 测试表 score</span><br><span class="line">mysql&gt; select * from score;</span><br><span class="line">+---------+------+-------+</span><br><span class="line">| sno     | cno  | grade |</span><br><span class="line">+---------+------+-------+</span><br><span class="line">| 0811101 | C001 |    96 |</span><br><span class="line">| 0811101 | C002 |    80 |</span><br><span class="line">| 0811102 | C002 |    90 |</span><br><span class="line">| 0811102 | C005 |    73 |</span><br><span class="line">| 0811102 | C007 |  NULL |</span><br><span class="line">| 0811103 | C001 |    50 |</span><br><span class="line">| 0811103 | C004 |    80 |</span><br><span class="line">+---------+------+-------+</span><br><span class="line">7 rows in set (0.03 sec)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris 动态分区表</title>
    <url>/2021/05/16/Doris%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8/</url>
    <content><![CDATA[<p>Doris 动态分区使用方法说明。</p>
<a id="more"></a>
<hr>
<h2 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h2><p><a href="http://doris.apache.org/master/zh-CN/administrator-guide/dynamic-partition.html#示例" target="_blank" rel="noopener">http://doris.apache.org/master/zh-CN/administrator-guide/dynamic-partition.html#%E7%A4%BA%E4%BE%8B</a></p>
<h2 id="动态分区表DDL"><a href="#动态分区表DDL" class="headerlink" title="动态分区表DDL"></a>动态分区表DDL</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test.tbl1</span><br><span class="line">(</span><br><span class="line">    a DATE,</span><br><span class="line">    b VARCHAR(10)</span><br><span class="line">)</span><br><span class="line">PARTITION BY RANGE(a) ()</span><br><span class="line">DISTRIBUTED BY HASH(a)</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">    &quot;dynamic_partition.enable&quot; &#x3D; &quot;true&quot;,</span><br><span class="line">    &quot;dynamic_partition.time_unit&quot; &#x3D; &quot;DAY&quot;,</span><br><span class="line">    &quot;dynamic_partition.start&quot; &#x3D; &quot;-7&quot;,</span><br><span class="line">    &quot;dynamic_partition.end&quot; &#x3D; &quot;2&quot;,</span><br><span class="line">    &quot;dynamic_partition.prefix&quot; &#x3D; &quot;p_&quot;,</span><br><span class="line">    &quot;dynamic_partition.buckets&quot; &#x3D; &quot;3&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>建表完成DDL确认：</p>
<p><img src="/2021/05/16/Doris%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8/image2021-5-16_14-6-32.png" alt></p>
<h2 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test.tbl1</span><br><span class="line">(</span><br><span class="line">    a DATE,                 -- 指定按时间分区，动态分区字段可以设置为DATE、DATETIME类型，此处按DAY分区，所以选择了DATE；如果需要更细的粒度，例如小时分区，可以使用DATETIME类型</span><br><span class="line">    b VARCHAR(10)</span><br><span class="line">)</span><br><span class="line">PARTITION BY RANGE(a) ()    -- 指定动态分区字段</span><br><span class="line">DISTRIBUTED BY HASH(a)</span><br><span class="line">PROPERTIES</span><br><span class="line">(</span><br><span class="line">    &quot;dynamic_partition.enable&quot; &#x3D; &quot;true&quot;,     -- 开启动态分区</span><br><span class="line">    &quot;dynamic_partition.time_unit&quot; &#x3D; &quot;DAY&quot;,   -- 分区的规则，此处为按天分区；还可以设置为HOUR、WEEK、MONTH。分别表示按小时、按周、按月进行分区创建或删除。</span><br><span class="line">    &quot;dynamic_partition.start&quot; &#x3D; &quot;-7&quot;,        -- 自动删除7天以前的分区</span><br><span class="line">    &quot;dynamic_partition.end&quot; &#x3D; &quot;2&quot;,           -- 建表时自动创建未来两天的分区</span><br><span class="line">    &quot;dynamic_partition.prefix&quot; &#x3D; &quot;p_&quot;,       -- 分区命名的前缀</span><br><span class="line">    &quot;dynamic_partition.buckets&quot; &#x3D; &quot;3&quot;        -- 每个分区的分桶数量</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h2 id="分区状态查看"><a href="#分区状态查看" class="headerlink" title="分区状态查看"></a>分区状态查看</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW DYNAMIC PARTITION TABLES;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/05/16/Doris%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8/image2021-5-16_14-41-56.png" alt></p>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>需要注意，动态分区表分区的创建不是在插入了新的分区值数据的时候创建，</p>
<p>而是预先创建的，建议在建表时，设置预先创建1~2个分区。</p>
]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris 实时表使用Hive 数据初始化</title>
    <url>/2021/05/21/Doris%E5%AE%9E%E6%97%B6%E8%A1%A8%E4%BD%BF%E7%94%A8Hive%E6%95%B0%E6%8D%AE%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
    <content><![CDATA[<p>Doris 实时表使用 Hive 数据进行数据初始化操作流程。</p>
<a id="more"></a>
<hr>
<h3 id="1-Doris-table-建表语句确认"><a href="#1-Doris-table-建表语句确认" class="headerlink" title="1. Doris table 建表语句确认"></a>1. Doris table 建表语句确认</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show create table dwd_homedo_real.dwd_sub_account_info;</span><br></pre></td></tr></table></figure>

<h3 id="2-Doris-临时表创建"><a href="#2-Doris-临时表创建" class="headerlink" title="2. Doris 临时表创建"></a>2. Doris 临时表创建</h3><p>因为Hive 表字段全部为小写，为了避免字段大小写不一致，所以需要创建doris全字段小写表</p>
<p>将步骤1查询到的交表语句字段修改为小写，并建表。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;dwd_sub_account_info_test&#96; (</span><br><span class="line">&#96;account_id&#96; bigint(20) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;account_name&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;audit_company_id&#96; bigint(20) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;audit_company_name&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;org_code&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;org_name&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_credit&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;credit_amount&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;amount_cycle_bymonth&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;amount_cycle_byday&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;amount_cycle_type&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_used_amount_cycle&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_used_credit_amount&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;integral_balance&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;change_recommended_time&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;recommended_id&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;recommended_name&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;recommended_source&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;channel&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;flag_id&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;flag_name&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;parent_account_id&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;follower_account_id&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;ketuo_xiaoneng_id&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;aladdin_id&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;ctype&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;ctype_name&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_inner&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_handler&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_effect&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_channel&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_jicai&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;jicai_audit_time&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;jicai_overdue_time&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;registered_time&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;audit_time&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;audit_status&#96; int(11) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;audit_status_name&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;account_status&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;account_status_name&#96; varchar(1000) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;is_vip&#96; bigint(20) NULL COMMENT &quot;&quot;,</span><br><span class="line">&#96;vip_level&#96; varchar(1000) NULL COMMENT &quot;&quot;</span><br><span class="line">) ENGINE&#x3D;OLAP</span><br><span class="line">UNIQUE KEY(&#96;account_id&#96;)</span><br><span class="line">COMMENT &quot;OLAP&quot;</span><br><span class="line">DISTRIBUTED BY HASH(&#96;account_id&#96;) BUCKETS 3</span><br><span class="line">PROPERTIES (</span><br><span class="line">&quot;replication_num&quot; &#x3D; &quot;3&quot;,</span><br><span class="line">&quot;in_memory&quot; &#x3D; &quot;false&quot;,</span><br><span class="line">&quot;storage_format&quot; &#x3D; &quot;V2&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="3-Hive-临时表创建"><a href="#3-Hive-临时表创建" class="headerlink" title="3. Hive 临时表创建"></a>3. Hive 临时表创建</h3><p>字段顺序和 Doirs table一样，hive 不区分大小写，默认小写</p>
<p>Hive 临时表创建的目的有两个：</p>
<ol>
<li>为了统一Hive 和 Doris的字段顺序和字段数量</li>
<li>将Hive 的存储类型统一为 parquet</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE dwd_homedo.dwd_sub_account_info_test (</span><br><span class="line">account_id string,</span><br><span class="line">account_name string,</span><br><span class="line">audit_company_id string,</span><br><span class="line">audit_company_name string,</span><br><span class="line">org_code string,</span><br><span class="line">org_name string,</span><br><span class="line">is_credit string,</span><br><span class="line">credit_amount string,</span><br><span class="line">amount_cycle_bymonth string,</span><br><span class="line">amount_cycle_byday string,</span><br><span class="line">amount_cycle_type string,</span><br><span class="line">is_used_amount_cycle string,</span><br><span class="line">is_used_credit_amount string,</span><br><span class="line">integral_balance string,</span><br><span class="line">change_recommended_time string,</span><br><span class="line">recommended_id string,</span><br><span class="line">recommended_name string,</span><br><span class="line">recommended_source string,</span><br><span class="line">channel string,</span><br><span class="line">flag_id string,</span><br><span class="line">flag_name string,</span><br><span class="line">parent_account_id string,</span><br><span class="line">follower_account_id string,</span><br><span class="line">ketuo_xiaoneng_id string,</span><br><span class="line">aladdin_id string,</span><br><span class="line">ctype string,</span><br><span class="line">ctype_name string,</span><br><span class="line">is_inner string,</span><br><span class="line">is_handler string,</span><br><span class="line">is_effect string,</span><br><span class="line">is_channel string,</span><br><span class="line">is_jicai string,</span><br><span class="line">jicai_audit_time string,</span><br><span class="line">jicai_overdue_time string,</span><br><span class="line">registered_time string,</span><br><span class="line">audit_time string,</span><br><span class="line">audit_status string,</span><br><span class="line">audit_status_name string,</span><br><span class="line">account_status string,</span><br><span class="line">account_status_name string,</span><br><span class="line">is_VIP string,</span><br><span class="line">VIP_level string</span><br><span class="line">) stored as parquet</span><br><span class="line">location &#39;&#x2F;opt&#x2F;user&#x2F;HomedoDB&#x2F;warehouse&#x2F;dwd_homedo&#x2F;dwd_sub_account_info_test&#39;;</span><br></pre></td></tr></table></figure>

<h3 id="4-Hive-临时表数据导入"><a href="#4-Hive-临时表数据导入" class="headerlink" title="4. Hive 临时表数据导入"></a>4. Hive 临时表数据导入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">insert overwrite table dwd_homedo.dwd_sub_account_info_test</span><br><span class="line">select</span><br><span class="line">account_id,</span><br><span class="line">account_name,</span><br><span class="line">audit_company_id,</span><br><span class="line">audit_company_name,</span><br><span class="line">org_code,</span><br><span class="line">org_name,</span><br><span class="line">is_credit,</span><br><span class="line">credit_amount,</span><br><span class="line">amount_cycle_bymonth,</span><br><span class="line">amount_cycle_byday,</span><br><span class="line">amount_cycle_type,</span><br><span class="line">is_used_amount_cycle,</span><br><span class="line">is_used_credit_amount,</span><br><span class="line">integral_balance,</span><br><span class="line">change_recommended_time,</span><br><span class="line">recommended_id,</span><br><span class="line">recommended_name,</span><br><span class="line">recommended_source,</span><br><span class="line">channel,</span><br><span class="line">flag_id,</span><br><span class="line">flag_name,</span><br><span class="line">parent_account_id,</span><br><span class="line">follower_account_id,</span><br><span class="line">ketuo_xiaoneng_id,</span><br><span class="line">aladdin_id,</span><br><span class="line">ctype_id,</span><br><span class="line">ctype_name,</span><br><span class="line">is_inner,</span><br><span class="line">is_handler,</span><br><span class="line">is_effect,</span><br><span class="line">is_channel,</span><br><span class="line">is_jicai,</span><br><span class="line">jicai_audit_time,</span><br><span class="line">jicai_overdue_time,</span><br><span class="line">registered_time,</span><br><span class="line">audit_time,</span><br><span class="line">audit_status,</span><br><span class="line">audit_status_name,</span><br><span class="line">account_status,</span><br><span class="line">account_status_name,</span><br><span class="line">is_VIP,</span><br><span class="line">VIP_level from dwd_homedo.dwd_sub_account_info;</span><br></pre></td></tr></table></figure>

<h3 id="5-Hive-数据导入-Doris-临时表"><a href="#5-Hive-数据导入-Doris-临时表" class="headerlink" title="5. Hive 数据导入 Doris 临时表"></a>5. Hive 数据导入 Doris 临时表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">use dwd_homedo_real; -- Doris 库名</span><br><span class="line"> </span><br><span class="line">LOAD LABEL label_dwd_sub_account_info1(</span><br><span class="line">DATA INFILE(&quot;hdfs:&#x2F;&#x2F;hmdservice&#x2F;opt&#x2F;user&#x2F;HomedoDB&#x2F;warehouse&#x2F;dwd_homedo&#x2F;dwd_sub_account_info_test&#x2F;*&quot;) -- hive表路径</span><br><span class="line">INTO TABLE &#96;dwd_sub_account_info_test&#96; -- Doris 表</span><br><span class="line">FORMAT AS &quot;parquet&quot;</span><br><span class="line">)</span><br><span class="line">WITH BROKER broker_name</span><br><span class="line">PROPERTIES(</span><br><span class="line">&quot;timeout&quot;&#x3D;&quot;3600&quot;,</span><br><span class="line">&quot;max_filter_ratio&quot;&#x3D;&quot;0.1&quot; );</span><br></pre></td></tr></table></figure>

<h3 id="6-Doris-数据导入运行结果查看"><a href="#6-Doris-数据导入运行结果查看" class="headerlink" title="6. Doris 数据导入运行结果查看"></a>6. Doris 数据导入运行结果查看</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW LOAD WHERE LABEL &#x3D; &#39;label_dwd_sub_account_info1&#39;;</span><br></pre></td></tr></table></figure>

<h3 id="7-Doris-实时表数据导入"><a href="#7-Doris-实时表数据导入" class="headerlink" title="7. Doris 实时表数据导入"></a>7. Doris 实时表数据导入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT INTO dwd_homedo_real.dwd_homedo_promotion_flag (</span><br><span class="line">promotionId,</span><br><span class="line">tagGroupId,</span><br><span class="line">tagId,</span><br><span class="line">tagValue,</span><br><span class="line">mark</span><br><span class="line">)</span><br><span class="line">SELECT</span><br><span class="line">promotionid,</span><br><span class="line">taggroupid,</span><br><span class="line">tagid,</span><br><span class="line">tagvalue,</span><br><span class="line">mark</span><br><span class="line">FROM dwd_homedo_real.dwd_homedo_promotion_flag_test;</span><br></pre></td></tr></table></figure>

<h3 id="8-数据量确认"><a href="#8-数据量确认" class="headerlink" title="8. 数据量确认"></a>8. 数据量确认</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select SUM(t.a)</span><br><span class="line">from</span><br><span class="line">(select COUNT(DISTINCT promotionId) a from dwd_homedo_real.dwd_homedo_promotion_flag group by promotionId) t;</span><br><span class="line"> </span><br><span class="line">select SUM(t.a)</span><br><span class="line">from</span><br><span class="line">(select COUNT(DISTINCT promotionid) a from dwd_homedo_real.dwd_homedo_promotion_flag_test group by promotionid) t;</span><br></pre></td></tr></table></figure>

<h3 id="9-临时表删除"><a href="#9-临时表删除" class="headerlink" title="9. 临时表删除"></a>9. 临时表删除</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- Hive 临时表删除</span><br><span class="line">drop table dwd_homedo.dwd_sub_account_info_test;</span><br><span class="line"> </span><br><span class="line">-- Doris 临时表删除</span><br><span class="line">drop table dwd_homedo_real.dwd_homedo_promotion_flag_test;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris 模型使用说明</title>
    <url>/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>Doris 三种模型的使用场景、方法说明。</p>
<a id="more"></a>
<hr>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">关键字</th>
<th align="left">特点</th>
</tr>
</thead>
<tbody><tr>
<td align="left">明细模型</td>
<td align="left">DUPLICATE KEY</td>
<td align="left">默认模型</td>
</tr>
<tr>
<td align="left">聚合模型</td>
<td align="left">AGGREGATE KEY</td>
<td align="left">按照 AGGREGATE KEY 指定的字段和 表中定义了的聚合字段进行聚合</td>
</tr>
<tr>
<td align="left">更新模型</td>
<td align="left">UNIQUE KEY</td>
<td align="left">按照 AGGREGATE KEY 指定的字段判断是否重复，如果重复则覆盖更新</td>
</tr>
</tbody></table>
<h2 id="明细模型"><a href="#明细模型" class="headerlink" title="明细模型"></a>明细模型</h2><h3 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h3><p>Doris 建表的默认模型是明细模型。</p>
<p>一般用明细模型来处理的场景有如下特点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.需要保留最原始的数据(如，原始日志，原始操作记录等)来进行分析;</span><br><span class="line">2.分析方式一般不固定，并且相对灵活。因为查询方式很灵活，所以传统的预聚合方式难以命中，效率差;</span><br><span class="line">3.数据更新不是很频繁。导入的数据一般来源于日志类数据，产生后就不太会发生变化。</span><br></pre></td></tr></table></figure>

<h3 id="模型原理"><a href="#模型原理" class="headerlink" title="模型原理"></a>模型原理</h3><p>Doris 会为明细模型的表维护排序列，表中的数据会按照排序列进行排序存储。</p>
<p>排序列可以是一列，也可以由多列构成。排序列可以由用户指定，如果用户没有明确指定，那么 Doris 会为表选择默认的几个列作为排序列。</p>
<p>这样，在查询中，如果有相关排序列的过滤条件时，Doris 能够 快速地过滤数据，降低整个查询的时延。</p>
<h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><p>创建表的默认数据模型就是明细模型，在向 Doris 明细模型表中导入完全相同的两行数据时，Doris会认为是两行数据。</p>
<p>因为在使用过程中，用户经常会用到时间以及事件类型作为过滤条件，所以将这两个列放在建表的前两 列(事件时间和事件类型)作为排序列。以下是一个使用明细模型创建数据表的例子: </p>
<h4 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> demo_model_detail;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> demo_model_detail (</span><br><span class="line"> event_time DATETIME <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">"datetime of event"</span>,</span><br><span class="line"> event_type <span class="built_in">INT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">"type of event"</span>,</span><br><span class="line"> user_id <span class="built_in">INT</span> <span class="keyword">COMMENT</span> <span class="string">"id of user"</span>,</span><br><span class="line"> device_code <span class="built_in">INT</span> <span class="keyword">COMMENT</span> <span class="string">"device of "</span>,</span><br><span class="line"> channel <span class="built_in">INT</span> <span class="keyword">COMMENT</span> <span class="string">""</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">DUPLICATE</span> <span class="keyword">KEY</span>(event_time, event_type, user_id)</span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(user_id) BUCKETS <span class="number">8</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/image2021-4-13_15-32-51.png" alt></p>
<h4 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> demo_model_detail <span class="keyword">values</span></span><br><span class="line">(<span class="string">"2020-05-25 11:22:33"</span>, <span class="number">1</span>, <span class="number">101</span>, <span class="number">3302</span>, <span class="number">13</span>),</span><br><span class="line">(<span class="string">"2020-05-25 11:22:33"</span>, <span class="number">1</span>, <span class="number">101</span>, <span class="number">3302</span>, <span class="number">13</span>);</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/image2021-4-13_15-35-17.png" alt></p>
<h4 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> demo_model_detail;</span><br></pre></td></tr></table></figure>

<h4 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h4><p>充分利用排序列，在建表时将经常在查询中用于过滤的列放在表的前面，这样能够提升查询速度。</p>
<h2 id="聚合模型"><a href="#聚合模型" class="headerlink" title="聚合模型"></a>聚合模型</h2><h3 id="适用场景-1"><a href="#适用场景-1" class="headerlink" title="适用场景"></a>适用场景</h3><p>在数据分析领域中，有很多场景需要对数据进行统计和汇总操作。</p>
<p>例如：</p>
<ol>
<li><p>在网站、APP分析中对访问流量进行分析，获得用户的访问时长总计、访问次数总计等统计;</p>
</li>
<li><p>广告厂商为广告主提供的广告 点击总量、展示总量、消费统计等。</p>
</li>
</ol>
<p>上述的场景就比较适合使用聚合模型来进行查询加速，以下是一些适合聚合模型的场景:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.业务方进行的查询都是汇总类查询，比如sum、count等类型的查询;</span><br><span class="line">2.不需要查询最原始的明细数据;</span><br><span class="line">3.原始数据不会被频繁更新，只会进行追加新数据。</span><br></pre></td></tr></table></figure>

<h3 id="模型原理-1"><a href="#模型原理-1" class="headerlink" title="模型原理"></a>模型原理</h3><p>Doris 会将指标按照相同维度进行聚合。当多条数据具有相同的维度时，Doris会把指标进行聚合。<br>从而能够减少查询时所需要的处理的数据量，提升查询的效率。 </p>
<p>以下面的原始数据为例:</p>
<table>
<thead>
<tr>
<th align="left">date</th>
<th align="left">Country</th>
<th align="left">PV</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2021-04-10</td>
<td align="left">CHN</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">2021-04-10</td>
<td align="left">CHN</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left">2021-04-10</td>
<td align="left">USA</td>
<td align="left">3</td>
</tr>
<tr>
<td align="left">2021-04-10</td>
<td align="left">USA</td>
<td align="left">4</td>
</tr>
</tbody></table>
<p>在 Doris 聚合模型的表中，存储内容会从四条数据变为两条数据。</p>
<p>这样在后续查询处理的时候，处理的数据量就会有指数级的下降。</p>
<table>
<thead>
<tr>
<th align="left">date</th>
<th align="left">Country</th>
<th align="left">PV</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2021-04-10</td>
<td align="left">CHN</td>
<td align="left">3</td>
</tr>
<tr>
<td align="left">2021-04-10</td>
<td align="left">USA</td>
<td align="left">7</td>
</tr>
</tbody></table>
<h3 id="使用案例-1"><a href="#使用案例-1" class="headerlink" title="使用案例"></a>使用案例</h3><p>在建表时，为需要进行聚合的列分配对应的聚合操作就能够使用聚合模型。</p>
<p>以下是一个使用聚合模型创建数据表的例子:</p>
<h4 id="建表-1"><a href="#建表-1" class="headerlink" title="建表"></a>建表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> example_db.demo_model_aggregate;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> example_db.demo_model_aggregate (</span><br><span class="line"><span class="built_in">date</span> <span class="built_in">DATE</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">"日期"</span>,</span><br><span class="line">country <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">"国家"</span>,</span><br><span class="line">pv <span class="built_in">BIGINT</span> <span class="keyword">SUM</span> <span class="keyword">DEFAULT</span> <span class="string">"0"</span> <span class="keyword">COMMENT</span> <span class="string">"total page views"</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">AGGREGATE</span> <span class="keyword">KEY</span>(<span class="built_in">date</span>, country)</span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(country) BUCKETS <span class="number">8</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/image2021-4-13_16-1-58.png" alt></p>
<h4 id="插入数据-1"><a href="#插入数据-1" class="headerlink" title="插入数据"></a>插入数据</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> demo_model_aggregate </span><br><span class="line"><span class="keyword">values</span> </span><br><span class="line">(<span class="string">"2021-04-10"</span>, <span class="string">"USA"</span>, <span class="number">4</span>),</span><br><span class="line">(<span class="string">"2021-04-10"</span>, <span class="string">"CHN"</span>, <span class="number">2</span>),</span><br><span class="line">(<span class="string">"2021-04-10"</span>, <span class="string">"USA"</span>, <span class="number">3</span>),</span><br><span class="line">(<span class="string">"2021-04-10"</span>, <span class="string">"USA"</span>, <span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/image2021-4-13_16-7-56.png" alt></p>
<h4 id="查询-1"><a href="#查询-1" class="headerlink" title="查询"></a>查询</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> demo_model_aggregate;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/image2021-4-13_16-9-40.png" alt></p>
<h4 id="Tips-1"><a href="#Tips-1" class="headerlink" title="Tips"></a>Tips</h4><p>1.对于聚合列上的过滤条件，Doris 并不能够直接对于扫描出来的单行数据进行过滤。</p>
<p>  Doris 会将所有 Key 列(指的是上面的 date, country 两个列，他们没有被定义为聚合列)数据以及相应的聚合列数据全部读取出来。</p>
<p>  然后将 Key 列相同的不同版本数据进行合并计算出对应的聚合列内容。最后再对计算出的最终聚合列内容进行条件过滤。</p>
<p>  所以聚合列上的过滤会比 Key 列上的查询速度慢。这里可以进行优化，如果这个列没有聚合的操作，那么可以将这个列创建为 Key 列。</p>
<p>2.聚合模型目前支持的聚合函数有SUM, MIN, MAX, REPLACE。</p>
<h2 id="更新模型"><a href="#更新模型" class="headerlink" title="更新模型"></a>更新模型</h2><h3 id="适用场景-2"><a href="#适用场景-2" class="headerlink" title="适用场景"></a>适用场景</h3><p>在有些分析场景中，分析的数据经常会发生改变。对于这些分析场景的需求，可以使用 Doris 的更新模型来实现。</p>
<p>比如在电商场景中，定单的状态经常会发生变化，而且变化量又比较大，大型电商公司，每天的订单更新量可能上亿。</p>
<p>要在这种量级的更新场景下进行实时数据分析，如果在明细模型下通过 delete + insert 的方式，是无法满足这么频繁的更新需求的。</p>
<p>这种场景下，用户需要使用更来满足数据分析需求。</p>
<p>以下是一些适合更新模型的场景: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.已经写入的数据有大量的更新需求;</span><br><span class="line">2.需要进行实时数据分析。</span><br></pre></td></tr></table></figure>

<h3 id="模型原理-2"><a href="#模型原理-2" class="headerlink" title="模型原理"></a>模型原理</h3><p>Doris 存储内部会给每一个批次导入数据分配一个版本号，在查询的时候对于主键相同的两条数据，</p>
<p>版本大的数据(也就是更新的数据)被返回，而版本较小的数据不会被返回。</p>
<table>
<thead>
<tr>
<th align="left">ID</th>
<th align="left">value</th>
<th align="left">__version</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">100</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">101</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">100</td>
<td align="left">3</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">101</td>
<td align="left">4</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">102</td>
<td align="left">5</td>
</tr>
</tbody></table>
<p>具体的示例如上表所示，ID是表的主键，value是表的内容，而 __version 是 Doris 内部的版本号。</p>
<p>其中 ID为1的数据有两个导入批次，版本分别为1，2;</p>
<p>ID 为 2 的数据有三个批次导入，版本分别为 3，4，5。</p>
<p>在查询的时候对于 ID 为 1 只会返回最新版本 2 的数据，而对于 ID 为 2 只会返回最新版本 5 的数据，</p>
<p>那么对于用户能能够看到的数据如下表所示:</p>
<table>
<thead>
<tr>
<th align="left">ID</th>
<th align="left">value</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">100</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">102</td>
</tr>
</tbody></table>
<p>通过这种机制，Doris可以支持对于频繁更新数据的分析。</p>
<h3 id="使用案例-2"><a href="#使用案例-2" class="headerlink" title="使用案例"></a>使用案例</h3><p>例如在订单分析场景中，经常会有根据订单状态进行的统计分析需求。</p>
<p>因为订单状态经常改变，而 create_time 和order_id 不会改变，并且经常会在查询中作为过滤条件。</p>
<p>所以可以将 create_time 和 order_id 两个列作为这个表的主键(即：在建表时用 UNIQUE KEY 关键字定义)，</p>
<p>这样既能够满足订单状态的更新需求，又能够在查询中进行快速过滤。</p>
<p>以下是一个使用更新模型创建数据表的例子: </p>
<h4 id="建表-2"><a href="#建表-2" class="headerlink" title="建表"></a>建表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> example_db.demo_model_update;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> example_db.demo_model_update (</span><br><span class="line">    create_time <span class="built_in">DATE</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">"create time of an order"</span>,</span><br><span class="line">    order_id <span class="built_in">BIGINT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">"id of an order"</span>,</span><br><span class="line">    order_state <span class="built_in">INT</span> <span class="keyword">COMMENT</span> <span class="string">"state of an order"</span>,</span><br><span class="line">    total_price <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">"price of an order"</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">UNIQUE</span> <span class="keyword">KEY</span>(create_time, order_id)</span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(order_id) BUCKETS <span class="number">8</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/image2021-4-13_16-31-24.png" alt></p>
<h4 id="插入数据-2"><a href="#插入数据-2" class="headerlink" title="插入数据"></a>插入数据</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> example_db.demo_model_update </span><br><span class="line"><span class="keyword">values</span> </span><br><span class="line">(<span class="string">"2021-04-10 11:22:33"</span>, <span class="number">101</span>, <span class="number">1</span>, <span class="number">3344</span>),</span><br><span class="line">(<span class="string">"2021-04-10 11:22:33"</span>, <span class="number">101</span>, <span class="number">2</span>, <span class="number">4455</span>),</span><br><span class="line">(<span class="string">"2021-04-10 11:22:33"</span>, <span class="number">101</span>, <span class="number">3</span>, <span class="number">5566</span>);</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/image2021-4-13_16-36-3.png" alt></p>
<h4 id="查询-2"><a href="#查询-2" class="headerlink" title="查询"></a>查询</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> example_db.demo_model_update;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/13/Doris%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/image2021-4-13_16-36-31.png" alt></p>
<h4 id="Tips-2"><a href="#Tips-2" class="headerlink" title="Tips"></a>Tips</h4><p>1.导入数据时需要将所有字段补全才能够完成更新操作，</p>
<p>  如上述例子中的 create_time、order_id、order_state 和 total_price 四个字段都需必须存在。</p>
<p>2.对于更新模型的数据读取，需要在查询时完成多版本合并，当版本过多时会导致查询性能降低。</p>
<p>  所以在向更新模型导入数据时，应该适当降低导入频率，从而提升查询性能。</p>
<p>  建议在设计导入频率时以满足业务对实时性的要求为准。如果业务对实时性的要求是分钟级别，那么每分钟导入一次更新数据即可，不需要秒级导入。</p>
<p>3.在查询时，对于 value 字段的过滤通常在多版本合并之后。</p>
<p>  可以将经常过滤字段且不会被修改的字段放在主键上。这样能够在合并之前就将数据过滤掉，从而提升查询性能。</p>
<p>4.因为合并过程需要将所有主键字段进行比较，所以应该避免放置过多的主键字段，以免降低查询能。</p>
<p>  如果某个字段只是偶尔会作为查询中的过滤条件存在，不需要放在主键中。</p>
]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris 部署说明</title>
    <url>/2021/04/12/Doris%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>Doris 集群的部署说明。</p>
<a id="more"></a>
<hr>
<h3 id="官网地址"><a href="#官网地址" class="headerlink" title="官网地址"></a>官网地址</h3><h4 id="安装包下载地址"><a href="#安装包下载地址" class="headerlink" title="安装包下载地址"></a>安装包下载地址</h4><p><a href="https://cloud.baidu.com/doc/DORIS/s/bkn5owgit" target="_blank" rel="noopener">https://cloud.baidu.com/doc/DORIS/s/bkn5owgit</a></p>
<h4 id="官方文档地址"><a href="#官方文档地址" class="headerlink" title="官方文档地址"></a>官方文档地址</h4><p><a href="http://doris.apache.org/master/zh-CN/installing/install-deploy.html#%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2" target="_blank" rel="noopener">http://doris.apache.org/master/zh-CN/installing/install-deploy.html#%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2</a></p>
<h3 id="路径相关"><a href="#路径相关" class="headerlink" title="路径相关"></a>路径相关</h3><h4 id="安装路径"><a href="#安装路径" class="headerlink" title="安装路径"></a>安装路径</h4><p>/opt/doris</p>
<h4 id="手动创建文件夹"><a href="#手动创建文件夹" class="headerlink" title="手动创建文件夹"></a>手动创建文件夹</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p fe&#x2F;doris-meta</span><br><span class="line">mkdir -p be&#x2F;storage</span><br></pre></td></tr></table></figure>

<h3 id="FE-主节点启动"><a href="#FE-主节点启动" class="headerlink" title="FE 主节点启动"></a>FE 主节点启动</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;fe&#x2F;bin&#x2F;start_fe.sh --daemon</span><br></pre></td></tr></table></figure>

<h3 id="登录-Doris-使用MySQL控制台登录"><a href="#登录-Doris-使用MySQL控制台登录" class="headerlink" title="登录 Doris (使用MySQL控制台登录)"></a>登录 Doris (使用MySQL控制台登录)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h10.0.15.131 -P9030 -uroot</span><br></pre></td></tr></table></figure>

<h4 id="设置密码"><a href="#设置密码" class="headerlink" title="设置密码"></a>设置密码</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET PASSWORD FOR &#39;root&#39; &#x3D; PASSWORD(&#39;root&#39;);</span><br></pre></td></tr></table></figure>

<h4 id="使用密码登录-Doris"><a href="#使用密码登录-Doris" class="headerlink" title="使用密码登录 Doris"></a>使用密码登录 Doris</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h10.0.15.131 -P9030 -uroot -proot</span><br></pre></td></tr></table></figure>

<h3 id="FE-子节点注册、启动"><a href="#FE-子节点注册、启动" class="headerlink" title="FE 子节点注册、启动"></a>FE 子节点注册、启动</h3><h4 id="FE-子节点注册"><a href="#FE-子节点注册" class="headerlink" title="FE 子节点注册"></a>FE 子节点注册</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; ALTER SYSTEM ADD FOLLOWER &quot;10.0.15.132:19010&quot;;</span><br><span class="line">mysql&gt; ALTER SYSTEM ADD FOLLOWER &quot;10.0.15.135:19010&quot;;</span><br></pre></td></tr></table></figure>

<h4 id="FE-集群启动"><a href="#FE-集群启动" class="headerlink" title="FE 集群启动"></a>FE 集群启动</h4><p>注意ip、端口为主节点对应的ip、端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;start_fe.sh --helper 10.0.15.131:19010 --daemon</span><br></pre></td></tr></table></figure>

<h4 id="FE-状态查看"><a href="#FE-状态查看" class="headerlink" title="FE 状态查看"></a>FE 状态查看</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW PROC &#39;&#x2F;frontends&#39;\G</span><br></pre></td></tr></table></figure>

<h3 id="BE-节点注册、启动"><a href="#BE-节点注册、启动" class="headerlink" title="BE 节点注册、启动"></a>BE 节点注册、启动</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER SYSTEM ADD BACKEND &quot;10.0.15.131:9050&quot;;</span><br><span class="line">ALTER SYSTEM ADD BACKEND &quot;10.0.15.132:9050&quot;;</span><br><span class="line">ALTER SYSTEM ADD BACKEND &quot;10.0.15.135:9050&quot;;</span><br></pre></td></tr></table></figure>

<h4 id="BE-启动"><a href="#BE-启动" class="headerlink" title="BE 启动"></a>BE 启动</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;be&#x2F;bin&#x2F;start_be.sh --daemon</span><br></pre></td></tr></table></figure>

<h4 id="BE-状态查看"><a href="#BE-状态查看" class="headerlink" title="BE 状态查看"></a>BE 状态查看</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW PROC &#39;&#x2F;backends&#39;\G</span><br></pre></td></tr></table></figure>

<h3 id="Broker-启动、注册"><a href="#Broker-启动、注册" class="headerlink" title="Broker 启动、注册"></a>Broker 启动、注册</h3><h4 id="Broker-启动"><a href="#Broker-启动" class="headerlink" title="Broker 启动"></a>Broker 启动</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sh bin&#x2F;start_broker.sh --daemon</span><br></pre></td></tr></table></figure>

<h4 id="Broker-节点注册"><a href="#Broker-节点注册" class="headerlink" title="Broker 节点注册"></a>Broker 节点注册</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER SYSTEM ADD BROKER broker_name &quot;10.0.15.131:8000&quot;,&quot;10.0.15.132:8000&quot;;</span><br></pre></td></tr></table></figure>

<h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><h4 id="前端报错修复方法"><a href="#前端报错修复方法" class="headerlink" title="前端报错修复方法"></a>前端报错修复方法</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">enable_http_server_v2</span><br><span class="line">是否启用的 V2 版本的 HTTP Server 实现。新的 HTTP Server 采用 SpringBoot 实现。</span><br><span class="line">并且实现了前后端分离。 只有当开启后，才能使用 ui&#x2F; 目录下的新版 UI 界面。</span><br><span class="line">默认为 false。</span><br></pre></td></tr></table></figure>

<h4 id="修改可打开文件数"><a href="#修改可打开文件数" class="headerlink" title="修改可打开文件数"></a>修改可打开文件数</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ulimit -n 65535</span><br></pre></td></tr></table></figure>

<h4 id="FE-集群启动失败的处理"><a href="#FE-集群启动失败的处理" class="headerlink" title="FE 集群启动失败的处理"></a>FE 集群启动失败的处理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">因为IP冲突、启动命令使用错误引起的 FE 字节点启动失败，需要在下次启动前重建fe&#x2F;doris-meta路径。</span><br></pre></td></tr></table></figure>

<h4 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h4><h5 id="删除-FE-节点"><a href="#删除-FE-节点" class="headerlink" title="删除 FE 节点"></a>删除 FE 节点</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER SYSTEM DROP FOLLOWER &quot;10.0.15.132:9010&quot;;</span><br></pre></td></tr></table></figure>

<h5 id="删除-Broker-节点"><a href="#删除-Broker-节点" class="headerlink" title="删除 Broker 节点"></a>删除 Broker 节点</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER SYSTEM DROP BROKER broker_name &quot;10.0.15.131:8000&quot;,&quot;10.0.15.132:8000&quot;;</span><br></pre></td></tr></table></figure>

<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><h4 id="FE-conf"><a href="#FE-conf" class="headerlink" title="FE.conf"></a>FE.conf</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"># or more contributor license agreements.  See the NOTICE file</span><br><span class="line"># distributed with this work for additional information</span><br><span class="line"># regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"># to you under the Apache License, Version 2.0 (the</span><br><span class="line"># &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"># with the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#   http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing,</span><br><span class="line"># software distributed under the License is distributed on an</span><br><span class="line"># &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span><br><span class="line"># KIND, either express or implied.  See the License for the</span><br><span class="line"># specific language governing permissions and limitations</span><br><span class="line"># under the License.</span><br><span class="line"></span><br><span class="line">#####################################################################</span><br><span class="line">## The uppercase properties are read and exported by bin&#x2F;start_fe.sh.</span><br><span class="line">## To see all Frontend configurations,</span><br><span class="line">## see fe&#x2F;src&#x2F;org&#x2F;apache&#x2F;doris&#x2F;common&#x2F;Config.java</span><br><span class="line">#####################################################################</span><br><span class="line"></span><br><span class="line"># the output dir of stderr and stdout </span><br><span class="line">LOG_DIR &#x3D; $&#123;DORIS_HOME&#125;&#x2F;log</span><br><span class="line"></span><br><span class="line">DATE &#x3D; &#96;date +%Y%m%d-%H%M%S&#96;</span><br><span class="line">JAVA_OPTS&#x3D;&quot;-Xmx4096m -XX:+UseMembar -XX:SurvivorRatio&#x3D;8 -XX:MaxTenuringThreshold&#x3D;7 -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:-CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction&#x3D;80 -XX:SoftRefLRUPolicyMSPerMB&#x3D;0 -Xloggc:$DORIS_HOME&#x2F;log&#x2F;fe.gc.log.$DATE&quot;</span><br><span class="line"></span><br><span class="line"># For jdk 9+, this JAVA_OPTS will be used as default JVM options</span><br><span class="line">JAVA_OPTS_FOR_JDK_9&#x3D;&quot;-Xmx4096m -XX:SurvivorRatio&#x3D;8 -XX:MaxTenuringThreshold&#x3D;7 -XX:+CMSClassUnloadingEnabled -XX:-CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction&#x3D;80 -XX:SoftRefLRUPolicyMSPerMB&#x3D;0 -Xlog:gc*:$DORIS_HOME&#x2F;log&#x2F;fe.gc.log.$DATE:time&quot;</span><br><span class="line"></span><br><span class="line">##</span><br><span class="line">## the lowercase properties are read by main program.</span><br><span class="line">##</span><br><span class="line"></span><br><span class="line"># INFO, WARN, ERROR, FATAL</span><br><span class="line">sys_log_level &#x3D; INFO</span><br><span class="line"></span><br><span class="line"># store metadata, must be created before start FE.</span><br><span class="line"># Default value is $&#123;DORIS_HOME&#125;&#x2F;doris-meta</span><br><span class="line"># meta_dir &#x3D; $&#123;DORIS_HOME&#125;&#x2F;doris-meta</span><br><span class="line"></span><br><span class="line">http_port &#x3D; 18030</span><br><span class="line">rpc_port &#x3D; 9020</span><br><span class="line">query_port &#x3D; 9030</span><br><span class="line">edit_log_port &#x3D; 19010</span><br><span class="line">mysql_service_nio_enabled &#x3D; true</span><br><span class="line"></span><br><span class="line"># Choose one if there are more than one ip except loopback address. </span><br><span class="line"># Note that there should at most one ip match this list.</span><br><span class="line"># If no ip match this rule, will choose one randomly.</span><br><span class="line"># use CIDR format, e.g. 10.10.10.0&#x2F;24</span><br><span class="line"># Default value is empty.</span><br><span class="line">priority_networks &#x3D; 10.10.110.236&#x2F;24</span><br><span class="line"></span><br><span class="line"># Advanced configurations </span><br><span class="line"># log_roll_size_mb &#x3D; 1024</span><br><span class="line"># sys_log_dir &#x3D; $&#123;DORIS_HOME&#125;&#x2F;log</span><br><span class="line"># sys_log_roll_num &#x3D; 10</span><br><span class="line"># sys_log_verbose_modules &#x3D; </span><br><span class="line"># audit_log_dir &#x3D; $&#123;DORIS_HOME&#125;&#x2F;log</span><br><span class="line"># audit_log_modules &#x3D; slow_query, query</span><br><span class="line"># audit_log_roll_num &#x3D; 10</span><br><span class="line"># meta_delay_toleration_second &#x3D; 10</span><br><span class="line"># qe_max_connection &#x3D; 1024</span><br><span class="line"># max_conn_per_user &#x3D; 100</span><br><span class="line"># qe_query_timeout_second &#x3D; 300</span><br><span class="line"># qe_slow_log_ms &#x3D; 5000</span><br><span class="line"></span><br><span class="line">enable_http_server_v2 &#x3D; true</span><br></pre></td></tr></table></figure>

<h4 id="BE-conf"><a href="#BE-conf" class="headerlink" title="BE.conf"></a>BE.conf</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"># or more contributor license agreements.  See the NOTICE file</span><br><span class="line"># distributed with this work for additional information</span><br><span class="line"># regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"># to you under the Apache License, Version 2.0 (the</span><br><span class="line"># &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"># with the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#   http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing,</span><br><span class="line"># software distributed under the License is distributed on an</span><br><span class="line"># &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span><br><span class="line"># KIND, either express or implied.  See the License for the</span><br><span class="line"># specific language governing permissions and limitations</span><br><span class="line"># under the License.</span><br><span class="line"></span><br><span class="line">PPROF_TMPDIR&#x3D;&quot;$DORIS_HOME&#x2F;log&#x2F;&quot;</span><br><span class="line"></span><br><span class="line"># INFO, WARNING, ERROR, FATAL</span><br><span class="line">sys_log_level &#x3D; INFO</span><br><span class="line"></span><br><span class="line"># ports for admin, web, heartbeat service </span><br><span class="line">be_port &#x3D; 9060</span><br><span class="line">be_rpc_port &#x3D; 9070</span><br><span class="line">webserver_port &#x3D; 18040</span><br><span class="line">heartbeat_service_port &#x3D; 9050</span><br><span class="line">brpc_port &#x3D; 18060</span><br><span class="line"></span><br><span class="line"># Choose one if there are more than one ip except loopback address. </span><br><span class="line"># Note that there should at most one ip match this list.</span><br><span class="line"># If no ip match this rule, will choose one randomly.</span><br><span class="line"># use CIDR format, e.g. 10.10.10.0&#x2F;24</span><br><span class="line"># Default value is empty.</span><br><span class="line">priority_networks &#x3D; 10.10.110.236&#x2F;24</span><br><span class="line"></span><br><span class="line"># data root path, separate by &#39;;&#39;</span><br><span class="line"># you can specify the storage medium of each root path, HDD or SSD</span><br><span class="line"># you can add capacity limit at the end of each root path, seperate by &#39;,&#39;</span><br><span class="line"># eg:</span><br><span class="line"># storage_root_path &#x3D; &#x2F;home&#x2F;disk1&#x2F;doris.HDD,50;&#x2F;home&#x2F;disk2&#x2F;doris.SSD,1;&#x2F;home&#x2F;disk2&#x2F;doris</span><br><span class="line"># &#x2F;home&#x2F;disk1&#x2F;doris.HDD, capacity limit is 50GB, HDD;</span><br><span class="line"># &#x2F;home&#x2F;disk2&#x2F;doris.SSD, capacity limit is 1GB, SSD;</span><br><span class="line"># &#x2F;home&#x2F;disk2&#x2F;doris, capacity limit is disk capacity, HDD(default)</span><br><span class="line"># </span><br><span class="line"># you also can specify the properties by setting &#39;&lt;property&gt;:&lt;value&gt;&#39;, seperate by &#39;,&#39;</span><br><span class="line"># property &#39;medium&#39; has a higher priority than the extension of path</span><br><span class="line">#</span><br><span class="line"># Default value is $&#123;DORIS_HOME&#125;&#x2F;storage, you should create it by hand.</span><br><span class="line"># storage_root_path &#x3D; $&#123;DORIS_HOME&#125;&#x2F;storage</span><br><span class="line"></span><br><span class="line"># Advanced configurations</span><br><span class="line"># sys_log_dir &#x3D; $&#123;DORIS_HOME&#125;&#x2F;log</span><br><span class="line"># sys_log_roll_mode &#x3D; SIZE-MB-1024</span><br><span class="line"># sys_log_roll_num &#x3D; 10</span><br><span class="line"># sys_log_verbose_modules &#x3D; *</span><br><span class="line"># log_buffer_level &#x3D; -1</span><br><span class="line"># palo_cgroups</span><br></pre></td></tr></table></figure>

<h4 id="apache-hdfs-broker-conf"><a href="#apache-hdfs-broker-conf" class="headerlink" title="apache_hdfs_broker.conf"></a>apache_hdfs_broker.conf</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"># or more contributor license agreements.  See the NOTICE file</span><br><span class="line"># distributed with this work for additional information</span><br><span class="line"># regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"># to you under the Apache License, Version 2.0 (the</span><br><span class="line"># &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"># with the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#   http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing,</span><br><span class="line"># software distributed under the License is distributed on an</span><br><span class="line"># &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span><br><span class="line"># KIND, either express or implied.  See the License for the</span><br><span class="line"># specific language governing permissions and limitations</span><br><span class="line"># under the License.</span><br><span class="line"></span><br><span class="line"># the thrift rpc port</span><br><span class="line">broker_ipc_port&#x3D;8000</span><br><span class="line"></span><br><span class="line"># client session will be deleted if not receive ping after this time</span><br><span class="line">client_expire_seconds&#x3D;300</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">需要在conf路径增加以下两个文件，需要将集群对应文件复制到该路径</span><br><span class="line">core-site.xml</span><br><span class="line">hdfs-site.xml</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Doris</category>
      </categories>
      <tags>
        <tag>Doris</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Checkpoint 设置</title>
    <url>/2021/03/06/FlinkCheckPoint/</url>
    <content><![CDATA[<p>Flink Checkpoint 设置 Demo</p>
<a id="more"></a>
<hr>
<h4 id="Pom"><a href="#Pom" class="headerlink" title="Pom"></a>Pom</h4><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- checkpoint 依赖 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="配置-Checkpoint"><a href="#配置-Checkpoint" class="headerlink" title="配置 Checkpoint"></a>配置 Checkpoint</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Checkpoint 间隔 10s</span></span><br><span class="line">env.enableCheckpointing(<span class="number">10000</span>);</span><br><span class="line"><span class="comment">// 模式为 exactly-once（默认值）</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">10000</span>);</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> RocksDBStateBackend(<span class="string">"file:///Users/emerk/Documents//flink-demo/checkpoint"</span>));</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Side Output</title>
    <url>/2021/07/28/FlinkSideOutput/</url>
    <content><![CDATA[<p>Flink Side Output 代码示例</p>
<p>示例背景为用户型为分析数据清洗时，使用 Side Output 进行 event 数据、profile 数据的分离</p>
<a id="more"></a>
<hr>
<h4 id="代码片段"><a href="#代码片段" class="headerlink" title="代码片段"></a>代码片段</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// sideoutput 处理 profile数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;JSONObject&gt; dataMap = ds.process(<span class="keyword">new</span> ProfileSideoutputProcessFunction());</span><br><span class="line">SingleOutputStreamOperator&lt;JSONObject&gt; profiledata = dataMap.getSideOutput(<span class="keyword">new</span> OutputTag&lt;JSONObject&gt;(<span class="string">"PROFILE"</span>) &#123;&#125;);</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xxx.flink.etl.sideoutput_function;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.OutputTag;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProfileSideoutputProcessFunction</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>&lt;<span class="title">JSONObject</span>, <span class="title">JSONObject</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(JSONObject value, Context ctx, Collector&lt;JSONObject&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 分流操作，xwhat的事件ID为profile类型的，侧输出到profile流中</span></span><br><span class="line">        String xwhat = value.getString(<span class="string">"xwhat"</span>);</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"$profile_set"</span>.equals(xwhat) ||</span><br><span class="line">                <span class="string">"$profile_set_once"</span>.equals(xwhat) ||</span><br><span class="line">                <span class="string">"$profile_unset"</span>.equals(xwhat) ||</span><br><span class="line">                <span class="string">"$profile_increment"</span>.equals(xwhat) ||</span><br><span class="line">                <span class="string">"$profile_append"</span>.equals(xwhat) ||</span><br><span class="line">                <span class="string">"$profile_delete"</span>.equals(xwhat)) &#123;</span><br><span class="line">            ctx.output(<span class="keyword">new</span> OutputTag&lt;JSONObject&gt;(<span class="string">"PROFILE"</span>) &#123;</span><br><span class="line">            &#125;, value);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            out.collect(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Sink Doris 中文乱码</title>
    <url>/2021/08/11/FlinkSinkDoris%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</url>
    <content><![CDATA[<p>Kafka -&gt; Flink -&gt; StreamLoad Doris</p>
<p>数据进入 Doris 后，中文数据会显示为 ？？？</p>
<a id="more"></a>
<hr>
<h4 id="Flink-启动命令增加以下命令后解决了中文乱码问题"><a href="#Flink-启动命令增加以下命令后解决了中文乱码问题" class="headerlink" title="Flink 启动命令增加以下命令后解决了中文乱码问题"></a>Flink 启动命令增加以下命令后解决了中文乱码问题</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-yD env.java.opts="-Dfile.encoding=UTF-8 -Dsun.jnu.encoding=UTF-8"</span><br></pre></td></tr></table></figure>

<h4 id="具体启动命令"><a href="#具体启动命令" class="headerlink" title="具体启动命令"></a>具体启动命令</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/cloudera/parcels/FLINK/lib/flink/bin/flink run -yD env.java.opts="-Dfile.encoding=UTF-8 -Dsun.jnu.encoding=UTF-8" -m yarn-cluster -ynm data-real-time-sink-doris -c org.app.SinkDorisApp /opt/realtime/flink-first-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Doris</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 动态 Sink Kafka 不同 Topic</title>
    <url>/2021/08/05/Flink%E5%8A%A8%E6%80%81SinkKafka%E5%A4%9A%E4%B8%AATopic/</url>
    <content><![CDATA[<p>Flink 根据某个字段动态的把数据写入 Kafka 的某些 Topic</p>
<p>可能会想到多写几个 Sink 就可以了，的确，Flink 支持多个 Sink</p>
<p>但是如果需求是写入非常多的 Topic，那么代码会非常冗余</p>
<p>Flink 提供了高级的序列化模式，FlinkKafkaProducer 提供了 KafkaSerializationSchema 接口</p>
<p>这个模式允许分开的序列化 Key 和 Value，同时允许<strong>重写目标 Topic</strong>，因此一个 FlinkKafkaProducer 可以发送数据到多个 Topic</p>
<a id="more"></a>
<hr>
<h4 id="实现-KafkaSerializationSchema-接口"><a href="#实现-KafkaSerializationSchema-接口" class="headerlink" title="实现 KafkaSerializationSchema 接口"></a>实现 KafkaSerializationSchema 接口</h4><p>实现的需求为：将数据动态的 Sink 到 <code>topic</code> 字段值的 Topic 中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xxx.flink.utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.analysys.ark.streaming.utils.JSONUtil;</span><br><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.databind.JsonNode;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.KafkaSerializationSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> javax.annotation.Nullable;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyKafkaSerialization</span> <span class="keyword">implements</span> <span class="title">KafkaSerializationSchema</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; serialize(String s, <span class="meta">@Nullable</span> Long aLong) &#123;</span><br><span class="line">        JsonNode jsonNode = JSONUtil.stringToJson(s);</span><br><span class="line">        String topic = jsonNode.get(<span class="string">"topic"</span>).asText();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;(topic, s.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Flink-Sink-Diff-Kafka-Topic-Demo"><a href="#Flink-Sink-Diff-Kafka-Topic-Demo" class="headerlink" title="Flink Sink Diff Kafka Topic Demo"></a>Flink Sink Diff Kafka Topic Demo</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xxx.flink.app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.xxx.flink.etl.SinkDiffTopicTest;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">3</span>);</span><br><span class="line">        </span><br><span class="line">        Properties kafkaProperties = <span class="keyword">new</span> Properties();</span><br><span class="line">        kafkaProperties.setProperty(<span class="string">"bootstrap.servers"</span>, Constant.KAFKA_BOOTSTRAP_SERVERS);</span><br><span class="line">        kafkaProperties.setProperty(<span class="string">"enable.auto.commit"</span>, Constant.KAFKA_ENABLE_AUTO_COMMIT);</span><br><span class="line">        kafkaProperties.setProperty(<span class="string">"group.id"</span>, Constant.KAFKA_GROUP_ID);</span><br><span class="line">        </span><br><span class="line">        DataStream&lt;String&gt; ds = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), kafkaProperties).setStartFromGroupOffsets());</span><br><span class="line"></span><br><span class="line">        ds.addSink(<span class="keyword">new</span> FlinkKafkaProducer&lt;&gt;(<span class="string">""</span>, <span class="keyword">new</span> MyKafkaSerialization(), kafkaProperties, FlinkKafkaProducer.Semantic.EXACTLY_ONCE));</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 资源占用评估</title>
    <url>/2021/11/05/Flink%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>Flink 程序资源占用评估方法、Flink On Yarn启动脚本参数说明</p>
<a id="more"></a>
<hr>
<h4 id="Flink-程序资源占用评估"><a href="#Flink-程序资源占用评估" class="headerlink" title="Flink 程序资源占用评估"></a>Flink 程序资源占用评估</h4><ol>
<li><p>程序最大并行度</p>
<p>程序的最大并行度在 <strong>启动脚本 -p</strong> 和 <strong>程序内指定的 parallelism</strong> 两者中取最大值</p>
<p>程序需要的 Slot = 最大的 parallelism</p>
<p><strong>vcore = 程序需要的 Slot + 1（ jobManager）</strong></p>
</li>
<li><p>-ys 数量（每个 taskmanager 的 Slot 数量）</p>
<p>taskManager = 程序需要的 Slot / Slot     （向上取整）</p>
<p><strong>container = taskManager + 1（ jobManager）</strong></p>
<p><strong>Allocate Memory = container * 3GB</strong> （这里的 3GB 是 yarn-site 配置的每个 container 内存大小）</p>
</li>
</ol>
<h4 id="启动主要脚本参数说明"><a href="#启动主要脚本参数说明" class="headerlink" title="启动主要脚本参数说明"></a>启动主要脚本参数说明</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-ys  yarn slots 指定每个 taskmanager 的 slots 数量</span><br><span class="line">-p   parallelism, it is the count the job be divided into. the count of subtask.（取max）（对应 vcore 数量 - <span class="number">1</span>）</span><br><span class="line">-yjm job manager 占用所在 container 的<span class="number">1</span>G内存</span><br><span class="line">-ytm task manager 占用所在 container 的<span class="number">1</span>G内存</span><br><span class="line">-c   指定主类</span><br><span class="line">-ynm Yarn 上作业的名名字</span><br></pre></td></tr></table></figure>

<h4 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/cloudera/parcels/FLINK/lib/flink/bin/flink run \</span><br><span class="line">-yD env.java.opts="-Dfile.encoding=UTF-8 -Dsun.jnu.encoding=UTF-8" \</span><br><span class="line">-m yarn-cluster \</span><br><span class="line">-ytm 16384 \</span><br><span class="line">-ynm data-real-time-sink-doris \</span><br><span class="line">-c org.app.SinkDorisApp \</span><br><span class="line">/opt/realtime/flink-first-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<h4 id="Flink-Task-Manager-内存配置"><a href="#Flink-Task-Manager-内存配置" class="headerlink" title="Flink Task Manager 内存配置"></a>Flink Task Manager 内存配置</h4><p>Task Manager 内存的分配计算可以使用以下 excel 进行计算预估</p>
<p><a href="https://github.com/emerkfu/Flink-Memory-Compute/blob/main/%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97.xlsx" target="_blank" rel="noopener">https://github.com/emerkfu/Flink-Memory-Compute/blob/main/%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97.xlsx</a></p>
<p><img src="/2021/11/05/Flink%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D.png" alt="Flink 内存分配"></p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Watermark Window 的理解</title>
    <url>/2021/04/11/FlinkWatermarkWindow%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>Watermark Window 的一些理解</p>
<a id="more"></a>
<hr>
<h4 id="Watermark"><a href="#Watermark" class="headerlink" title="Watermark"></a>Watermark</h4><p>在一定的时间范围内允许迟到的时间</p>
<p>不建议设置等待的时间太长，一般等待 5s ~ 10s</p>
<p>所以 WaterMark 主要用来解决延迟不那么多的情况</p>
<p>当然也要权衡业务场景，如果不考虑数据的时效性，可以长时间等待</p>
<p>流处理从事件产生，流经 source 再到 operator，中间是有一个过程和时间的。</p>
<p>虽然大部分情况下，流转到 operator 的数据都是按照时间产生的时间顺序来的，但是也不排除由于网络延迟等原因，导致乱序的产生，特别是使用 Kafka 的话，多个分区的数据无法保证有序。</p>
<p>所以，在进行 window 计算的时候，我们不能无限期的等下去，必须要有一个机制来保证一个特定的时间后，必须触发 window 进行计算，这个特别的机制就是 WaterMark，WaterMark 适用于处理乱序事件的。WaterMark 可以翻译为水位线。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WaterMark 是一种度量 eventTime 进度的机制，</span><br><span class="line">WaterMark 作为流数据中的一部分在 stream 中流动，并携带 timestamp。</span><br><span class="line">一个 WaterMark(t) 表明在流中处理的 eventTime 已经到达了 t，</span><br><span class="line">那么在流中就不会再有 eventTime 小于 t 的时间产生。</span><br></pre></td></tr></table></figure>



<p>Window + WaterMark + eventTime 一起使用</p>
<p>确定了窗口的大小后，Flink 会将窗口的间隔预先划分好</p>
<p>Window 触发的时间、条件：</p>
<p>1.WaterMark 时间 &gt;= Window_end_time</p>
<p>2.[ Window_start_time, Window_end_time ) 区间中有数据存在，注意是左闭右开的区间，而且是以 eventTime 来计算的</p>
<h4 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h4><p>聚合时间（例如计数、求和）在流数据上的工作方式与批处理不同。</p>
<p>对流中的所有元素进行计数是不可能的，通常流是无限的（无界的），所以流数据的聚合需要由 Window 来划定范围。</p>
<p>例如：计算过去 5 分钟…；最后 100 个元素的和…</p>
<p>Window 是一种可以把无限数据流分割为有限数据块的手段。</p>
<p>窗口可以是<strong>事件驱动</strong>的 [ Time Window ] （例如：每 30 秒）</p>
<p>或者是<strong>数据驱动</strong>的 [ Count Window ]（例如：每 100 个元素）</p>
<p><strong>窗口的类型</strong></p>
<p>tumbling window：滚动窗口 [ 没有重叠 ]</p>
<p>sliding window：滑动窗口 [ 有重叠 ]</p>
<p>session window：会话窗口</p>
<p>global window：没有窗口</p>
<p><strong>Flink 的灵活性</strong></p>
<p>state -&gt; 自定义算子</p>
<p>global window + trigger + evictor 自定义更复杂的窗口规则</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 本地开发 Web UI DashBoard</title>
    <url>/2021/07/26/Flink%E6%9C%AC%E5%9C%B0%E8%B0%83%E8%AF%95WebUI/</url>
    <content><![CDATA[<p>Flink 本地调试 Web UI DashBoard</p>
<a id="more"></a>
<hr>
<h4 id="Pom"><a href="#Pom" class="headerlink" title="Pom"></a>Pom</h4><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-runtime-web_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xxx.flink.app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.xxx.flink.etl.UbtOdsEtl;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.RestOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.CheckpointingMode;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.setString(RestOptions.BIND_PORT, <span class="string">"8071-8089"</span>);</span><br><span class="line">        <span class="comment">//本地 WebUI DashBoard</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交 Flink 集群使用</span></span><br><span class="line">        <span class="comment">//StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span></span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">3</span>);</span><br><span class="line">        env.enableCheckpointing(<span class="number">60000</span>);</span><br><span class="line">        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"></span><br><span class="line">        UbtOdsEtl.etl(env);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/07/26/Flink%E6%9C%AC%E5%9C%B0%E8%B0%83%E8%AF%95WebUI/FlinkWebUIDashBoard.png" alt="Flink WebUI DashBoard"></p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 获取最新的 Checkpoint 路径</title>
    <url>/2021/08/07/Flink%E8%8E%B7%E5%8F%96Checkpoint%E8%B7%AF%E5%BE%84/</url>
    <content><![CDATA[<p>Flink On Yarn运行时，作业的 ApplicationId 是随机生成的</p>
<p>Checkpoint 保存的位置是使用 ApplicationId 生成的路径（我们保存在 HDFS）</p>
<p>以下 Shell 脚本可以自动获取最新的 Checkpoint 保存路径</p>
<a id="more"></a>
<hr>
<h4 id="lastCheckpointExternalPath-sh"><a href="#lastCheckpointExternalPath-sh" class="headerlink" title="lastCheckpointExternalPath.sh"></a>lastCheckpointExternalPath.sh</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/sh</span></span><br><span class="line"></span><br><span class="line">jobname=$1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 获取 checkpointid_path </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -t                                  按照时间取最新（倒序）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -C                                  只显示值</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> awk <span class="string">'NR==1'</span>                         取第一行</span></span><br><span class="line">checkpointid_path=`hadoop fs -ls -t -C /opt/user/flink/checkpoint/"$jobname" | awk 'NR==1'`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 获取 lastCheckpointExternalPath</span></span></span><br><span class="line">lastCheckpointExternalPath=`hadoop fs -ls -t -C "$checkpointid_path" | awk 'NR==1'`</span><br><span class="line"></span><br><span class="line">echo "hdfs://hmdservice$lastCheckpointExternalPath"</span><br></pre></td></tr></table></figure>

<h4 id="启动脚本-demo"><a href="#启动脚本-demo" class="headerlink" title="启动脚本 demo"></a>启动脚本 demo</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">flinkUbtOdsCheckpointPath=`sh /opt/sync/sync_script/flink/lastCheckpointExternalPath.sh flink-ubt-ods`</span><br><span class="line">flinkdimaccountCheckpointPath=`sh /opt/sync/sync_script/flink/lastCheckpointExternalPath.sh dim_account_info`</span><br><span class="line">flinkfbehaviourcdpCheckpointPath=`sh /opt/sync/sync_script/flink/lastCheckpointExternalPath.sh flink-ubt-m4-flow`</span><br><span class="line">flink_homedo_realdata_chk=`sh /opt/sync/sync_script/flink/lastCheckpointExternalPath.sh flow_order_item_detail_real`</span><br><span class="line"></span><br><span class="line">ssh root@10.0.**.*** &gt; /dev/null 2&gt;&amp;1 &lt;&lt; eeooff</span><br><span class="line"></span><br><span class="line">  nohup /opt/cloudera/parcels/FLINK/lib/flink/bin/flink run -m yarn-cluster -ynm flink-ubt-ods -c com.homedo.flink.app.Main -s $flinkUbtOdsCheckpointPath /opt/flink_job/flink-ubt-ods-1.0.0.jar &gt; /dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">  nohup /opt/cloudera/parcels/FLINK/lib/flink/bin/flink run -m yarn-cluster -ynm flink-dwd -c com.homedo.flink.app.Main /opt/flink_job/flink-dwd-1.0.0.jar &gt; /dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">  sh /opt/flink_job/flink-realdata-run.sh</span><br><span class="line"></span><br><span class="line">  sh /opt/flink_job/flink-realdata-run2.sh</span><br><span class="line"></span><br><span class="line">nohup /opt/cloudera/parcels/FLINK/lib/flink/bin/flink run -m yarn-cluster -ynm dim_account_info -s $flinkdimaccountCheckpointPath -c com.homedo.user.app.AsyncJoin /opt/flink_job/dim_account_info-1.0-SNAPSHOT-jar-with-dependencies.jar &gt; /dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">nohup /opt/cloudera/parcels/FLINK/lib/flink/bin/flink run -m yarn-cluster -ynm flink-ubt-m4-flow -s $flinkfbehaviourcdpCheckpointPath -c com.homedo.flink.app.Main /opt/flink_job/flink-ubt-ods-m4-flow-1.0.0-jar-with-dependencies.jar &gt; /opt/flink_job/log/ubt-m4-flow_`date +%Y%m%d_%H%M%S`.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">nohup /opt/cloudera/parcels/FLINK/lib/flink/bin/flink run -m yarn-cluster -ynm flow_order_item_detail_real -yqu users.flink -s $flink_homedo_realdata_chk -c com.homedo.HmdApplication /opt/flink_job/homedo_realdata-1.0.jar &gt; /opt/flink_job/log/orderitem_detailreal_`date +%Y%m%d_%H%M%S`.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">  exit</span><br><span class="line"></span><br><span class="line">eeooff</span><br></pre></td></tr></table></figure>



<h4 id="kill-all-flink-job-sh"><a href="#kill-all-flink-job-sh" class="headerlink" title="kill_all_flink_job.sh"></a>kill_all_flink_job.sh</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">/opt/cloudera/parcels/CDH/bin/yarn application --list &gt; /opt/sync/sync_script/flink/flink-job.txt</span><br><span class="line"></span><br><span class="line">cat /opt/sync/sync_script/flink/flink-job.txt | while read line</span><br><span class="line"></span><br><span class="line">do</span><br><span class="line"></span><br><span class="line">  if [[ $line == *"Apache Flink"* ]]</span><br><span class="line"></span><br><span class="line">  then</span><br><span class="line"></span><br><span class="line">    array=($&#123;line//'\t'/ &#125;)</span><br><span class="line"></span><br><span class="line">    application_id=$&#123;array[0]&#125;</span><br><span class="line"></span><br><span class="line">    /opt/cloudera/parcels/CDH/bin/yarn application --kill $application_id</span><br><span class="line"></span><br><span class="line">    rm -f /opt/sync/sync_script/flink/flink-job.txt</span><br><span class="line"></span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 获取 Kafka Metadata</title>
    <url>/2021/07/28/Flink%E8%8E%B7%E5%8F%96KafkaOffSet/</url>
    <content><![CDATA[<p>Flink 消费 Kafka 数据时，有些场景需要获取 Kafka Partition、Offset</p>
<p>可以使用 JSONKeyValueDeserializationSchema 反序列化器获取 Kafka 数据的 Metadata 信息</p>
<a id="more"></a>
<hr>
<h4 id="代码片段"><a href="#代码片段" class="headerlink" title="代码片段"></a>代码片段</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> DataStream&lt;ObjectNode&gt; <span class="title">getWithSchema</span><span class="params">(StreamExecutionEnvironment env, String topic)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> JSONKeyValueDeserializationSchema(<span class="keyword">true</span>), kafkaProperties).setStartFromGroupOffsets());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="JSONKeyValueDeserializationSchema-反序列化器获得的数据"><a href="#JSONKeyValueDeserializationSchema-反序列化器获得的数据" class="headerlink" title="JSONKeyValueDeserializationSchema 反序列化器获得的数据"></a>JSONKeyValueDeserializationSchema 反序列化器获得的数据</h4><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"value"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"xwhen"</span>:<span class="number">1628419647453</span>,</span><br><span class="line">            <span class="attr">"xcontext"</span>:&#123;</span><br><span class="line">                <span class="attr">"$lib"</span>:<span class="string">"JS"</span>,</span><br><span class="line">                <span class="attr">"$debug"</span>:<span class="number">2</span>,</span><br><span class="line">                <span class="attr">"$is_login"</span>:<span class="literal">false</span>,</span><br><span class="line">                <span class="attr">"$first_visit_language"</span>:<span class="string">"zh-cn"</span>,</span><br><span class="line">                <span class="attr">"$first_visit_time"</span>:<span class="string">"2021-08-08 18:47:27.453"</span>,</span><br><span class="line">                <span class="attr">"$lib_version"</span>:<span class="string">"4.5.5.1"</span>,</span><br><span class="line">                <span class="attr">"$platform"</span>:<span class="string">"JS"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"appid"</span>:<span class="string">"66f27704d3162247"</span>,</span><br><span class="line">            <span class="attr">"xwho"</span>:<span class="string">"JSe81d8850b1da3aa471658c6f99ac1a0fe81d"</span>,</span><br><span class="line">            <span class="attr">"xwhat"</span>:<span class="string">"$profile_set_once"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"metadata"</span>:&#123;</span><br><span class="line">        <span class="attr">"offset"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"topic"</span>:<span class="string">"ubt_data_collection_202105"</span>,</span><br><span class="line">        <span class="attr">"partition"</span>:<span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume安装部署</title>
    <url>/2018/08/15/Flume%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h4 id="安装地址"><a href="#安装地址" class="headerlink" title="安装地址"></a>安装地址</h4><p>Flume官网地址<br><a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p>
<p>文档查看地址<br><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></p>
<p>下载地址<br><a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz</a></p>
<a id="more"></a>
<hr>
<h4 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h4><ol>
<li>将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</li>
<li>解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li>修改apache-flume-1.7.0-bin的名称为flume<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure></li>
<li>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv flume-env.sh.template flume-env.sh</span><br><span class="line">vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><ol>
<li><p>安装netcat工具</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y nc</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断44444端口是否被占用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo netstat -tunlp | grep 44444</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建Flume Agent配置文件flume-netcat-logger.conf<br>在flume目录下创建job文件夹并进入job文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir job</span><br><span class="line">cd job/</span><br></pre></td></tr></table></figure>
</li>
<li><p>在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent    <span class="comment">#a1表示agent的名称</span></span></span><br><span class="line">a1.sources = r1		#r1表示a1的输入源</span><br><span class="line">a1.sinks = k1		#k1表示a1的输出目的地</span><br><span class="line">a1.channels = c1	#c1表示a1的缓冲区</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat		#表示a1的输入源类型为netcat端口类型</span><br><span class="line">a1.sources.r1.bind = localhost	#表示a1的监听的主机</span><br><span class="line">a1.sources.r1.port = 44444		#表示a1的监听的端口号</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger		#表示a1的输出目的地是控制台logger类型</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory		#表示a1的channel类型是memory内存型</span><br><span class="line">a1.channels.c1.capacity = 1000		#表示a1的channel总容量1000个event</span><br><span class="line">a1.channels.c1.transactionCapacity = 10		#表示a1的channel传输时收集到了100条event以后再去提交事务</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1		#将r1和c1连接起来</span><br><span class="line">a1.sinks.k1.channel = c1		#将k1和c1连接起来</span><br></pre></td></tr></table></figure>
</li>
<li><p>先开启flume监听端口<br>第一种写法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p>第二种写法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flume-ng agent -c conf/ -n a1 –f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p>参数说明：<br>​ –conf conf/：表示配置文件存储在conf/目录<br>​ –name a1：表示给agent起名为a1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">`--conf-file job/flume-netcat.conf ：`flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</span><br></pre></td></tr></table></figure>
<p>​ –Dflume.root.logger==INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p>
</li>
<li><p>使用netcat工具向本机的44444端口发送内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nc localhost 44444</span><br><span class="line">hello </span><br><span class="line">aliyun</span><br></pre></td></tr></table></figure>
</li>
<li><p>在Flume监听页面观察接收数据情况</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume 应用配置</title>
    <url>/2021/04/19/Flume%E5%BA%94%E7%94%A8%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>Flume使用的版本为1.6.0。</p>
<p>几种Flume的使用场景Config配置。</p>
<a id="more"></a>
<hr>
<h4 id="netcat-Source-Sink-To-Console"><a href="#netcat-Source-Sink-To-Console" class="headerlink" title="netcat Source Sink To Console"></a>netcat Source Sink To Console</h4><p>用于测试，控制台输入，控制台打印。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agent1.sources = kafkaSource</span><br><span class="line">agent1.channels = mc1</span><br><span class="line">agent1.sinks = avro-sink</span><br><span class="line"></span><br><span class="line">agent1.sources.kafkaSource.channels = mc1</span><br><span class="line">agent1.sinks.avro-sink.channel = mc1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span></span></span><br><span class="line">agent1.sources.kafkaSource.type = netcat</span><br><span class="line">agent1.sources.kafkaSource.bind = localhost</span><br><span class="line">agent1.sources.kafkaSource.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">channel1</span></span><br><span class="line">agent1.channels.mc1.type = memory</span><br><span class="line">agent1.channels.mc1.capacity = 10000</span><br><span class="line">agent1.channels.mc1.transactionCapacity = 10000</span><br><span class="line">agent1.channels.mc1.keep-alive = 60</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">sink1</span></span><br><span class="line">agent1.sinks.avro-sink.type = logger</span><br></pre></td></tr></table></figure>

<h4 id="Kafka-Source-Sink-To-Console"><a href="#Kafka-Source-Sink-To-Console" class="headerlink" title="Kafka Source Sink To Console"></a>Kafka Source Sink To Console</h4><p>用于测试，消费Kafka的数据，打印到控制台。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agent1.sources = logsource</span><br><span class="line">agent1.channels = mc1</span><br><span class="line">agent1.sinks = avro-sink</span><br><span class="line"></span><br><span class="line">agent1.sources.logsource.channels = mc1</span><br><span class="line">agent1.sinks.avro-sink.channel = mc1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span></span></span><br><span class="line">agent1.sources.logsource.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent1.sources.logsource.zookeeperConnect = cdh9:2181/kafka</span><br><span class="line">agent1.sources.logsource.topic = ubtDemo</span><br><span class="line">agent1.sources.logsource.groupId = flume</span><br><span class="line">agent1.sources.logsource.kafka.consumer.timeout.ms = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">channel1</span></span><br><span class="line">agent1.channels.mc1.type = memory</span><br><span class="line">agent1.channels.mc1.capacity = 10000</span><br><span class="line">agent1.channels.mc1.transactionCapacity = 10000</span><br><span class="line">agent1.channels.mc1.keep-alive = 60</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">sink1</span></span><br><span class="line">agent1.sinks.avro-sink.type = logger</span><br></pre></td></tr></table></figure>

<p>启动命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;flume-ng agent -c &#x2F;opt&#x2F;software&#x2F;flume&#x2F;apache-flume-1.6.0-bin&#x2F;conf -f &#x2F;opt&#x2F;software&#x2F;flume&#x2F;apache-flume-1.6.0-bin&#x2F;jobs&#x2F;flume-kafka-source1.conf -n agent1 -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure>

<h4 id="Kafka-Source-Sink-To-Kafka"><a href="#Kafka-Source-Sink-To-Kafka" class="headerlink" title="Kafka Source Sink To Kafka"></a>Kafka Source Sink To Kafka</h4><p>消费Kafka数据，Sink Kafka，数据的转发，</p>
<p>我们的应用场景：Source Kafka版本过低且不方便更新，所以使用Flume做消息的高低版本适配。</p>
<p>Flume 1.6.0 同时使用Kafka Source 、Kafka Sink时存在Bug：</p>
<p>配置的Sink Topic name会被Source Topic name覆盖失效，数据不会Sink到目标Kafka Topic，会循环Sink到Source Kafka Topic。<br>增加拦截器，使Sink Topic name 生效，该问题解决。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">agent1.sources &#x3D; kafkaSource</span><br><span class="line">agent1.channels &#x3D; mc1</span><br><span class="line">agent1.sinks &#x3D; avro-sink</span><br><span class="line"></span><br><span class="line">agent1.sources.kafkaSource.channels &#x3D; mc1</span><br><span class="line">agent1.sinks.avro-sink.channel &#x3D; mc1</span><br><span class="line"></span><br><span class="line">#source</span><br><span class="line">agent1.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent1.sources.kafkaSource.zookeeperConnect &#x3D; cdh9:2181&#x2F;kafka</span><br><span class="line">agent1.sources.kafkaSource.topic &#x3D; ubtDemo</span><br><span class="line">agent1.sources.kafkaSource.groupId &#x3D; flume</span><br><span class="line">agent1.sources.kafkaSource.kafka.consumer.timeout.ms &#x3D; 100</span><br><span class="line"></span><br><span class="line">#增加拦截器，使Sink Topic name 生效，解决Source Topic name覆盖失效问题</span><br><span class="line">agent1.sources.kafkaSource.interceptors &#x3D; i1</span><br><span class="line">agent1.sources.kafkaSource.interceptors.i1.type &#x3D; static</span><br><span class="line">agent1.sources.kafkaSource.interceptors.i1.key &#x3D; flumetest</span><br><span class="line">agent1.sources.kafkaSource.interceptors.i1.preserveExisting &#x3D; false</span><br><span class="line">agent1.sources.kafkaSource.interceptors.i1.value &#x3D; sinkTopic</span><br><span class="line"></span><br><span class="line">#channel1</span><br><span class="line">agent1.channels.mc1.type &#x3D; memory</span><br><span class="line">agent1.channels.mc1.capacity &#x3D; 10000</span><br><span class="line">agent1.channels.mc1.transactionCapacity &#x3D; 10000</span><br><span class="line">agent1.channels.mc1.keep-alive &#x3D; 60</span><br><span class="line"></span><br><span class="line">#sink1</span><br><span class="line">agent1.sinks.avro-sink.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">#agent1.sinks.avro-sink.topic &#x3D; flumetest</span><br><span class="line">agent1.sinks.avro-sink.brokerList &#x3D; cdh9:9092</span><br><span class="line">agent1.sinks.avro-sink.requiredAcks &#x3D; 1</span><br><span class="line">agent1.sinks.avro-sink.batchSize &#x3D; 20</span><br><span class="line">agent1.sinks.avro-sink.channel &#x3D; mc1</span><br></pre></td></tr></table></figure>

<p>启动命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;flume-ng agent -c &#x2F;opt&#x2F;software&#x2F;flume&#x2F;apache-flume-1.6.0-bin&#x2F;conf -f &#x2F;opt&#x2F;software&#x2F;flume&#x2F;apache-flume-1.6.0-bin&#x2F;jobs&#x2F;flume-kafka-source-kafka-sink.conf -n agent1 -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume架构基础</title>
    <url>/2018/08/14/Flume%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h4 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h4><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。<br>Flume基于流式架构，灵活简单。</p>
<a id="more"></a>
<hr>
<h4 id="Flume的优点"><a href="#Flume的优点" class="headerlink" title="Flume的优点"></a>Flume的优点</h4><ol>
<li>可以和任意存储进程集成。</li>
<li>输入的的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。</li>
<li>flume中的事务基于channel，使用了两个事务模型（sender + receiver），确保消息被可靠发送。</li>
</ol>
<p>Flume使用两个独立的事务分别负责从soucrce到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为该数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。</p>
<h4 id="Flume组成架构"><a href="#Flume组成架构" class="headerlink" title="Flume组成架构"></a>Flume组成架构</h4><p><img src="/2018/08/14/Flume%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/flume.jpg" alt="Flume组成架构"></p>
<ul>
<li><p>Agent<br>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。<br>Agent主要有3个部分组成，Source、Channel、Sink。</p>
</li>
<li><p>Source<br>Source是负责接收数据到Flume Agent的组件。<br>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p>
</li>
<li><p>Channel<br>Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。<br>Flume自带两种Channel：Memory Channel和File Channel。</p>
</li>
</ul>
<ol>
<li>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</li>
<li>File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</li>
</ol>
<ul>
<li><p>Sink<br>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。<br>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。<br>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p>
</li>
<li><p>Event<br>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。 Event由可选的header和载有数据的一个byte array 构成。Header是容纳了key-value字符串对的HashMap。</p>
</li>
</ul>
<h3 id="Flume拓扑结构"><a href="#Flume拓扑结构" class="headerlink" title="Flume拓扑结构"></a>Flume拓扑结构</h3><h4 id="Flume-Agent连接"><a href="#Flume-Agent连接" class="headerlink" title="Flume Agent连接"></a>Flume Agent连接</h4><p><img src="/2018/08/14/Flume%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/flume2.jpg" alt="Flume Agent连接"></p>
<p>这种模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p>
<h4 id="单source，多channel、sink"><a href="#单source，多channel、sink" class="headerlink" title="单source，多channel、sink"></a>单source，多channel、sink</h4><p><img src="/2018/08/14/Flume%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/flume3.jpg" alt="Flume 单source，多channel、sink"></p>
<p>Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送的不同的目的地。</p>
<h4 id="Flume负载均衡"><a href="#Flume负载均衡" class="headerlink" title="Flume负载均衡"></a>Flume负载均衡</h4><p><img src="/2018/08/14/Flume%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/flume4.jpg" alt="Flume Flume负载均衡"></p>
<p>Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。</p>
<h4 id="Flume-Agent聚合"><a href="#Flume-Agent聚合" class="headerlink" title="Flume Agent聚合"></a>Flume Agent聚合</h4><p><img src="/2018/08/14/Flume%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/flume5.jpg" alt="Flume Flume负载均衡"></p>
<p>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。</p>
]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume案例</title>
    <url>/2018/08/17/Flume%E6%A1%88%E4%BE%8B/</url>
    <content><![CDATA[<p>Flume的一些配置案例</p>
<a id="more"></a>
<hr>
<h4 id="实时读取本地文件到HDFS案例"><a href="#实时读取本地文件到HDFS案例" class="headerlink" title="实时读取本地文件到HDFS案例"></a>实时读取本地文件到HDFS案例</h4><p>案例需求：实时监控Hive日志，并上传到HDFS中<br><img src="/2018/08/17/Flume%E6%A1%88%E4%BE%8B/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A31.jpg" alt="单源单出口1"></p>
<ol>
<li><p>给Flume的lib目录下添加aliyun相关的jar包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">commons-configuration-1.6.jar</span><br><span class="line">aliyun-auth-2.7.2.jar</span><br><span class="line">aliyun-common-2.7.2.jar</span><br><span class="line">aliyun-hdfs-2.7.2.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume-file-hdfs.conf文件<br>创建文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 job]$ touch flume-file-hdfs.conf</span><br></pre></td></tr></table></figure>
<p>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 job]$ vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r2                             #定义source</span><br><span class="line">a2.sinks = k2                               #定义sink</span><br><span class="line">a2.channels = c2                            #定义channels</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r2.type = exec                   #定义source类型为exec可执行命令的</span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a2.sources.r2.shell = /bin/bash -c          #执行shell脚本的绝对路径</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/%Y%m%d/%H</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-         #上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.round = true               #是否按照时间滚动文件夹		</span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1             #多少时间单位创建一个新的文件夹	</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour           #重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true   #是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 1000           #积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream      #设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 60          #多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700       #设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0              #文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>
<p>注意：<br>对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p>
</li>
<li><p>执行监控配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>开启aliyun和Hive并操作Hive产生日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 aliyun-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[emerk@hadoop001 aliyun-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[emerk@hadoop001 hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在HDFS上查看文件。</p>
</li>
</ol>
<h4 id="实时读取目录文件到HDFS案例"><a href="#实时读取目录文件到HDFS案例" class="headerlink" title="实时读取目录文件到HDFS案例"></a>实时读取目录文件到HDFS案例</h4><p>案例需求：使用Flume监听整个目录的文件</p>
<p><img src="/2018/08/17/Flume%E6%A1%88%E4%BE%8B/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A32.jpg" alt="单源单出口2"></p>
<ol>
<li><p>创建配置文件flume-dir-hdfs.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 job]$ touch flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>打开文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 job]$ vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a3.sources = r3		#定义sources</span><br><span class="line">a3.sinks = k3		#定义sink</span><br><span class="line">a3.channels = c3	#定义channel</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r3.type = spooldir                       #定义souce类型为目录</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload   #定义监控目录</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED               #定义文件上传完的后缀</span><br><span class="line">a3.sources.r3.fileHeader = true                     #是否有文件头</span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)          #忽略所有以.tmp结尾的文件，不上传</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs                             #sink类型为hdfs</span><br><span class="line">a3.sinks.k3.hdfs.pat=hdfs://hadoop001:9000/flume/upload/%Y%m%d/%H  #文件上传到hdfs的路径</span><br><span class="line"></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-               #上传文件到hdfs的前缀</span><br><span class="line">a3.sinks.k3.hdfs.round = true                       #是否按照时间滚动文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1                     #多少时间单位创建一个新的文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour                   #重新定义时间单位</span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true           #是否使用本地时间戳</span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100                    #积攒多少个Event才flush到HDFS一次</span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream              #设置文件类型，可支持压缩</span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60                  #多久生成一个新的文件</span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700               #设置每个文件的滚动大小大概是128M</span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0                      #文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动监控文件夹命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure>
<p>说明： 在使用Spooling Directory Source时</p>
<ol>
<li>不要在监控目录中创建并持续修改文件</li>
<li>上传完成的文件会以.COMPLETED结尾</li>
<li>被监控文件夹每500毫秒扫描一次文件变动</li>
</ol>
</li>
<li><p>向upload文件夹中添加文件<br>在/opt/module/flume目录下创建upload文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 flume]$ mkdir upload</span><br></pre></td></tr></table></figure>
<p>向upload文件夹中添加文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 upload]$ touch aliyun.txt</span><br><span class="line">[emerk@hadoop001 upload]$ touch aliyun.tmp</span><br><span class="line">[emerk@hadoop001 upload]$ touch aliyun.log</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看HDFS上的数据</p>
</li>
<li><p>等待1s，再次查询upload文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 upload]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 emerk emerk 0 8月  20 22:31 aliyun.log.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 emerk emerk 0 8月  20 22:31 aliyun.tmp</span><br><span class="line">-rw-rw-r--. 1 emerk emerk 0 8月  20 22:31 aliyun.txt.COMPLETED</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="单数据源多出口案例-选择器"><a href="#单数据源多出口案例-选择器" class="headerlink" title="单数据源多出口案例(选择器)"></a>单数据源多出口案例(选择器)</h4><p>案例需求：<br>使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。<br>同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。</p>
<p><img src="/2018/08/17/Flume%E6%A1%88%E4%BE%8B/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A31.jpg" alt="单源多出口1"></p>
<ol>
<li><p>准备工作<br>在/opt/module/flume/job目录下创建group1文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 job]$ cd group1/</span><br></pre></td></tr></table></figure>
<p>在/opt/module/datas/目录下创建flume3文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 datas]$ mkdir flume3</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume-file-flume.conf<br>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。<br>创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 group1]$ touch flume-file-flume.conf</span><br><span class="line">[emerk@hadoop001 group1]$ vim flume-file-flume.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将数据流复制给所有channel</span></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sink端的avro是一个数据发送者</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop001 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop001</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>
<p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。<br>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p>
</li>
<li><p>创建flume-flume-hdfs.conf<br>配置上级Flume输出的Source，输出是到HDFS的Sink。<br>创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 group1]$ touch flume-flume-hdfs.conf</span><br><span class="line">[emerk@hadoop001 group1]$ vim flume-flume-hdfs.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>端的avro是一个数据接收服务</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop001</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://hadoop001:9000/flume2/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume-flume-dir.conf<br>配置上级Flume输出的Source，输出是到本地目录的Sink。<br>创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 group1]$ touch flume-flume-dir.conf</span><br><span class="line">[emerk@hadoop001 group1]$ vim flume-flume-dir.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop001</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>
<p>提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p>
</li>
<li><p>执行配置文件<br>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</span><br><span class="line">[emerk@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</span><br><span class="line">[emerk@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动aliyun和Hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 aliyun-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[emerk@hadoop001 aliyun-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line">[emerk@hadoop001 hive]$ bin/hive</span><br><span class="line"></span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查HDFS上数据</p>
</li>
<li><p>检查/opt/module/datas/flume3目录中数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 flume3]$ ll</span><br><span class="line"></span><br><span class="line">总用量 8</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 5942 5月 22 00:09 1526918887550-3</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="单数据源多出口案例-Sink组"><a href="#单数据源多出口案例-Sink组" class="headerlink" title="单数据源多出口案例(Sink组)"></a>单数据源多出口案例(Sink组)</h4><p>案例需求：<br>使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。<br>同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS</p>
<p><img src="/2018/08/17/Flume%E6%A1%88%E4%BE%8B/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A32.jpg" alt="单源多出口2"></p>
<ol>
<li><p>准备工作<br>在/opt/module/flume/job目录下创建group2文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 job]$ cd group2/</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume-netcat-flume.conf<br>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。<br>创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 group2]$ touch flume-netcat-flume.conf</span><br><span class="line">[emerk@hadoop001 group2]$ vim flume-netcat-flume.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop001</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop001</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>
<p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。<br>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p>
</li>
<li><p>创建flume-flume-console1.conf<br>配置上级Flume输出的Source，输出是到本地控制台。<br>创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 group2]$ touch flume-flume-console1.conf</span><br><span class="line">[emerk@hadoop001 group2]$ vim flume-flume-console1.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop001</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume-flume-console2.conf<br>配置上级Flume输出的Source，输出是到本地控制台。<br>创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 group2]$ touch flume-flume-console2.conf</span><br><span class="line">[emerk@hadoop001 group2]$ vim flume-flume-console2.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop001</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行配置文件<br>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</span><br><span class="line">[emerk@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</span><br><span class="line">[emerk@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用netcat工具向本机的44444端口发送内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc localhost 44444</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看Flume2及Flume3的控制台打印</p>
</li>
</ol>
<h4 id="多数据源汇总案例"><a href="#多数据源汇总案例" class="headerlink" title="多数据源汇总案例"></a>多数据源汇总案例</h4><p>案例需求：</p>
<p>hadoop002上的Flume-1监控文件/opt/module/group.log，</p>
<p>hadoop001上的Flume-2监控某一个端口的数据流，</p>
<p>Flume-1与Flume-2将数据发送给aliyun104上的Flume-3，Flume-3将最终数据打印到控制台。</p>
<p><img src="/2018/08/17/Flume%E6%A1%88%E4%BE%8B/%E5%A4%9A%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A3.jpg" alt="多源单出口"></p>
<ol>
<li><p>准备工作<br>分发Flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 module]$ xsync flume</span><br></pre></td></tr></table></figure>
<p>在hadoop001、hadoop002以及hadoop003的/opt/module/flume/job目录下创建一个group3文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 job]$ mkdir group3</span><br><span class="line"></span><br><span class="line">[emerk@hadoop002 job]$ mkdir group3</span><br><span class="line"></span><br><span class="line">[emerk@hadoop003 job]$ mkdir group3</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume1-logger-flume.conf<br>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume。<br>在在hadoop002上创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop002 group3]$ touch flume1-logger-flume.conf</span><br><span class="line"></span><br><span class="line">[emerk@hadoop002 group3]$ vim flume1-logger-flume.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop003</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume2-netcat-flume.conf<br>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：<br>在aliyun102上创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 group3]$ touch flume2-netcat-flume.conf</span><br><span class="line"></span><br><span class="line">[emerk@hadoop001 group3]$ vim flume2-netcat-flume.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = 在hadoop001</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = hadoop003</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume3-flume-logger.conf<br>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。<br>在hadoop003上创建配置文件并打开</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop003 group3]$ touch flume3-flume-logger.conf</span><br><span class="line"></span><br><span class="line">[emerk@hadoop003 group3]$ vim flume3-flume-logger.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop003</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1+</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行配置文件<br>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop003 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-netcat-flume.conf</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop002 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-logger-flume.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>在hadoop002上向/opt/module目录下的group.log追加内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop002 module]$ echo 'hello' &gt; group.log</span><br></pre></td></tr></table></figure>
</li>
<li><p>在hadoop001上向44444端口发送数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 flume]$ telnet hadoop001 44444</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查hadoop001上数据</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume 负载均衡 KafkaSource KafkaSink</title>
    <url>/2021/04/22/Flume%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1KafkaSourceKafkaSink/</url>
    <content><![CDATA[<p>Flume使用的版本为1.6.0。</p>
<p>Flume配置负载均衡，Kafka Source -&gt; Avro -&gt; Avro -&gt; Kafka Sink 场景。</p>
<a id="more"></a>
<hr>
<h3 id="Kafka-Source-Sink-To-Kafka"><a href="#Kafka-Source-Sink-To-Kafka" class="headerlink" title="Kafka Source Sink To Kafka"></a>Kafka Source Sink To Kafka</h3><h4 id="结构信息"><a href="#结构信息" class="headerlink" title="结构信息"></a>结构信息</h4><p><img src="/2021/04/22/Flume%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1KafkaSourceKafkaSink/Flume%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1KafkaSourceKafkaSink.PNG" alt="Flume负载均衡KafkaSourceKafkaSink"></p>
<h4 id="配置信息"><a href="#配置信息" class="headerlink" title="配置信息"></a>配置信息</h4><h5 id="Agent节点配置"><a href="#Agent节点配置" class="headerlink" title="Agent节点配置"></a>Agent节点配置</h5><p>flume_analysys_loadbalance_agent.conf</p>
<p>sink port ip 为目标机器 ip</p>
<p>Tips：<strong>拦截器一定要配置！！！</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agentX.sources = sX</span><br><span class="line">agentX.channels = chX</span><br><span class="line">agentX.sinks = sk0 sk1 sk2</span><br><span class="line"></span><br><span class="line">agentX.sources.sX.channels = chX</span><br><span class="line"></span><br><span class="line">agentX.sources.sX.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agentX.sources.sX.zookeeperConnect = ark1:2181/kafka</span><br><span class="line">agentX.sources.sX.topic = post_66f27704d3162247</span><br><span class="line">agentX.sources.sX.groupId = flume</span><br><span class="line">agentX.sources.sX.kafka.consumer.timeout.ms = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Configure interceptors</span></span><br><span class="line">agentX.sources.sX.interceptors = i1</span><br><span class="line">agentX.sources.sX.interceptors.i1.type = static</span><br><span class="line">agentX.sources.sX.interceptors.i1.key = topic</span><br><span class="line">agentX.sources.sX.interceptors.i1.preserveExisting = false</span><br><span class="line">agentX.sources.sX.interceptors.i1.value = flume-test01</span><br><span class="line"></span><br><span class="line">agentX.channels.chX.type = memory</span><br><span class="line">agentX.channels.chX.capacity = 10000</span><br><span class="line">agentX.channels.chX.transactionCapacity = 10000</span><br><span class="line">agent1.channels.chX.keep-alive = 60</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Configure sinks</span></span><br><span class="line">agentX.sinks.sk0.channel = chX</span><br><span class="line">agentX.sinks.sk0.type = avro</span><br><span class="line">agentX.sinks.sk0.hostname = 10.0.15.182</span><br><span class="line">agentX.sinks.sk0.port = 44441</span><br><span class="line">agentX.sinks.sk1.channel = chX</span><br><span class="line">agentX.sinks.sk1.type = avro</span><br><span class="line">agentX.sinks.sk1.hostname = 10.0.15.181</span><br><span class="line">agentX.sinks.sk1.port = 44441</span><br><span class="line">agentX.sinks.sk2.channel = chX</span><br><span class="line">agentX.sinks.sk2.type = avro</span><br><span class="line">agentX.sinks.sk2.hostname = 10.0.15.133</span><br><span class="line">agentX.sinks.sk2.port = 44441</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Configure loadbalance</span></span><br><span class="line">agentX.sinkgroups = g1</span><br><span class="line">agentX.sinkgroups.g1.sinks = sk0 sk1 sk2</span><br><span class="line">agentX.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">agentX.sinkgroups.g1.processor.backoff=true</span><br><span class="line">agentX.sinkgroups.g1.processor.selector=round_robin</span><br><span class="line"><span class="meta">#</span><span class="bash">agentX.sinkgroups.g1.processor.selector=random</span></span><br></pre></td></tr></table></figure>

<h5 id="load-balance-0-节点配置"><a href="#load-balance-0-节点配置" class="headerlink" title="load balance 0 节点配置"></a>load balance 0 节点配置</h5><p>flume_loadbalance_collector0.conf</p>
<p>bind配置的ip为本机ip</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agent0.sources = s0</span><br><span class="line">agent0.channels = ch0</span><br><span class="line">agent0.sinks = sk0</span><br><span class="line"></span><br><span class="line">agent0.sources.s0.channels = ch0</span><br><span class="line">agent0.sources.s0.type = avro</span><br><span class="line">agent0.sources.s0.bind = 10.0.15.182</span><br><span class="line">agent0.sources.s0.port = 44441</span><br><span class="line"></span><br><span class="line">agent0.channels.ch0.type = memory</span><br><span class="line">agent0.channels.ch0.capacity = 10000</span><br><span class="line">agent0.channels.ch0.transactionCapacity = 10000</span><br><span class="line">agent0.channels.ch0.keep-alive = 60</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">sink0</span></span><br><span class="line">agent0.sinks.sk0.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent0.sinks.sk0.topic = flume-test01</span><br><span class="line">agent0.sinks.sk0.brokerList = 10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092</span><br><span class="line">agent0.sinks.sk0.requiredAcks = 1</span><br><span class="line">agent0.sinks.sk0.batchSize = 20</span><br><span class="line">agent0.sinks.sk0.channel = ch0</span><br></pre></td></tr></table></figure>

<h5 id="load-balance-1-节点配置"><a href="#load-balance-1-节点配置" class="headerlink" title="load balance 1 节点配置"></a>load balance 1 节点配置</h5><p>flume_loadbalance_collector1.conf</p>
<p>bind配置的ip为本机ip</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agent1.sources = s1</span><br><span class="line">agent1.channels = ch1</span><br><span class="line">agent1.sinks = sk1</span><br><span class="line"></span><br><span class="line">agent1.sources.s1.channels = ch1</span><br><span class="line">agent1.sources.s1.type = avro</span><br><span class="line">agent1.sources.s1.bind = 10.0.15.181</span><br><span class="line">agent1.sources.s1.port = 44441</span><br><span class="line"></span><br><span class="line">agent1.channels.ch1.type = memory</span><br><span class="line">agent1.channels.ch1.capacity = 10000</span><br><span class="line">agent1.channels.ch1.transactionCapacity = 10000</span><br><span class="line">agent1.channels.ch1.keep-alive = 60</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">sink1</span></span><br><span class="line">agent1.sinks.sk1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent1.sinks.sk1.topic = flume-test01</span><br><span class="line">agent1.sinks.sk1.brokerList = 10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092</span><br><span class="line">agent1.sinks.sk1.requiredAcks = 1</span><br><span class="line">agent1.sinks.sk1.batchSize = 20</span><br><span class="line">agent1.sinks.sk1.channel = ch1</span><br></pre></td></tr></table></figure>

<h5 id="load-balance-2-节点配置"><a href="#load-balance-2-节点配置" class="headerlink" title="load balance 2 节点配置"></a>load balance 2 节点配置</h5><p>flume_loadbalance_collector2.conf</p>
<p>bind配置的ip为本机ip</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agent2.sources = s2</span><br><span class="line">agent2.channels = ch2</span><br><span class="line">agent2.sinks = sk2</span><br><span class="line"></span><br><span class="line">agent2.sources.s2.channels = ch2</span><br><span class="line">agent2.sources.s2.type = avro</span><br><span class="line">agent2.sources.s2.bind = 10.0.15.133</span><br><span class="line">agent2.sources.s2.port = 44441</span><br><span class="line"></span><br><span class="line">agent2.channels.ch2.type = memory</span><br><span class="line">agent2.channels.ch2.capacity = 1000</span><br><span class="line">agent2.channels.ch2.transactionCapacity = 100</span><br><span class="line">agent2.channels.ch2.keep-alive = 60</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">sink2</span></span><br><span class="line">agent2.sinks.sk2.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent2.sinks.sk2.topic = flume-test01</span><br><span class="line">agent2.sinks.sk2.brokerList = 10.0.15.130:9092,10.0.15.131:9092,10.0.15.132:9092</span><br><span class="line">agent2.sinks.sk2.requiredAcks = 1</span><br><span class="line">agent2.sinks.sk2.batchSize = 20</span><br><span class="line">agent2.sinks.sk2.channel = ch2</span><br></pre></td></tr></table></figure>

<h4 id="启动命令"><a href="#启动命令" class="headerlink" title="启动命令"></a>启动命令</h4><p>启动顺序：load balance节点 -&gt; agent节点</p>
<h5 id="load-balance-0-节点启动"><a href="#load-balance-0-节点启动" class="headerlink" title="load balance 0 节点启动"></a>load balance 0 节点启动</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./flume-ng agent -c /opt/software/flume/apache-flume-1.6.0-bin/conf -f /opt/software/flume/apache-flume-1.6.0-bin/jobs/flume_loadbalance_collector0.conf -n agent0 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h5 id="load-balance-1-节点启动"><a href="#load-balance-1-节点启动" class="headerlink" title="load balance 1 节点启动"></a>load balance 1 节点启动</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./flume-ng agent -c /opt/software/flume/apache-flume-1.6.0-bin/conf -f /opt/software/flume/apache-flume-1.6.0-bin/jobs/flume_loadbalance_collector1.conf -n agent1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h5 id="load-balance-2-节点启动"><a href="#load-balance-2-节点启动" class="headerlink" title="load balance 2 节点启动"></a>load balance 2 节点启动</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./flume-ng agent -c /opt/software/flume/apache-flume-1.6.0-bin/conf -f /opt/software/flume/apache-flume-1.6.0-bin/jobs/flume_loadbalance_collector2.conf -n agent2 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h5 id="agent-节点启动"><a href="#agent-节点启动" class="headerlink" title="agent 节点启动"></a>agent 节点启动</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./flume-ng agent -c /opt/soft/apache-flume-1.6.0-bin/conf -f /opt/soft/apache-flume-1.6.0-bin/jobs/flume_analysys_loadbalance_agent.conf -n agentX -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume 负载均衡配置</title>
    <url>/2021/04/21/Flume%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>Flume使用的版本为1.6.0。</p>
<p>Flume配置负载均衡案例。</p>
<a id="more"></a>
<hr>
<h3 id="netcat-Source-Sink-To-Console"><a href="#netcat-Source-Sink-To-Console" class="headerlink" title="netcat Source Sink To Console"></a>netcat Source Sink To Console</h3><h4 id="配置信息"><a href="#配置信息" class="headerlink" title="配置信息"></a>配置信息</h4><h5 id="Agent节点配置"><a href="#Agent节点配置" class="headerlink" title="Agent节点配置"></a>Agent节点配置</h5><p>sink port ip 为目标机器 ip</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agentX.sources = sX</span><br><span class="line">agentX.channels = chX</span><br><span class="line">agentX.sinks = sk1 sk2</span><br><span class="line"></span><br><span class="line">agentX.sources.sX.channels = chX</span><br><span class="line">agentX.sources.sX.type = netcat</span><br><span class="line">agentX.sources.sX.bind = localhost</span><br><span class="line">agentX.sources.sX.port = 44444</span><br><span class="line"></span><br><span class="line">agentX.channels.chX.type = memory</span><br><span class="line">agentX.channels.chX.capacity = 1000</span><br><span class="line">agentX.channels.chX.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Configure sinks</span></span><br><span class="line">agentX.sinks.sk1.channel = chX</span><br><span class="line">agentX.sinks.sk1.type = avro</span><br><span class="line">agentX.sinks.sk1.hostname = 10.0.15.181</span><br><span class="line">agentX.sinks.sk1.port = 44441</span><br><span class="line">agentX.sinks.sk2.channel = chX</span><br><span class="line">agentX.sinks.sk2.type = avro</span><br><span class="line">agentX.sinks.sk2.hostname = 10.0.15.133</span><br><span class="line">agentX.sinks.sk2.port = 44441</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Configure loadbalance</span></span><br><span class="line">agentX.sinkgroups = g1</span><br><span class="line">agentX.sinkgroups.g1.sinks = sk1 sk2</span><br><span class="line">agentX.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">agentX.sinkgroups.g1.processor.backoff=true</span><br><span class="line">agentX.sinkgroups.g1.processor.selector=round_robin</span><br><span class="line"><span class="meta">#</span><span class="bash">agentX.sinkgroups.g1.processor.selector=random</span></span><br></pre></td></tr></table></figure>

<h5 id="load-balance-1-节点配置"><a href="#load-balance-1-节点配置" class="headerlink" title="load balance 1 节点配置"></a>load balance 1 节点配置</h5><p>bind配置的ip为本机ip</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agent1.sources = s1</span><br><span class="line">agent1.channels = ch1</span><br><span class="line">agent1.sinks = sk1</span><br><span class="line">agent1.sources.s1.channels = ch1</span><br><span class="line">agent1.sources.s1.type = avro</span><br><span class="line">agent1.sources.s1.bind = 10.0.15.181</span><br><span class="line">agent1.sources.s1.port = 44441</span><br><span class="line">agent1.channels.ch1.type = memory</span><br><span class="line">agent1.channels.ch1.capacity = 1000</span><br><span class="line">agent1.channels.ch1.transactionCapacity = 100</span><br><span class="line">agent1.sinks.sk1.channel = ch1</span><br><span class="line">agent1.sinks.sk1.type = logger</span><br></pre></td></tr></table></figure>

<h5 id="load-balance-2-节点配置"><a href="#load-balance-2-节点配置" class="headerlink" title="load balance 2 节点配置"></a>load balance 2 节点配置</h5><p>bind配置的ip为本机ip</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">agent2.sources = s2</span><br><span class="line">agent2.channels = ch2</span><br><span class="line">agent2.sinks = sk2</span><br><span class="line">agent2.sources.s2.channels = ch2</span><br><span class="line">agent2.sources.s2.type = avro</span><br><span class="line">agent2.sources.s2.bind = 10.0.15.133</span><br><span class="line">agent2.sources.s2.port = 44441</span><br><span class="line">agent2.channels.ch2.type = memory</span><br><span class="line">agent2.channels.ch2.capacity = 1000</span><br><span class="line">agent2.channels.ch2.transactionCapacity = 100</span><br><span class="line">agent2.sinks.sk2.channel = ch2</span><br><span class="line">agent2.sinks.sk2.type = logger</span><br></pre></td></tr></table></figure>

<h4 id="启动命令"><a href="#启动命令" class="headerlink" title="启动命令"></a>启动命令</h4><p>启动顺序：load balance节点 -&gt; agent节点</p>
<h5 id="load-balance-1-节点启动"><a href="#load-balance-1-节点启动" class="headerlink" title="load balance 1 节点启动"></a>load balance 1 节点启动</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./flume-ng agent -c /opt/software/flume/apache-flume-1.6.0-bin/conf -f /opt/software/flume/apache-flume-1.6.0-bin/jobs/flume_loadbalance_collector1.conf -n agent1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h5 id="load-balance-2-节点启动"><a href="#load-balance-2-节点启动" class="headerlink" title="load balance 2 节点启动"></a>load balance 2 节点启动</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./flume-ng agent -c /opt/software/flume/apache-flume-1.6.0-bin/conf -f /opt/software/flume/apache-flume-1.6.0-bin/jobs/flume_loadbalance_collector2.conf -n agent2 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h5 id="agent-节点启动"><a href="#agent-节点启动" class="headerlink" title="agent 节点启动"></a>agent 节点启动</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./flume-ng agent -c /opt/software/flume/apache-flume-1.6.0-bin/conf -f /opt/software/flume/apache-flume-1.6.0-bin/jobs/flume_loadbalance_agent.conf -n agentX -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase 基本概念和使用</title>
    <url>/2020/06/28/HBase%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h4 id="HBase简介"><a href="#HBase简介" class="headerlink" title="HBase简介"></a>HBase简介</h4><p>HBase是一个分布式的、面向列的开源数据库，它是一个适合于非结构化数据存储的数据库。</p>
<p>HBase是基于列的而不是基于行的模式。</p>
<p>面向列：面向列（族）的存储和权限控制，列（族）独立检索。</p>
<p>稀疏：对于为空（null）的列，并不占用存储空间，因此，表的设计非常的稀疏。</p>
<a id="more"></a>

<hr>
<h4 id="HBase的角色"><a href="#HBase的角色" class="headerlink" title="HBase的角色"></a>HBase的角色</h4><h5 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h5><ol>
<li>监控RegionServer</li>
<li>处理RegionServer故障转移</li>
<li>处理元数据的变更</li>
<li>处理region的分配或移除</li>
<li>在空闲时间进行数据的负载均衡</li>
<li>通过zookeeper发布自己的位置给客户端</li>
</ol>
<h5 id="HRegionServer"><a href="#HRegionServer" class="headerlink" title="HRegionServer"></a>HRegionServer</h5><ol>
<li>负责存储HBase的实际数据</li>
<li>处理分配给它的Region</li>
<li>刷新缓存到HDFS</li>
<li>维护HLog</li>
<li>执行压缩</li>
<li>负责处理Region分片</li>
</ol>
<h5 id="Write-Ahead-Logs"><a href="#Write-Ahead-Logs" class="headerlink" title="Write-Ahead Logs"></a>Write-Ahead Logs</h5><ul>
<li>HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。</li>
<li>但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</li>
</ul>
<h5 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h5><p>这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。</p>
<h5 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h5><p>HFile存储在Store中，一个Store对应HBase表中的一个列族。</p>
<h5 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h5><p>内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegionServer会在内存中存储键值对。</p>
<h5 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h5><p>HBase表的分片，HBase表会根据rowkey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。</p>
<h4 id="HBase的架构"><a href="#HBase的架构" class="headerlink" title="HBase的架构"></a>HBase的架构</h4><p>一个RegionServer可以包含多个HRegion，每个RegionServer维护一个HLog，和多个HFiles以及其对应的MemStore。RegionServer运行于DataNode上，数量可以与DatNode数量一致，请参考如下架构图：</p>
<p><img src="/2020/06/28/HBase%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/hbase-architecture.png" alt="HBase的架构"></p>
<h4 id="HBase数据模型"><a href="#HBase数据模型" class="headerlink" title="HBase数据模型"></a>HBase数据模型</h4><p>确定一个单元格（cell）的位置，需要如下四个：</p>
<p>rowkey + column family + column + timestamp(version版本)，数据有版本的概念，即一个单元格可能有多个值，但是只有最新的一个对外显示。</p>
<ul>
<li>HBase中有两张特殊的Table，-ROOT-和.META.</li>
<li>.META.：记录了用户表的Region信息，.META.可以有多个region</li>
<li>-ROOT-：记录了.META.表的Region信息，-ROOT-只有一个region</li>
<li>Zookeeper中记录了-ROOT-表的location</li>
<li>Client访问用户数据之前需要首先访问zookeeper，然后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过client端会做cache缓存（在0.96版本后，Hbase移除了-ROOT-表）。</li>
</ul>
<p><img src="/2020/06/28/HBase%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/hbase-read-data.png" alt="访问数据流程"></p>
<h5 id="RowKey"><a href="#RowKey" class="headerlink" title="RowKey"></a>RowKey</h5><p>行键，Table的主键，Table中的记录默认按照rowkey升序排序。</p>
<h5 id="Timestamp"><a href="#Timestamp" class="headerlink" title="Timestamp"></a>Timestamp</h5><p>时间戳，每次数据操作对应的时间戳，可以看作是数据的version number。</p>
<h5 id="Column-Family"><a href="#Column-Family" class="headerlink" title="Column Family"></a>Column Family</h5><p>列族，Table在水平方向有一个或者多个Column Family组成，一个Column Family中可以由多个Column组成，即Column Family支持动态扩展，无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。</p>
<h5 id="Table-amp-Region"><a href="#Table-amp-Region" class="headerlink" title="Table&amp;Region"></a>Table&amp;Region</h5><p>当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由[startkey,endkey)表示，不同的region会被Master分配给相应的RegionServer进行管理。</p>
<h5 id="HMaster-1"><a href="#HMaster-1" class="headerlink" title="HMaster"></a>HMaster</h5><p>HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作：</p>
<ol>
<li>管理用户对Table的增、删、改、查操作</li>
<li>管理HRegionServer的负载均衡，调整Region分布</li>
<li>在Region Split后，负责新Region的分配</li>
<li>在HRegionServer停机后，负责失效HRegionServer上的Regions迁移</li>
</ol>
<h5 id="HRegionServer-1"><a href="#HRegionServer-1" class="headerlink" title="HRegionServer"></a>HRegionServer</h5><p>HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。</p>
<p>每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元，因此最好将具备共同IO特性的column放在一个Column Family中，这样最高效。</p>
<h5 id="MemStore-amp-StoreFiles"><a href="#MemStore-amp-StoreFiles" class="headerlink" title="MemStore&amp;StoreFiles"></a>MemStore&amp;StoreFiles</h5><p>HStore存储是HBase存储的核心了，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。</p>
<p>MemStore是Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile），当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要进入内存中就可以立即返回，保证了HBase I/O的高性能。</p>
<p>当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer上，使得原先1个Region的压力得以分流到2个Region上。</p>
<h5 id="HLog"><a href="#HLog" class="headerlink" title="HLog"></a>HLog</h5><p>每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中，HLog文件定期会滚动出新的，并删除旧的文件（已持久化到StoreFile中的数据）。</p>
<p>当HRegionServer意外终止后，HMaster会通过Zookeeper感知到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。</p>
<h5 id="文件类型"><a href="#文件类型" class="headerlink" title="文件类型"></a>文件类型</h5><p>HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型：</p>
<ol>
<li>HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile。</li>
<li>HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File。</li>
</ol>
<p>Zookeeper中hbase的节点的存储信息：</p>
<ul>
<li>rs：regionserver节点信息</li>
<li>table-lock：hbase的除meta以外的所有表</li>
<li>Table：hbase的所有的表</li>
</ul>
<h4 id="HBase的使用"><a href="#HBase的使用" class="headerlink" title="HBase的使用"></a>HBase的使用</h4><h5 id="HBase服务的启动"><a href="#HBase服务的启动" class="headerlink" title="HBase服务的启动"></a>HBase服务的启动</h5><ul>
<li>启动方式1：<br>bin/hbase-daemon.sh start master<br>bin/hbase-daemon.sh start regionserver<br>提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。</li>
<li>启动方式2：<br>bin/start-hbase.sh<br>对应的停止服务：<br>bin/stop-hbase.sh</li>
</ul>
<h5 id="查看HBase页面"><a href="#查看HBase页面" class="headerlink" title="查看HBase页面"></a>查看HBase页面</h5><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：<br><a href="http://hadoop001:16010" target="_blank" rel="noopener">http://hadoop001:16010</a></p>
<h5 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h5><ul>
<li>进入HBase客户端命令行<br>bin/hbase shell</li>
<li>查看帮助命令<br>hbase(main)&gt; help</li>
<li>查看当前数据库中有哪些表<br>hbase(main)&gt; list</li>
<li>查看当前数据库中有哪些命名空间<br>hbase(main)&gt; list_namespace</li>
</ul>
<h5 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h5><ul>
<li><p>创建表<br>hbase(main)&gt; create ‘student’,’info’<br>hbase(main)&gt; create ‘iparkmerchant_order’,’smzf’<br>hbase(main)&gt; create ‘staff’,’info’</p>
</li>
<li><p>插入数据到表<br>hbase(main) &gt; put ‘student’,’1001’,’info:name’,’Thomas’<br>hbase(main) &gt; put ‘student’,’1001’,’info:sex’,’male’<br>hbase(main) &gt; put ‘student’,’1001’,’info:age’,’18’<br>hbase(main) &gt; put ‘student’,’1002’,’info:name’,’Janna’<br>hbase(main) &gt; put ‘student’,’1002’,’info:sex’,’female’<br>hbase(main) &gt; put ‘student’,’1002’,’info:age’,’20’<br>数据插入后的数据模型：<br><img src="/2020/06/28/HBase%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/hbase-student-data-model.png" alt="student表的数据模型"></p>
</li>
<li><p>扫描查看表数据<br>hbase(main) &gt; scan ‘student’<br>hbase(main) &gt; scan ‘student’,{STARTROW =&gt; ‘1001’, STOPROW =&gt; ‘1001’}<br>hbase(main) &gt; scan ‘student’,{STARTROW =&gt; ‘1001’} 从哪一个rowkey开始扫描</p>
</li>
<li><p>查看表结构<br>hbase(main):&gt; desc ‘student’</p>
</li>
<li><p>更新指定字段的数据<br>hbase(main) &gt; put ‘student’,’1001’,’info:name’,’Nick’<br>hbase(main) &gt; put ‘student’,’1001’,’info:age’,’100’<br>hbase(main) &gt; put ‘student’,’1001’,’info:isNull’,’’（仅测试空值问题）</p>
</li>
<li><p>查看指定行或指定列族:列的数据<br>hbase(main) &gt; get ‘student’,’1001’<br>hbase(main) &gt; get ‘student’,’1001’,’info:name’</p>
</li>
<li><p>删除数据<br>删除某rowkey的全部数据：<br>hbase(main) &gt; deleteall ‘student’,’1001’</p>
</li>
<li><p>清空表数据<br>hbase(main) &gt; truncate ‘student’<br>提示：清空表的操作顺序为先disable，然后再truncate。</p>
</li>
<li><p>删除表<br>首先需要先让该表为disable状态：<br>hbase(main) &gt; disable ‘student’<br>检查这个表是否被禁用：<br>hbase(main) &gt; is_enabled ‘hbase_book’<br>hbase(main) &gt; is_disabled ‘hbase_book’<br>然后才能drop这个表：<br>hbase(main) &gt; drop ‘student’<br>提示：如果直接drop表，会报错：Drop the named table. Table must first be disabled<br>ERROR: Table student is enabled. Disable it first.<br>恢复被禁用的表：<br>enable ‘student’</p>
</li>
<li><p>统计表数据行数<br>hbase(main) &gt; count ‘student’</p>
</li>
<li><p>变更表信息<br>将info列族中的数据存放3个版本：<br>hbase(main) &gt; alter ‘student’,{NAME=&gt;’info’,VERSIONS=&gt;3}<br>查看student的最新版本的数据：<br>hbase(main) &gt; get ‘student’,’1001’<br>查看HBase中的多版本：<br>hbase(main) &gt; get ‘student’,’1001’,{COLUMN=&gt;’info:name’,VERSIONS=&gt;10}</p>
</li>
</ul>
<h5 id="常用shell操作"><a href="#常用shell操作" class="headerlink" title="常用shell操作"></a>常用shell操作</h5><ul>
<li><p>status：显示服务器状态<br>hbase&gt; status ‘hadoop001’</p>
</li>
<li><p>exists：检查表是否存在，适用于表量特别多的情况<br>hbase&gt; exists ‘hbase_book’</p>
</li>
<li><p>is_enabled/is_disabled：检查表是否启用或禁用<br>hbase&gt; is_enabled ‘hbase_book’<br>hbase&gt; is_disabled ‘hbase_book’</p>
</li>
<li><p>alter：该命令可以改变表和列族的模式<br>为当前表增加列族：<br>hbase&gt; alter ‘hbase_book’, NAME =&gt; ‘CF2’, VERSIONS =&gt; 2<br>为当前表删除列族：<br>hbase&gt; alter ‘hbase_book’, ‘delete’ =&gt; ‘CF2’</p>
</li>
<li><p>drop：删除一张表<br>hbase&gt; disable ‘hbase_book’<br>hbase&gt; drop ‘hbase_book’</p>
</li>
<li><p>delete：删除一行中一个单元格的值<br>hbase&gt; delete ‘hbase_book’, ‘rowKey’, ‘CF:C’</p>
</li>
<li><p>truncate：清空表数据，即禁用表 =&gt; 删除表 =&gt; 创建表<br>hbase&gt; truncate ‘hbase_book’</p>
</li>
<li><p>create：创建多个列族<br>hbase&gt; create ‘t1’, {NAME =&gt; ‘f1’}, {NAME =&gt; ‘f2’}, {NAME =&gt; ‘f3’}</p>
</li>
</ul>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>基本概念</tag>
        <tag>常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS 读写流程</title>
    <url>/2018/06/16/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>关于HDFS文件的读写流程</p>
<a id="more"></a>
<hr>
<hr>
<h4 id="读流程（FSDataInputStream）"><a href="#读流程（FSDataInputStream）" class="headerlink" title="读流程（FSDataInputStream）"></a>读流程（FSDataInputStream）</h4><p>Client上传文件到HDFS，也就是类似于下面的命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -get /test.log</span><br></pre></td></tr></table></figure>
<p><img src="/2018/06/16/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/hdfs-input.png" alt="HDFS文件写流程"></p>
<h5 id="HDFS读的过程："><a href="#HDFS读的过程：" class="headerlink" title="HDFS读的过程："></a>HDFS读的过程：</h5><ol>
<li>Client调用FileSystem.open(filePath)方法，与NN进行[RPC]通信，返回该文件的部分或者全部的block列表，也就是返回FSDataInputStream对象。</li>
<li>Client调用FSDataInputStream.read()方法。<br>a.与第一个块最近的DN进行read，读取完成后，会check；假如OK，就关闭与当前DN的通信；假如失败，会记录失败块+DN信息，下次不会再读取，会去该块的第二个DN地址读取。<br>b.接着去第二个块的最近的DN上通信读取，check后，关闭通信。<br>c.假如block列表读取完成后，文件还未结束，FileSystem会再次从NN获取该文件的下一批次的block列表。<br>（感觉就是连续的数据流，对于客户端操作是透明无感知的）</li>
<li>Client调用FSDataInputStream.close()方法，关闭输入流。</li>
</ol>
<hr>
<h4 id="写流程（FSDataOutputStream）"><a href="#写流程（FSDataOutputStream）" class="headerlink" title="写流程（FSDataOutputStream）"></a>写流程（FSDataOutputStream）</h4><p>Client上传文件到HDFS，也就是类似于下面的命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -put test.log /</span><br></pre></td></tr></table></figure>
<p><img src="/2018/06/16/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/hdfs-output.png" alt="HDFS文件写流程"></p>
<h5 id="HDFS写的过程："><a href="#HDFS写的过程：" class="headerlink" title="HDFS写的过程："></a>HDFS写的过程：</h5><ol>
<li>Client 调用FileSystem.create(filePath)方法，与NN进行[RPC]通信，check是否存在及是否有权限创建。假如不OK，就返回错误信息；假如OK，就创  建一个新文件，不关联任何的block块，返回一个FSDataOutputStream对象。</li>
<li>Client调用FSDataOutputStream对象的write()方法，先将第一块的第一个副本写到第一个DN，第一个副本写完就传输给第二个DN，第二个副本写完就传输给第三个DN，直至写完第三个副本。然后返回一个ack packet确认包给第二个DN，第二个DN接收到第三个的ack packet确认包加上自身OK，就返回一个ack packet确认包给第一个DN，第一个DN接收到第二个DN的ack packet确认包加上自身OK，就返回ack packet确认包给FSDataOutputStream对象，标志第一个块的3个副本写完。依次写完余下的块。</li>
<li>当文件写入数据完成后，Client调用FSDataOutputStream.close()方法，关闭输出流。</li>
<li>然后调用FileSystem.complete()方法，告诉NN该文件写入成功。</li>
</ol>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>基本概念</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop Block和副本数&amp;小文件问题</title>
    <url>/2018/06/20/HadoopBLOCK%E5%92%8C%E5%89%AF%E6%9C%AC%E6%95%B0&amp;%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h4 id="机架感知（副本放置策略）"><a href="#机架感知（副本放置策略）" class="headerlink" title="机架感知（副本放置策略）"></a>机架感知（副本放置策略）</h4><p>官网地址：</p>
<p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html" target="_blank" rel="noopener">Rack Awareness</a></p>
<p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication" target="_blank" rel="noopener">HDFS Architecture</a></p>
<a id="more"></a>
<hr>
<p>一个hadoop分布式集群会有很多的服务器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，机架内的服务器之间的网络速度通常都会高于跨机架服务器之间的网络速度，并且机架之间服务器的网络通信通常受到上层交换机间网络带宽的限制。</p>
<p><img src="/2018/06/20/HadoopBLOCK%E5%92%8C%E5%89%AF%E6%9C%AC%E6%95%B0&%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/replication-and-rack-awareness.jpg" alt="副本放置策略"></p>
<p>HDFS对数据文件是分block存储，每个block默认有3个副本（也可以配置大于3），HDFS对副本的存放策略如下：</p>
<ol>
<li>第一个副本：放置在Client所在的DN节点上（如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的DN节点）</li>
<li>第二个副本：放置在与第一个副本不同的机架的节点上（随机选择）</li>
<li>第三个副本：与第二个副本相同机架的不同节点上</li>
<li>如果还有更多的副本，随机放在集群的节点中<br>这样的策略主要是为了数据的可靠性和数据访问的性能：</li>
</ol>
<ul>
<li>数据分布在不同的机架上，就算当前机架挂掉，其他机架上还有冗余备份，整个集群依然能对外服务。</li>
<li>数据分布在不同的机架上，运行MR任务时可以就近获取所需的数据。</li>
</ul>
<h4 id="块大小和副本数"><a href="#块大小和副本数" class="headerlink" title="块大小和副本数"></a>块大小和副本数</h4><p>块大小和副本数需要在hdfs-site.xml中配置。</p>
<p>官网中的相应的默认参数如下：<br><img src="/2018/06/20/HadoopBLOCK%E5%92%8C%E5%89%AF%E6%9C%AC%E6%95%B0&%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0.png" alt="默认参数"></p>
<p>可以看到，默认块大小为128M，默认副本数为3.</p>
<p>这里会有一个常规的面试题：</p>
<p>问：假如一个文件300M，块128M，副本2。请问实际存储空间多大，多少块？</p>
<p>答：300 x 2 = 600M，3 x 2 = 6块</p>
<p>需要注意的是，实际存储空间=文件大小x副本数，并不是块大小，即使44M也占用一个块。</p>
<h4 id="HDFS小文件问题"><a href="#HDFS小文件问题" class="headerlink" title="HDFS小文件问题"></a>HDFS小文件问题</h4><p>HDFS上每个文件都要在NameNode上建立一个索引，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p>
<p>小文件的危害：</p>
<ol>
<li>HDFS不适合大量小文件的存储，因namenode将文件系统的元数据存放在内存中，因此存储的文件数目受限于 namenode的内存大小。HDFS中每个文件、目录、数据块占用150Bytes。如果存放的文件数目过多的话会占用很大的内存甚至撑爆内存。</li>
<li>HDFS适用于高吞吐量，而不适合低时间延迟的访问。如果同时存入大量的小文件会花费很长的时间。</li>
<li>流式读取的方式，不适合多用户写入，以及任意位置写入。如果访问小文件，则必须从一个datanode跳转到另外一个datanode，这样大大降低了读取性能。</li>
</ol>
<h4 id="小文件优化"><a href="#小文件优化" class="headerlink" title="小文件优化"></a>小文件优化</h4><p>小文件的优化无非以下几种方式：</p>
<ol>
<li>在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。</li>
<li>在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。</li>
</ol>
<p>根据上面的思想，可以考虑采用下面的解决方案：</p>
<ol>
<li>Hadoop Archive：<br>是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了namenode的内存使用。</li>
<li>Sequence file：<br>sequence file由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。</li>
<li>CombineFileInputFormat：<br>CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。</li>
<li>开启JVM重用：<br>对于大量小文件job，可以开启JVM重用，会减少45%运行时间。<br>JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他map。<br>具体设置：mapreduce.job.jvm.numtasks值在10-20之间。</li>
</ol>
<p>总的来说，解决小文件问题主要就是将小文件合并成大文件，一般约定：尽量使得合并后的大文件&lt;=blocksize，比如110M（假如块大小128M）。</p>
<p>一般在生产上会设置一个阈值，比如10M，作为小文件的门槛，并使用shell脚本调用程序进行小文件的定期合并。</p>
<h4 id="小文件合并脚本"><a href="#小文件合并脚本" class="headerlink" title="小文件合并脚本"></a>小文件合并脚本</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.practice.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FileSystem</span>, <span class="type">Path</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.internal.<span class="type">Logging</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">miniFileMerge</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[2]"</span>).appName(<span class="keyword">this</span>.getClass.getSimpleName).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> target = <span class="string">"hdfs://fushuaidate:9000/practice/minifilemerge"</span> <span class="comment">//待合并文件夹</span></span><br><span class="line">    <span class="keyword">val</span> output = <span class="keyword">new</span> <span class="type">Path</span>(target)</span><br><span class="line">    <span class="keyword">val</span> hdfs: <span class="type">FileSystem</span> = output.getFileSystem(sparkSession.sparkContext.hadoopConfiguration)</span><br><span class="line">    <span class="keyword">val</span> partitions = makeCoalesce(hdfs, output, <span class="number">5</span>)</span><br><span class="line">    println(<span class="string">s"合并后文件数量为：<span class="subst">$partitions</span>"</span>)</span><br><span class="line">    <span class="keyword">val</span> paths: <span class="type">List</span>[<span class="type">String</span>] = getPaths(hdfs, output)</span><br><span class="line">    <span class="keyword">val</span> mergeFiles: <span class="type">RDD</span>[<span class="type">String</span>] = sparkSession.sparkContext.textFile(target).coalesce(partitions)</span><br><span class="line">    <span class="keyword">val</span> dataFrame = rddToDF(sparkSession, mergeFiles)</span><br><span class="line">    dataFrame.write.text(<span class="string">s"<span class="subst">$target</span>/.TMP"</span>)</span><br><span class="line">    <span class="keyword">val</span> resStatus = hdfs.listStatus(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">s"<span class="subst">$target</span>/.TMP"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//移动结果文件</span></span><br><span class="line">    resStatus.foreach(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (x.getLen &gt; <span class="number">0</span>) &#123; <span class="comment">//排除spark自动生成的_success文件</span></span><br><span class="line">        hdfs.rename(x.getPath, output)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">s"merge <span class="subst">$&#123;paths.size&#125;</span> files =&gt; <span class="subst">$partitions</span> files"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//删除临时目录</span></span><br><span class="line">    hdfs.delete(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">s"<span class="subst">$target</span>/.TMP"</span>), <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//删除原小文件</span></span><br><span class="line">    paths.foreach(x =&gt; &#123;</span><br><span class="line">      hdfs.delete(<span class="keyword">new</span> <span class="type">Path</span>(x), <span class="literal">true</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makeCoalesce</span></span>(fs: <span class="type">FileSystem</span>, path: <span class="type">Path</span>, size: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> num = <span class="number">0</span>L</span><br><span class="line">    fs.listStatus(path).foreach(x =&gt; &#123;</span><br><span class="line">      num += x.getLen</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">val</span> partitions = (num / <span class="number">1024</span> / <span class="number">1024</span> / size).toInt + <span class="number">1</span></span><br><span class="line">    logInfo(<span class="string">"分区数为："</span> + partitions)</span><br><span class="line">    partitions</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPaths</span></span>(fs: <span class="type">FileSystem</span>, path: <span class="type">Path</span>): <span class="type">List</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">var</span> paths: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>.empty <span class="comment">//文件名字符串</span></span><br><span class="line">    fs.listStatus(path).foreach(x =&gt; &#123;</span><br><span class="line">      x.getLen</span><br><span class="line">      x.getPath</span><br><span class="line">      paths :+= x.getPath.toString</span><br><span class="line">      println(x.getLen + <span class="string">"|"</span> + x.getPath)</span><br><span class="line">    &#125;)</span><br><span class="line">    paths</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rddToDF</span></span>(sparkSession: <span class="type">SparkSession</span>, rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> sparkSession.implicits._</span><br><span class="line">    <span class="keyword">val</span> infoDF = rdd.map(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> splits = x.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">val</span> date = splits(<span class="number">0</span>).trim</span><br><span class="line">      <span class="keyword">val</span> ip = splits(<span class="number">1</span>).trim</span><br><span class="line">      <span class="keyword">val</span> domain = splits(<span class="number">6</span>).trim</span><br><span class="line">      date + <span class="string">"\t"</span> + ip + <span class="string">"\t"</span> + domain</span><br><span class="line">    &#125;).toDF() <span class="comment">// 最终转成DF</span></span><br><span class="line">    infoDF</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="本地调试修改owner"><a href="#本地调试修改owner" class="headerlink" title="本地调试修改owner"></a>本地调试修改owner</h4><p><img src="/2018/06/20/HadoopBLOCK%E5%92%8C%E5%89%AF%E6%9C%AC%E6%95%B0&%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/%E6%9C%AC%E5%9C%B0%E8%B0%83%E8%AF%95%E4%BF%AE%E6%94%B9owner.png" alt="本地调试修改owner"></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop优化</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop HDFS架构 NameNode和DataNode工作机制</title>
    <url>/2018/06/20/HadoopHDFS%E6%9E%B6%E6%9E%84NameNode%E5%92%8CDataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h4 id="Hadoop-HDFS架构"><a href="#Hadoop-HDFS架构" class="headerlink" title="Hadoop HDFS架构"></a>Hadoop HDFS架构</h4><p>HDFS主要由3个组件构成：NameNode, DataNode, SecondaryNameNode。<br>HDFS是以master/slave模式运行的，通常NameNode, SecondaryNameNode运行在master节点，DataNode运行在slave节点。</p>
<a id="more"></a>
<hr>
<h4 id="HDFS架构图"><a href="#HDFS架构图" class="headerlink" title="HDFS架构图"></a>HDFS架构图</h4><p><img src="/2018/06/20/HadoopHDFS%E6%9E%B6%E6%9E%84NameNode%E5%92%8CDataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/hdfs-architecture.png" alt="HDFS主从架构"></p>
<h4 id="NameNode（名称节点）"><a href="#NameNode（名称节点）" class="headerlink" title="NameNode（名称节点）"></a>NameNode（名称节点）</h4><p>存储：文件元数据信息，包含：</p>
<ul>
<li>文件名称</li>
<li>文件目录结构</li>
<li>文件的属性（权限，创建时间，副本数）</li>
<li>文件对应哪些数据块 –&gt; 数据块对应哪些DN节点</li>
</ul>
<p>作用：</p>
<ul>
<li>管理文件系统命名空间</li>
<li>维护文件系统树及树中的所有文件和目录</li>
<li>维护所有这些文件或目录的打开、关闭、移动、重命名等操作</li>
</ul>
<h4 id="NameNode工作机制"><a href="#NameNode工作机制" class="headerlink" title="NameNode工作机制"></a>NameNode工作机制</h4><p><img src="/2018/06/20/HadoopHDFS%E6%9E%B6%E6%9E%84NameNode%E5%92%8CDataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/namenode-process.png" alt="NN工作机制"></p>
<p>第一阶段：NameNode启动</p>
<ol>
<li>第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志（edits）和镜像文件（fsimage）到内存。</li>
<li>客户端对元数据进行增删改的请求。</li>
<li>NameNode记录操作日志，更新滚动日志。</li>
<li>NameNode在内存中对数据进行增删改查。</li>
</ol>
<p>第二阶段：SecondaryNameNode工作</p>
<ol>
<li>SecondaryNameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。</li>
<li>SecondaryNameNode请求执行checkpoint。</li>
<li>NameNode滚动正在写的edits日志。</li>
<li>将滚动前的编辑日志和镜像文件拷贝到SecondaryNameNode。</li>
<li>SecondaryNameNode加载编辑日志和镜像文件到内存，并合并。</li>
<li>生成新的镜像文件fsimage.chkpoint。</li>
<li>拷贝fsimage.chkpoint到NameNode。</li>
<li>NameNode将fsimage.chkpoint重新命名成fsimage。</li>
</ol>
<p>checkpoint检查时间参数设置</p>
<ol>
<li>通常情况下，SecondaryNameNode每隔1小时执行一次<br>[hdfs-default.xml]<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>一分钟检查一次操作次数，当操作次数达到一百万时，SecondaryNameNode执行一次<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="DataNode（数据节点）"><a href="#DataNode（数据节点）" class="headerlink" title="DataNode（数据节点）"></a>DataNode（数据节点）</h4><p>存储：数据块，数据块校验和，与NN通信</p>
<p>作用：</p>
<ul>
<li>读写文件的数据块</li>
<li>接收NN的指示来进行创建、删除、复制等操作</li>
<li>通过心跳定期向NN发送所存储文件块列表信息</li>
</ul>
<h4 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h4><p><img src="/2018/06/20/HadoopHDFS%E6%9E%B6%E6%9E%84NameNode%E5%92%8CDataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/datanode-detail.png" alt="DN工作机制"></p>
<ol>
<li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据，包括数据块的长度、块数据的校验和以及时间戳。</li>
<li>DataNode启动后向NameNode注册，通过后，周期性（1小时）地向NameNode上报所有的块信息。</li>
<li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一台机器或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li>
<li>集群运行中可以安全加入和退出一些机器。</li>
</ol>
<h4 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h4><ol>
<li>当DataNode读取block的时候，它会计算checksum校验和。</li>
<li>如果计算后的checksum，与block创建时值不一样，说明block已经损坏。</li>
<li>client读取其他DataNode上的block。</li>
<li>DataNode在其文件创建后周期验证checksum校验和。</li>
</ol>
<h4 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h4><p>DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：</p>
<ul>
<li><p>timeout = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval</p>
</li>
<li><p>默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p>
</li>
<li><p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p>
</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span> dfs.heartbeat.interval <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="ScondaryNameNode（第二名称节点）"><a href="#ScondaryNameNode（第二名称节点）" class="headerlink" title="ScondaryNameNode（第二名称节点）"></a>ScondaryNameNode（第二名称节点）</h4><p>存储：命名空间镜像文件fsimage+编辑日志editlog</p>
<p>作用：</p>
<ul>
<li>定期合并fsimage+editlog文件为新的fsimage，推送给NN</li>
<li>监控HDFS状态，每隔一段时间获取HDFS元数据的快照</li>
</ul>
<p>为了解决单点故障，设置了SNN的1小时备份机制，虽然能够减轻单点故障，但是还会有风险，即在那1小时中，还是会有发生单点故障的可能，造成数据丢失。为了解决这个问题，需要部署HDFS -HA高可用。</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>基本概念</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop Shuffle机制</title>
    <url>/2018/06/20/HadoopShuffle%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p>Hadoop Shuffle机制 Shuffle过程</p>
<a id="more"></a>
<hr>
<h4 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h4><p>MapReduce确保每个reduce的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reduce的过程）就是Shuffle。</p>
<p><img src="/2018/06/20/HadoopShuffle%E6%9C%BA%E5%88%B6/shuffle.png" alt="shuffle"></p>
<p>下图是Shuffle机制图<br><img src="/2018/06/20/HadoopShuffle%E6%9C%BA%E5%88%B6/shuffle-detail.png" alt="shuffle"></p>
<h4 id="Shuffle过程"><a href="#Shuffle过程" class="headerlink" title="Shuffle过程"></a>Shuffle过程</h4><p>shuffle一开始就是map阶段做输出操作，map在做输出时会在内存里开启一个环形缓冲区，这个缓冲区是专门用来输出的，默认大小是100MB，并且在配置文件里为这个缓冲区设定了一个阈值，默认是0.80。同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阈值的80%的时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill（溢写）。另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作互不干扰，如果缓存区被撑爆了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作。</p>
<p>写入磁盘前会有个排序操作，这个是在写入磁盘操作时进行，不是在写入内存时进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。每次spill操作也就是写入磁盘时就会写一个溢出文件，也就是说在做map输出时有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。</p>
<p>这个过程里还会有一个Partitioner操作，和map阶段的输入分片（Input Split）很像，一个Partitioner对应一个reduce作业，如果我们的MR操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片。</p>
<p>到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，这个复制过程和map写入磁盘过程类似，也有阈值和内存大小，阈值可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。</p>
<p>上面也提到，将map的输出作为reduce的输入的过程就是shuffle（洗牌）。</p>
<p>那么shuffle到底是属于map阶段还是reduce阶段呢？比较公认的说法是，shuffle应该是属于reduce阶段，因为map阶段的主要任务是数据映射，而shuffle的目的是主要是为reduce阶段做准备的。</p>
<p>也就是说，有多少个切片，就会启动相应数量的map task进行数据处理。那么，如果需要确定map task的数量，只需要确定切片的实际数量即可。</p>
<h4 id="MapReduce计算框架的不足之处"><a href="#MapReduce计算框架的不足之处" class="headerlink" title="MapReduce计算框架的不足之处"></a>MapReduce计算框架的不足之处</h4><p>从上图可以看到，shuffle阶段中会有多次写入磁盘的操作，基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。另外，当一些查询（Hive）翻译到MapReduce任务时，往往会产生多个stage（阶段），而这些串联的stage又依赖于底层文件系统（HDFS）来存储每一个stage的输出结果，而I/O的效率往往较低，从而影响了MapReduce的运行速度。相比之下，Spark的运算结果中间不落地，而且兼容HDFS、Hive，实际应用中会以Spark替代MR作为常用的计算引擎。</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>基本概念</tag>
        <tag>Hadoop</tag>
        <tag>Shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop 数据压缩</title>
    <url>/2018/06/27/Hadoop%E5%8E%8B%E7%BC%A9/</url>
    <content><![CDATA[<h4 id="Hadoop-数据压缩"><a href="#Hadoop-数据压缩" class="headerlink" title="Hadoop 数据压缩"></a>Hadoop 数据压缩</h4><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。</p>
<a id="more"></a>
<hr>
<h4 id="压缩的好处和坏处"><a href="#压缩的好处和坏处" class="headerlink" title="压缩的好处和坏处"></a>压缩的好处和坏处</h4><p>好处：</p>
<ul>
<li>减少磁盘存储空间</li>
<li>降低IO（网络的IO和磁盘的IO）</li>
<li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li>
</ul>
<p>坏处：</p>
<ul>
<li>由于使用数据时，需要先将数据解压，加重CPU负荷</li>
</ul>
<p>基本原则：</p>
<ul>
<li>运算密集型的job，少用压缩</li>
<li>IO密集型的job，多用压缩</li>
</ul>
<p><img src="/2018/06/27/Hadoop%E5%8E%8B%E7%BC%A9/%E5%8E%8B%E7%BC%A9.png" alt="压缩格式"><br>是否可分割是指，压缩后的文件是否可以再分割。可以分割的格式允许单一文件由多个Mapper程序同时读取，可以做到更好的并行化。</p>
<h4 id="压缩比"><a href="#压缩比" class="headerlink" title="压缩比"></a>压缩比</h4><p><img src="/2018/06/27/Hadoop%E5%8E%8B%E7%BC%A9/compress-codec-size.png" alt="压缩之后的size"></p>
<h4 id="压缩时间"><a href="#压缩时间" class="headerlink" title="压缩时间"></a>压缩时间</h4><p><img src="/2018/06/27/Hadoop%E5%8E%8B%E7%BC%A9/compress-codec-time.png" alt="压缩时间-两次不同大小的文件"><br>可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</p>
<h4 id="压缩格式的优缺点"><a href="#压缩格式的优缺点" class="headerlink" title="压缩格式的优缺点"></a>压缩格式的优缺点</h4><p><img src="/2018/06/27/Hadoop%E5%8E%8B%E7%BC%A9/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E4%BC%98%E7%BC%BA%E7%82%B9.png" alt="压缩格式的优缺点"></p>
<h4 id="压缩的应用场景"><a href="#压缩的应用场景" class="headerlink" title="压缩的应用场景"></a>压缩的应用场景</h4><p>在Hadoop中的应用场景主要在三方面：输入，中间，输出。</p>
<p>整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce</p>
<ol>
<li>Use Compressd Map Input: 从HDFS中读取文件进行MR作业，如果数据很大，可以使用压缩并且选择支持分片的压缩方式（bzip2, LZO），可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式，例如：Sequence Files, RC, ORC等。</li>
<li>Compress Intermediate Data: Map输出作为Reduce的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少存储文件所占空间，提升数据传输速率。建议使用压缩速度快的压缩方式，例如：Snappy和LZO。</li>
<li>Compress Reducer Output: 进行归档处理或者链接MR的工作（该作业的输出作为下个作业的输入），压缩可以减少存储文件所占空间，提升数据传输速率，如果作为归档处理，可以采用高的压缩比（Gzip, bzip2），如果作为下个作业的输入，考虑是否要分片进行选择。</li>
</ol>
<h4 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h4><p>要在Hadoop中启动压缩，可以配置如下参数：<br><img src="/2018/06/27/Hadoop%E5%8E%8B%E7%BC%A9/%E5%8E%8B%E7%BC%A9%E9%85%8D%E7%BD%AE.png" alt="压缩参数配置"></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop优化</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS 常用命令</title>
    <url>/2018/06/24/Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>Hadoop的常用命令</p>
<a id="more"></a>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">创建路径</span><br><span class="line">hadoop fs -mkdir -p /</span><br><span class="line"></span><br><span class="line">删除</span><br><span class="line">hadoop fs -rm /</span><br><span class="line"></span><br><span class="line">读文件</span><br><span class="line">hadoop fs -text /</span><br><span class="line"></span><br><span class="line">查看hdfs根目录下的文件</span><br><span class="line">hadoop fs -ls /</span><br><span class="line"></span><br><span class="line">查看hdfs某路径的大小</span><br><span class="line">hadoop fs -du -s -h /</span><br><span class="line">hadoop fs -du -s /</span><br><span class="line"></span><br><span class="line">将本地test.log文件上传至hdfs根目录</span><br><span class="line">hadoop fs -put test.log /</span><br><span class="line"></span><br><span class="line">将hdfs根目录下的test.log下载至本地当前目录下</span><br><span class="line">hadoop fs -get /test.log ./</span><br><span class="line"></span><br><span class="line">删除hdfs根目录下的test.log，如果设置了回收站，会放入回收站</span><br><span class="line">hadoop fs -rm /test.log</span><br><span class="line"></span><br><span class="line">直接删除test.log</span><br><span class="line">hadoop fs -rm -skipTrash /test.log</span><br><span class="line"></span><br><span class="line">类似于上面的-put</span><br><span class="line">hadoop fs -copyFromLocal test.log /</span><br><span class="line"></span><br><span class="line">类似于上面的-get</span><br><span class="line">hadoop fs -copyToLocal /test.log ./</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>常用命令</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop 数据切片 &amp; MapTask并行度机制</title>
    <url>/2018/06/19/Hadoop%E6%95%B0%E6%8D%AE%E5%88%87%E7%89%87&amp;MapTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p>Hadoop 数据切片 MapTask并行度</p>
<a id="more"></a>
<hr>
<h4 id="input-split与map-task的关系"><a href="#input-split与map-task的关系" class="headerlink" title="input split与map task的关系"></a>input split与map task的关系</h4><p>split数量和map task数量一一对应。</p>
<p>下图是wordcount的流程：<br><img src="/2018/06/19/Hadoop%E6%95%B0%E6%8D%AE%E5%88%87%E7%89%87&MapTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E6%9C%BA%E5%88%B6/wordcount-mr.png" alt="wordcount流程"></p>
<p>可以看到，一个job的Map阶段map task并行度（个数），由客户端提交job时的切片个数决定。</p>
<h4 id="数据切片-MapTask并行度机制"><a href="#数据切片-MapTask并行度机制" class="headerlink" title="数据切片 MapTask并行度机制"></a>数据切片 MapTask并行度机制</h4><p>假如有以下两个文件，blocksize=128M，则split和map task的关系如下图所示：<br><img src="/2018/06/19/Hadoop%E6%95%B0%E6%8D%AE%E5%88%87%E7%89%87&MapTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E6%9C%BA%E5%88%B6/split-maptask.png" alt="split-maptask的关系"></p>
<p>也就是说，有多少个切片，就会启动相应数量的map task进行数据处理。那么，如果需要确定map task的数量，只需要确定切片的实际数量即可。</p>
<h4 id="切片机制"><a href="#切片机制" class="headerlink" title="切片机制"></a>切片机制</h4><p>默认使用FileInputFormat类处理数据输入，遵循如下的切片机制：</p>
<ul>
<li>按照文件的内容长度进行切片</li>
<li>切片大小，默认等于block大小</li>
<li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li>
</ul>
<p>比如待处理数据有两个文件：</p>
<p>file01.txt 320M</p>
<p>file02.txt 10M</p>
<p>经过FileInputFormat的切片机制运算后，形成的切片信息如下：</p>
<p>file01.txt.split1– 0~128</p>
<p>file01.txt.split2– 128~256</p>
<p>file01.txt.split3– 256~320</p>
<p>file02.txt.split1– 0~10M</p>
<p>那么切片的数量是否就是分块的数量+小文件的数据量呢？其实是不一定的，因为源码中还有这样一个参数：private static final double SPLIT_SLOP = 1.1，也就是还有10%的切片裕度，下面结合源码进行说明。</p>
<h4 id="切片机制-1"><a href="#切片机制-1" class="headerlink" title="切片机制"></a>切片机制</h4><p>整体流程：</p>
<ol>
<li>找到数据存储的目录。</li>
<li>遍历处理（规划切片）目录下的每一个文件。</li>
<li>遍历第一个文件<ol>
<li>获取文件大小。</li>
<li>计算切片大小，默认情况下，切片大小=blocksize。</li>
<li>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分为一块切片。</li>
<li>将切片信息写到一个切片规划文件中。</li>
<li>整个切片的核心过程在getSplit()方法中完成。</li>
<li>数据切片只是在逻辑上对输入数据进行分片，并不会在磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。</li>
<li>注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分。</li>
</ol>
</li>
<li>提交切片规划文件到YARN上，YARN上的AppMaster就可以根据切片规划文件计算开启map task个数。</li>
</ol>
<p>总结：所以，切片的数量并不一定等于分块的数量+小文件的数据量，还要考虑大文件切分后剩余的部分是否大于块的1.1倍。</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>基本概念</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 基本概念 Hive的部署 Hive的常用命令</title>
    <url>/2018/07/14/Hive%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5Hive%E7%9A%84%E9%83%A8%E7%BD%B2Hive%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h4 id="Hive基本概念"><a href="#Hive基本概念" class="headerlink" title="Hive基本概念"></a>Hive基本概念</h4><p><a href="http://hive.apache.org" target="_blank" rel="noopener">Hive官网</a></p>
<p>The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p>
<a id="more"></a>
<hr>
<h4 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h4><p>Hive由Facebook开源，用于解决海量结构化日志的数据统计问题。</p>
<p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文化映射为一张表，并提供类SQL查询功能。</p>
<p>本质：将HQL/SQL转化成MapReduce程序</p>
<ol>
<li>Hive处理的数据存储在HDFS</li>
<li>Hive底层执行引擎：MapReduce/Tez/Spark，只需要通过一个参数就能够切换底层的执行引擎</li>
<li>Hive作业提交到YARN上运行</li>
</ol>
<h4 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h4><p>优点：</p>
<ol>
<li>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）</li>
<li>避免了MapReduce编程，减少学习成本</li>
<li>Hive的执行延迟比较高，因此Hive常用于离线分析，对实时性要求不高的场合</li>
<li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</li>
<li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li>
</ol>
<p>缺点：</p>
<ol>
<li>Hive的HQL表达能力有限<ol>
<li>迭代式算法无法表达</li>
<li>数据挖掘方面不擅长</li>
</ol>
</li>
<li>Hive的效率比较低<ol>
<li>Hive自动生成的MapReduce作业，通常不够智能化</li>
<li>Hive调优比较困难，粒度较粗</li>
</ol>
</li>
</ol>
<h4 id="Hive的架构原理"><a href="#Hive的架构原理" class="headerlink" title="Hive的架构原理"></a>Hive的架构原理</h4><p><img src="/2018/07/14/Hive%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5Hive%E7%9A%84%E9%83%A8%E7%BD%B2Hive%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/hive-architecture.png" alt="Hive的架构"></p>
<p>如上图所示，Hive通过给用户提供的一系列交互接口，接收到用户的指令（SQL），使用自己的Driver，结合元数据（MetaStore），将这些指令翻译成MapReduce，提交到Hadoop中执行，最后将执行返回的结果输出到用户交互接口。</p>
<ol>
<li>用户接口：Client<br>CLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive)</li>
<li>元数据：Meta store<br>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；<br>默认存储在自带的derby数据库中，推荐使用MySQL存储Meta store。</li>
<li>Hadoop<br>使用HDFS进行存储，使用MapReduce进行计算</li>
<li>驱动器：Driver<ol>
<li>解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</li>
<li>编译器（Physical Plan）：将AST编译生成逻辑执行计划。</li>
<li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li>
<li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划，对于Hive来说，就是MR/Spark。</li>
</ol>
</li>
</ol>
<h4 id="Hive的部署"><a href="#Hive的部署" class="headerlink" title="Hive的部署"></a>Hive的部署</h4><p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">Hive的使用文档</a></p>
<p>环境要求：</p>
<ul>
<li>JDK8</li>
<li>Hadoop2.x</li>
<li>Linux</li>
<li>MySQL</li>
</ul>
<p>Hive版本选择：wget <a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2.tar.gz</a></p>
<h4 id="Hive安装步骤"><a href="#Hive安装步骤" class="headerlink" title="Hive安装步骤"></a>Hive安装步骤</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">解压：</span><br><span class="line">tar -zxvf hive-1.1.0-cdh5.16.2.tar.gz -C ~/app/</span><br><span class="line"></span><br><span class="line">设置软连接：</span><br><span class="line">ln -s hive-1.1.0-cdh5.16.2 hive</span><br></pre></td></tr></table></figure>

<p>配置环境变量，建议普通用户的.bashrc文件，追加以下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HIVE_HOME=/home/hadoop/app/hive</span><br><span class="line">export PATH=$HIVE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>

<p>生效环境变量文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">source .bashrc</span><br></pre></td></tr></table></figure>

<p>检查是否生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">which hive</span><br></pre></td></tr></table></figure>

<p>进入配置文件目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd $HIVE_HOME/conf/</span><br><span class="line"></span><br><span class="line">vi hive-site.xml</span><br></pre></td></tr></table></figure>
<p>编译以下内容</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop001:3306/metadata_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>保存并退出。</p>
<p>然后，拷贝MySQL驱动包到$HIVE_HOME/lib/下。</p>
<p>最后，启动Hive（需要先启动HDFS和YARN）：hive</p>
<h4 id="Hive常用命令"><a href="#Hive常用命令" class="headerlink" title="Hive常用命令"></a>Hive常用命令</h4><ul>
<li>show databases; 查看数据库</li>
<li>use default; 打开默认数据库</li>
<li>create database company location ‘/user/company’; 创建库并指定hdfs路径</li>
<li>alter database company set dbproperties(‘creator’ = ‘vinx’); 为数据库添加额外的描述信息</li>
<li>drop database if exists company cascade; 删除不为空的数据库</li>
<li>show tables; 查看所在数据库的所有表</li>
<li>create table student(id int, name string); 创建一张表</li>
<li>desc student; 查看表结构</li>
<li>desc extended student; 查看表结构详细信息</li>
<li>desc formatted student; 查看表结构的格式化信息</li>
<li>insert into student values(1001, “zhangsan”); 向表中插入一条数据（慎用，会跑MR程序）</li>
<li>select * from student; 查询表中数据</li>
<li>!clear; 清空屏幕</li>
<li>quit; 退出hive</li>
<li>bin/hive -e “select * from student;” 不登录hive客户端直接操作hive</li>
<li>bin/hive -f /home/vinx/shell/select_stu.sql 执行HQL文件</li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>常用命令</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 数据倾斜</title>
    <url>/2018/08/06/Hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
    <content><![CDATA[<p>Hive数据倾斜问题   大表Join大表   大表Join小表   group By</p>
<a id="more"></a>

<h4 id="大表Join大表"><a href="#大表Join大表" class="headerlink" title="大表Join大表"></a>大表Join大表</h4><h5 id="思路一：SMB-Join"><a href="#思路一：SMB-Join" class="headerlink" title="思路一：SMB Join"></a>思路一：SMB Join</h5><p>smb是sort merge bucket操作，首先进行排序，继而合并，然后放到所对应的bucket中去，bucket是hive中和分区表类似的技术，就是按照key进行hash，相同的hash值都放到相同的buck中去。在进行两个表联合的时候。我们首先进行分桶，在join会大幅度的对性能进行优化。也就是说，在进行联合的时候，是bukect中的一小部分和bukect中的一小部分进行联合，table联合都是等值连接，相同的key都放到了同一个bucket中去了，那么在联合的时候就会大幅度的减小无关项的扫描。</p>
<ol>
<li>设置参数<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>两个表的bucket数量相等</li>
<li>Bucket列、Join列、Sort列、Skewed列为相同的字段</li>
<li>必须是应用在bucket mapjoin 的场景中</li>
<li>注意点<br>hive并不检查两个join的表是否已经做好bucket且sorted，需要用户自己去保证join的表，否则可能数据不正确。有两个办法<ul>
<li>hive.enforce.sorting 设置为true</li>
<li>手动生成符合条件的数据，通过在sql中用distributed c1 sort by c1 或者 cluster by c1，表创建时必须是CLUSTERED且SORTED，如下</li>
<li>创建Skewed Table提高有一个或多个列有倾斜值的表的性能，例如：<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_smb_2(<span class="keyword">mid</span> <span class="keyword">string</span>,age_id <span class="keyword">string</span>)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(<span class="keyword">mid</span>) SORTED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">INTO</span> <span class="number">500</span> BUCKETS</span><br><span class="line">SKEWED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">on</span> (<span class="number">001</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>案例<ol>
<li>设置先关参数<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_smb_2(<span class="keyword">mid</span> <span class="keyword">string</span>,age_id <span class="keyword">string</span>)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(<span class="keyword">mid</span>) SORTED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">INTO</span> <span class="number">500</span> BUCKETS</span><br><span class="line">SKEWED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">on</span> (<span class="number">001</span>)</span><br></pre></td></tr></table></figure></li>
<li>创建桶表<br>user表<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_info_bucket(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure>
domain表<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> domain_info_bucket(userid <span class="keyword">string</span>,domainid <span class="keyword">string</span>,<span class="keyword">domain</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure></li>
<li>分别倒入数据<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> user_info_bucket <span class="keyword">select</span> userid ,uname <span class="keyword">from</span> <span class="keyword">user</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> domain_info_bucket <span class="keyword">select</span> userid ,domainid,<span class="keyword">domain</span> <span class="keyword">from</span> doamin</span><br></pre></td></tr></table></figure></li>
<li>查询<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> user_info_bucket u  <span class="keyword">join</span> domain_info_bucket d <span class="keyword">on</span>(u.userid==d.userid)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h5 id="思路二：一分为二"><a href="#思路二：一分为二" class="headerlink" title="思路二：一分为二"></a>思路二：一分为二</h5><p>选择临时表的方式，将数据一分为二，把倾斜的key，和不倾斜的key分开处理，不倾斜的正常join，倾斜的根据情况选择mapjoin或加盐处理，最后结果union all结果</p>
<p>user表为用户基本表，domain为用户访问域名的宽表</p>
<p>注意：我们其实隐含使用到了mapjoin，hive中的参数为set hive.auto.convert.join=true;，自动开启，默认25M，不能超过1G。</p>
<ol>
<li>创建中间表<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_table(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure></li>
<li>count(*)出符合倾斜条件的数据存入中间表<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_table</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	d.userid,u.uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> userid</span><br><span class="line">	<span class="keyword">from</span> </span><br><span class="line">		(<span class="keyword">select</span> </span><br><span class="line">			userid,</span><br><span class="line">			<span class="keyword">count</span>(userid) u_cunt</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">from</span> </span><br><span class="line">			<span class="keyword">domain</span></span><br><span class="line">		<span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">			userid) t</span><br><span class="line">	<span class="keyword">where</span> u_cunt&gt;<span class="number">100</span>) d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u</span><br><span class="line"><span class="keyword">on</span> d.userid = u.userid;</span><br></pre></td></tr></table></figure></li>
<li>一分为二，分别查询中出结果，再union all<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	u1.userid,u1.uname,d1.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		d.userid,d.domain</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">domain</span> d</span><br><span class="line">	<span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">		tmp_table t</span><br><span class="line">	<span class="keyword">on</span> </span><br><span class="line">		d.userid = t.userid</span><br><span class="line">	<span class="keyword">where</span> </span><br><span class="line">		t.userid <span class="keyword">is</span> <span class="literal">null</span>) d1</span><br><span class="line"><span class="keyword">on</span> u1.userid = d1.userid</span><br><span class="line"></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">	u2.userid,u2.uname,d2.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	(<span class="keyword">select</span> 	</span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u2</span><br><span class="line"><span class="keyword">join</span> </span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		d.userid,d.domain</span><br><span class="line">		<span class="keyword">from</span></span><br><span class="line">			<span class="keyword">domain</span> d</span><br><span class="line">		<span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">			tmp_table t</span><br><span class="line">		<span class="keyword">on</span> </span><br><span class="line">			d.userid = t.userid</span><br><span class="line">		<span class="keyword">where</span> </span><br><span class="line">			t.userid <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>) d2</span><br><span class="line"><span class="keyword">on</span> u2.userid = d2.userid;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="大表Join小表"><a href="#大表Join小表" class="headerlink" title="大表Join小表"></a>大表Join小表</h4><h5 id="思路：MapJoin"><a href="#思路：MapJoin" class="headerlink" title="思路：MapJoin"></a>思路：MapJoin</h5><p>大表Join小表很好解决，把小表放进内存，大表再去匹配即可。</p>
<ol>
<li>开启MapJoin<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>调整MapJoin小表大小，默认25M(调整为可以容忍的大小)<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize</span><br></pre></td></tr></table></figure></li>
<li>如果是MR，小表放进Map，大表进入Mapper匹配Map(使用对象存储结果)</li>
</ol>
<h4 id="group-By解决"><a href="#group-By解决" class="headerlink" title="group By解决"></a>group By解决</h4><h5 id="思路：加盐去盐"><a href="#思路：加盐去盐" class="headerlink" title="思路：加盐去盐"></a>思路：加盐去盐</h5><ol>
<li>开启数据倾斜时的负载均衡<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata=<span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
<li>对groupby的key加盐去盐<br>开启上面这个参数，hive会自动拆解成两个MR，加盐去盐，最终输出结果<br>MR需要写两个MR，一个对key加盐，一个对key去盐</li>
</ol>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>数据倾斜</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 行列转换</title>
    <url>/2018/07/16/Hive%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<p>Hive 行列转换的sql写法</p>
<a id="more"></a>

<h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>使用函数：lateral view explode(split(column, ‘,’)) num<br>eg: 如表：t_row_to_column_tmp 数据如下，对tag列进行拆分<br><img src="/2018/07/16/Hive%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/1.png" alt></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,tag,tag_new</span><br><span class="line"><span class="keyword">from</span> t_row_to_column_tmp</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(tag, <span class="string">','</span>)) <span class="keyword">num</span> <span class="keyword">as</span> tag_new</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">212022894</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/16/Hive%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/2.png" alt></p>
<hr>
<h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><p>使用函数：concat_ws(‘,’,collect_set(column))<br>说明：collect_list 不去重，collect_set 去重。 column 的数据类型要求是 string<br>eg：如表：t_column_to_row ，根据id，对tag_new 进行合并<br><img src="/2018/07/16/Hive%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/3.png" alt></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,</span><br><span class="line"><span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_set(tag_new)) <span class="keyword">as</span> tag_col</span><br><span class="line"><span class="keyword">from</span> t_column_to_row</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/16/Hive%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/4.png" alt></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,</span><br><span class="line"><span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_list(tag_new)) <span class="keyword">as</span> tag_col</span><br><span class="line"><span class="keyword">from</span> t_column_to_row</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/16/Hive%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/5.png" alt></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hue 部署</title>
    <url>/2019/03/09/Hue_%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h4 id="hue-3-9-0-cdh5-16-2-的部署"><a href="#hue-3-9-0-cdh5-16-2-的部署" class="headerlink" title="hue-3.9.0-cdh5.16.2 的部署"></a>hue-3.9.0-cdh5.16.2 的部署</h4><p>本文主要包括以下内容：</p>
<ol>
<li>hue-3.9.0-cdh5.16.2 的部署流程</li>
<li>部署过程中遇到问题的解决方法<a id="more"></a>


</li>
</ol>
<h4 id="hue-下载-web-cdh5-3-9-0-5-16-2下载地址"><a href="#hue-下载-web-cdh5-3-9-0-5-16-2下载地址" class="headerlink" title="hue 下载 web cdh5-3.9.0_5.16.2下载地址"></a>hue 下载 web <a href="https://github.com/cloudera/hue/tree/cdh5-3.9.0_5.16.2" target="_blank" rel="noopener">cdh5-3.9.0_5.16.2下载地址</a></h4><h4 id="hue-依赖安装"><a href="#hue-依赖安装" class="headerlink" title="hue 依赖安装"></a>hue 依赖安装</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install krb5-devel cyrus-sasl-gssapi cyrus-sasl-devel libxml2-devel libxslt-devel</span><br><span class="line"></span><br><span class="line">sudo yum install mysql mysql-devel openldap-devel python-devel</span><br><span class="line"></span><br><span class="line">sudo yum install libffi-devel gmp mpfr gmp-devel</span><br><span class="line"></span><br><span class="line">sudo python-simplejson sqlite-devel</span><br><span class="line"></span><br><span class="line">sudo yum install libffi-devel libffi openssl-devel openssl openldap-devel mysql-devel gmp-devel gmp gcc-c++ rsync</span><br></pre></td></tr></table></figure>
<h4 id="hue-中文界面"><a href="#hue-中文界面" class="headerlink" title="hue 中文界面"></a>hue 中文界面</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi 此处为你的hue路径/desktop/core/src/desktop/settings.py</span><br><span class="line">LANGUAGE_CODE = 'zh_CN'</span><br><span class="line"><span class="meta">#</span><span class="bash">LANGUAGE_CODE = <span class="string">'en-us'</span></span></span><br><span class="line">LANGUAGES = [</span><br><span class="line">('en-us', _('English')),</span><br><span class="line">('zh_CN', _('Simplified Chinese')),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<h4 id="hue-编译"><a href="#hue-编译" class="headerlink" title="hue 编译"></a>hue 编译</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo make apps</span><br></pre></td></tr></table></figure>

<h4 id="hue-编译-可能出现的问题"><a href="#hue-编译-可能出现的问题" class="headerlink" title="hue 编译 可能出现的问题"></a>hue 编译 可能出现的问题</h4><h5 id="1-mysql-community-devel问题"><a href="#1-mysql-community-devel问题" class="headerlink" title="1.mysql-community-devel问题"></a>1.mysql-community-devel问题</h5><h6 id="需要注意mysql-community-devel的问题要找到对应的版本安装"><a href="#需要注意mysql-community-devel的问题要找到对应的版本安装" class="headerlink" title="需要注意mysql-community-devel的问题要找到对应的版本安装"></a>需要注意mysql-community-devel的问题要找到对应的版本安装</h6><p><a href="https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-community-devel-5.7.29-1.el7.x86_64.rpm" target="_blank" rel="noopener">mysql-community-devel-5.7.29-1.el7.x86_64.rpm官方下载地址</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo rpm -ivh mysql-community-devel-5.7.29-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>

<h5 id="2-must-have-python-development-packages-for-2-6-or-2-7…问题"><a href="#2-must-have-python-development-packages-for-2-6-or-2-7…问题" class="headerlink" title="2.must have python development packages for 2.6 or 2.7…问题"></a>2.must have python development packages for 2.6 or 2.7…问题</h5><h6 id="64位安装python-devel-x86-64"><a href="#64位安装python-devel-x86-64" class="headerlink" title="64位安装python-devel.x86_64"></a>64位安装python-devel.x86_64</h6><h6 id="32位安装python-devel-i686"><a href="#32位安装python-devel-i686" class="headerlink" title="32位安装python-devel.i686"></a>32位安装python-devel.i686</h6><h6 id="我这里安装-python-devel-x86-64"><a href="#我这里安装-python-devel-x86-64" class="headerlink" title="我这里安装:python-devel.x86_64"></a>我这里安装:python-devel.x86_64</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install python-devel.x86_64</span><br></pre></td></tr></table></figure>

<h4 id="hue-编译完成后，权限赋予"><a href="#hue-编译完成后，权限赋予" class="headerlink" title="hue 编译完成后，权限赋予"></a>hue 编译完成后，权限赋予</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo chmod 777 hue/desktop/</span><br><span class="line"></span><br><span class="line">sudo chmod 766 hue/desktop/desktop.db</span><br></pre></td></tr></table></figure>

<h4 id="hue-编译完成后，配置环境变量"><a href="#hue-编译完成后，配置环境变量" class="headerlink" title="hue 编译完成后，配置环境变量"></a>hue 编译完成后，配置环境变量</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HUE_HOME=此处为你的hue路径/hue</span><br><span class="line"></span><br><span class="line">export PATH=$HUE_HOME/build/env/bin:$PATH</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">source .***</span><br></pre></td></tr></table></figure>

<h4 id="hue-修改hue-ini文件"><a href="#hue-修改hue-ini文件" class="headerlink" title="hue 修改hue.ini文件"></a>hue 修改hue.ini文件</h4><h5 id="hue-ini文件文件路径："><a href="#hue-ini文件文件路径：" class="headerlink" title="hue.ini文件文件路径："></a>hue.ini文件文件路径：</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd 此处为你的hue路径/desktop/conf</span><br></pre></td></tr></table></figure>
<h5 id="修改hue-ini文件内容"><a href="#修改hue-ini文件内容" class="headerlink" title="修改hue.ini文件内容"></a>修改hue.ini文件内容</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Webserver listens on this address and port</span></span><br><span class="line"> http_host=你的host</span><br><span class="line"> http_port=8888</span><br><span class="line"><span class="meta">#</span><span class="bash"> Time zone name</span></span><br><span class="line"> time_zone=Asia/Shanghai</span><br></pre></td></tr></table></figure>

<h4 id="hue-启动"><a href="#hue-启动" class="headerlink" title="hue 启动"></a>hue 启动</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd $HUE_HOME/build/env/bin/ </span><br><span class="line">nohup supervisor &amp;</span><br></pre></td></tr></table></figure>

<h4 id="hue-登录"><a href="#hue-登录" class="headerlink" title="hue 登录"></a>hue 登录</h4><h5 id="你设置的host-8888"><a href="#你设置的host-8888" class="headerlink" title="你设置的host:8888"></a>你设置的host:8888</h5><h5 id="第一次登陆需要设置用户名密码"><a href="#第一次登陆需要设置用户名密码" class="headerlink" title="第一次登陆需要设置用户名密码"></a>第一次登陆需要设置用户名密码</h5><h3 id="至此-hue部署结束…"><a href="#至此-hue部署结束…" class="headerlink" title="至此 hue部署结束…"></a>至此 hue部署结束…</h3>]]></content>
      <categories>
        <category>Hue</category>
      </categories>
      <tags>
        <tag>Hue</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka 常用命令</title>
    <url>/2021/05/24/Kafka%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>Kafka 的常用命令整理</p>
<a id="more"></a>
<hr>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">创建主题</span></span><br><span class="line">kafka-topics --create --topic dwd_xxx_dwd_trd_order_received --partitions 2 --replication-factor 2 --zookeeper 10.0.15.130:2181,10.0.15.131:2181,10.0.15.132:2181/kafka</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看主题列表</span></span><br><span class="line">kafka-topics --list --zookeeper 10.0.15.130:2181,10.0.15.131:2181,10.0.15.132:2181/kafka</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">控制台发送消息</span></span><br><span class="line">kafka-console-producer --topic dwd_xxx_dwd_trd_order_received --broker-list 10.0.15.130:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">控制台消费主题</span></span><br><span class="line">kafka-console-consumer --topic dwd_xxx_dwd_trd_order_received --bootstrap-server 10.0.15.130:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">控制台消费主题（从头开始）</span></span><br><span class="line">kafka-console-consumer --from-beginning --topic dwd_xxx_dwd_trd_order_received --bootstrap-server 10.0.15.130:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">控制台消费主题（read_committed 的数据）</span></span><br><span class="line">kafka-console-consumer --topic flink_checkpoint_sink_test --bootstrap-server 10.0.15.130:9092 --isolation-level read_committed --from-beginning</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看主题详情</span></span><br><span class="line">kafka-topics --describe --topic dwd_xxx_dwd_trd_order_received --bootstrap-server 10.0.15.130:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">删除主题</span></span><br><span class="line">kafka-topics --delete --topic dwd_xxx_dwd_trd_order_received --zookeeper 10.0.15.130:2181,10.0.15.131:2181,10.0.15.132:2181/kafka</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查询所有 Group</span></span><br><span class="line">kafka-consumer-groups --bootstrap-server 10.0.15.130:9092 --list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查询某 Group Offset 信息</span></span><br><span class="line">kafka-consumer-groups --bootstrap-server 10.0.15.130:9092 --group test01 --describe</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">重置某 Group Offset</span></span><br><span class="line">kafka-consumer-groups --bootstrap-server 10.0.15.130:9092 --group test01 --topic ck_test:0 --reset-offsets --to-offset 5 --execute</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>KafkaEagle安装部署</title>
    <url>/2019/04/14/Kafka%20Eagle%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>Kafka Eagle安装部署和常用命令</p>
<a id="more"></a>
<hr>
<h4 id="kafka-zookeeper准备"><a href="#kafka-zookeeper准备" class="headerlink" title="kafka+zookeeper准备"></a>kafka+zookeeper准备</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">这里假设你已经把kafka+zookeeper安装完成，但是需要注意的几点是：</span><br><span class="line">1.kafka需要开启JMX端口</span><br><span class="line">    找到kafka安装路径，进入到bin文件夹，修改下面的地方。</span><br><span class="line">    vi kafka-server-start.sh</span><br><span class="line">...</span><br><span class="line">if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then</span><br><span class="line">    export KAFKA_HEAP_OPTS="-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"</span><br><span class="line">    export JMX_PORT="9999"</span><br><span class="line">fi</span><br><span class="line">...</span><br><span class="line">    参考链接：[kafka添加jmx端口]:https://ke.smartloli.org/3.Manuals/11.Metrics.html</span><br><span class="line"></span><br><span class="line">2.了解kafka在zookeeper配置</span><br><span class="line">    需要查看kafka的server.properties配置</span><br><span class="line">    找到zookeeper.connect此项配置，这个是要配置到eagle里面的</span><br><span class="line">    此处假设zookeeper.connect=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line">    ！！！PS:此处踩了坑，如果说这里的zookeeper地址后面加了其他路径，在kafka-eagle里面也要配置，</span><br><span class="line">    否则在kafka-eagle的Dashboard中无法读取到kafka的信息。比如我们有人安装的kafka集群里面就有    </span><br><span class="line">    192.168.18.11:2181/kafka1或者192.168.18.11:2181/kafka2这种地址。</span><br><span class="line">    如果你在安装kafka的时候没有配置多余路径，这样是最好的，如果有一定要加上。</span><br><span class="line"></span><br><span class="line">3.连通性测试</span><br><span class="line">  安装kafka-eagle的服务器，一定要提前测试是否能连接kafka注册的zookeeper端口</span><br><span class="line">  telnet 端口进行测试</span><br></pre></td></tr></table></figure>

<h4 id="JDK环境准备"><a href="#JDK环境准备" class="headerlink" title="JDK环境准备"></a>JDK环境准备</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">此处就忽略不说了，kafka既然会安装，也是依赖JDK环境的。版本没要求，但是最好是1.7以上。</span><br><span class="line">echo $JAVA_HOME</span><br><span class="line">测试一下JDK环境是否安装成功</span><br></pre></td></tr></table></figure>

<h4 id="开始安装kafka-eagle"><a href="#开始安装kafka-eagle" class="headerlink" title="开始安装kafka-eagle"></a>开始安装kafka-eagle</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.下载安装包</span><br><span class="line">软件安装目录建议按照自己的规范来，以后好找</span><br><span class="line">cd /tmp</span><br><span class="line">wget https://github.com/smartloli/kafka-eagle-bin/archive/v1.2.2.tar.gz</span><br><span class="line">tar zxf v1.2.2.tar.gz</span><br><span class="line">cd kafka-eagle-bin-1.2.2</span><br><span class="line">tar zxf kafka-eagle-web-1.2.2-bin.tar.gz -C /data/app/</span><br><span class="line">cd /data/app</span><br><span class="line">mv kafka-eagle-web-1.2.2 kafka-eagle</span><br><span class="line"></span><br><span class="line">2.环境配置</span><br><span class="line">vi /etc/profile</span><br><span class="line">export KE_HOME=/data/app/kafka-eagle</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin</span><br><span class="line">ps:此处的KE_HOME按照自己实际安装的目录来，我安装在/data/app/kafka-eagle下面</span><br><span class="line">如果你是安装的其他目录，别忘了修改。</span><br><span class="line"></span><br><span class="line">3.配置修改</span><br><span class="line">cd $&#123;KE_HOME&#125;/conf</span><br><span class="line">vi system-config.properties</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> multi zookeeper&amp;kafka cluster list -- The client connection address of the Zookeeper cluster is <span class="built_in">set</span> here</span></span><br><span class="line"><span class="meta">#</span><span class="bash">如果只有一个集群的话，就写一个cluster1就行了</span></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1,cluster2   </span><br><span class="line"><span class="meta">#</span><span class="bash">这里填上刚才上准备工作中的zookeeper.connect地址</span></span><br><span class="line">cluster1.zk.list=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line"><span class="meta">#</span><span class="bash">如果多个集群，继续写，如果没有注释掉</span></span><br><span class="line">cluster2.zk.list=192.168.18.21:2181,192.168.18.22:2181,192.168.18.23:2181/kafka </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> zk <span class="built_in">limit</span> -- Zookeeper cluster allows the number of clients to connect to</span></span><br><span class="line">kafka.zk.limit.size=25</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka eagel webui port -- WebConsole port access address</span></span><br><span class="line">kafka.eagle.webui.port=8048     ###web界面地址端口</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka offset storage -- Offset stored <span class="keyword">in</span> a Kafka cluster, <span class="keyword">if</span> stored <span class="keyword">in</span> the zookeeper, you can not use this option</span></span><br><span class="line">kafka.eagle.offset.storage=kafka</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> delete kafka topic token -- Set to delete the topic token, so that administrators can have the right to delete</span></span><br><span class="line">kafka.eagle.topic.token=keadmin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka sasl authenticate, current support SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">#</span><span class="bash">如果kafka开启了sasl认证，需要在这个地方配置sasl认证文件</span></span><br><span class="line">kafka.eagle.sasl.enable=false</span><br><span class="line">kafka.eagle.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">kafka.eagle.sasl.mechanism=PLAIN</span><br><span class="line">kafka.eagle.sasl.client=/data/kafka-eagle/conf/kafka_client_jaas.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">下面两项是配置数据库的，默认使用sqlite，如果量大，建议使用mysql，这里我使用的是sqlit</span></span><br><span class="line"><span class="meta">#</span><span class="bash">如果想使用mysql建议在文末查看官方文档</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Default use sqlite to store data</span></span><br><span class="line">kafka.eagle.driver=org.sqlite.JDBC</span><br><span class="line"><span class="meta">#</span><span class="bash"> It is important to note that the <span class="string">'/hadoop/kafka-eagle/db'</span> path must exist.</span></span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/data/app/kafka-eagle/db/ke.db   #这个地址，按照安装目录进行配置</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=smartloli</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> &lt;Optional&gt; <span class="built_in">set</span> mysql address</span></span><br><span class="line"><span class="meta">#</span><span class="bash">kafka.eagle.driver=com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="meta">#</span><span class="bash">kafka.eagle.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=<span class="literal">true</span>&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="meta">#</span><span class="bash">kafka.eagle.username=root</span></span><br><span class="line"><span class="meta">#</span><span class="bash">kafka.eagle.password=smartloli</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">如果开启了sasl认证，需要自己去修改kafka-eagle目录下的conf/kafka_client_jaas.conf</span><br><span class="line">此处不多说</span><br></pre></td></tr></table></figure>

<h4 id="启动kafka-eagle"><a href="#启动kafka-eagle" class="headerlink" title="启动kafka-eagle"></a>启动kafka-eagle</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd $&#123;KE_HOME&#125;/bin</span><br><span class="line">chmod +x ke.sh</span><br><span class="line">./ke.sh start</span><br><span class="line">查看日志是否出问题</span><br><span class="line">tailf ../log/log.log</span><br><span class="line">如果没问题，则直接登录</span><br><span class="line">http://host:8048/ke</span><br><span class="line">默认用户名:admin</span><br><span class="line">默认密码:12345</span><br><span class="line">如果进入到一下界面，就说明你安装成功了！</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/14/Kafka%20Eagle%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/01.png" alt="登陆界面"></p>
<h4 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h4><p>ZKPoolUtils.localhost-startStop-1 - ERROR - Unable to connect to zookeeper server within timeout: 100000</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">这个是网络问题，在kafka-eagle服务器上自己测试一下能否能telnet通配置的zk地址。</span><br><span class="line">cat system-config.properties|grep cluster1.zk.list</span><br><span class="line">测试这个配置的端口</span><br></pre></td></tr></table></figure>

<p>ERROR - Get kafka consumer has error,msg is No resolvable bootstrap urls given in bootstrap.servers</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">这个问题是配置的zk地址有问题，看看kafka配置的zk地址</span><br><span class="line">跟自己在eagle上配置的地址是否相同，有没有少目录或者端口配置。</span><br><span class="line">可以看看文章开头，kafka+zookeeper准备-了解kafka在zookeeper配置</span><br></pre></td></tr></table></figure>

<p>zookeeper state changed (AuthFailed)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">这个问题有两种情况</span><br><span class="line">1.认证的问题，确认你配置的认证文件是否正确</span><br><span class="line">2.zk地址问题，看看kafka配置的zk地址，跟自己在eagle上配置的地址是否相同</span><br><span class="line">可以看看文章开头，kafka+zookeeper准备-了解kafka在zookeeper配置</span><br></pre></td></tr></table></figure>

<h4 id="kafka-eagle使用"><a href="#kafka-eagle使用" class="headerlink" title="kafka-eagle使用"></a>kafka-eagle使用</h4><p>kafka-eagle 官方文档:    <a href="https://ke.smartloli.org/2.Install/2.Installing.html" target="_blank" rel="noopener">https://ke.smartloli.org/2.Install/2.Installing.html</a></p>
<p>kafka-eagle 下载地址:    <a href="http://download.smartloli.org/" target="_blank" rel="noopener">http://download.smartloli.org/</a></p>
<p>kafka-eagle git地址:        <a href="https://github.com/smartloli/kafka-eagle" target="_blank" rel="noopener">https://github.com/smartloli/kafka-eagle</a></p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux dmesg 命令查看 kill 程序的日志</title>
    <url>/2021/06/01/Linuxdmseg%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>程序被 kill 掉，但是并不知道为什么被 kill，</p>
<p>是因为内存原因还是其他原因，通过 dmesg 命令可以确认这部分信息。</p>
<a id="more"></a>

<hr>
<h4 id="dmesg-命令的三种用法"><a href="#dmesg-命令的三种用法" class="headerlink" title="dmesg 命令的三种用法"></a>dmesg 命令的三种用法</h4><p>三种方法都能得到 kill 信息，推荐使用第一种。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 有详细的时间</span></span><br><span class="line">grep -i 'killed process' /var/log/messages</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查询结果</span></span><br><span class="line">[root@cdh12 ~]# grep -i 'killed process' /var/log/messages</span><br><span class="line">Jun 17 04:12:20 cdh12 kernel: Killed process 4513 (palo_be), UID 0, total-vm:52091820kB, anon-rss:40273088kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">Jun 19 04:30:45 cdh12 kernel: Killed process 20561 (palo_be), UID 0, total-vm:51151564kB, anon-rss:41944112kB, file-rss:0kB, shmem-rss:0kB</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 没有详细的时间</span></span><br><span class="line">dmesg | grep -i 'killed process'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查询结果</span></span><br><span class="line">[root@cdh12 ~]# dmesg | grep -i 'killed process'</span><br><span class="line">[10257349.805906] Killed process 4513 (palo_be), UID 0, total-vm:52091820kB, anon-rss:40273088kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">[10431255.475118] Killed process 20561 (palo_be), UID 0, total-vm:51151564kB, anon-rss:41944112kB, file-rss:0kB, shmem-rss:0kB</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 有详细的时间和更多的信息</span></span><br><span class="line">journalctl -xb | egrep -i 'killed process' -C 5</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 得到的信息比较多，但是感觉没什么用</span></span><br><span class="line">--</span><br><span class="line">Jun 19 04:30:45 cdh12 kernel: [ 5108]     0  5108    28321      134      14        0             0 sqoop</span><br><span class="line">Jun 19 04:30:45 cdh12 kernel: [ 5154]     0  5154    28321       91      11        0             0 sqoop</span><br><span class="line">Jun 19 04:30:45 cdh12 kernel: [ 5155]     0  5155    28356      209      13        0             0 bash</span><br><span class="line">Jun 19 04:30:45 cdh12 kernel: [ 5179]     0  5179    28359      131      13        0             0 bash</span><br><span class="line">Jun 19 04:30:45 cdh12 kernel: Out of memory: Kill process 20561 (palo_be) score 629 or sacrifice child</span><br><span class="line">Jun 19 04:30:45 cdh12 kernel: Killed process 20561 (palo_be), UID 0, total-vm:51151564kB, anon-rss:41944112kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">Jun 19 04:30:49 cdh12 systemd-logind[778]: Removed session 20698.</span><br><span class="line">-- Subject: Session 20698 has been terminated</span><br><span class="line">-- Defined-By: systemd</span><br><span class="line">-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel</span><br><span class="line">-- Documentation: http://www.freedesktop.org/wiki/Software/systemd/multiseat</span><br></pre></td></tr></table></figure>

<h4 id="total-vm、anon-rss、file-rss含义"><a href="#total-vm、anon-rss、file-rss含义" class="headerlink" title="total-vm、anon-rss、file-rss含义"></a>total-vm、anon-rss、file-rss含义</h4><p>个人感觉 anon-rss 比较重要（实际占用的物理内存）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">total-vm：进程总共使用的虚拟内存</span><br><span class="line">anon-rss：虚拟内存实际占用的物理内存</span><br><span class="line">file-rss：虚拟内存实际占用的磁盘空间</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka工作流程</title>
    <url>/2019/04/13/Kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>Kafka工作流程的相关内容</p>
<a id="more"></a>
<hr>
<h3 id="Kafka生产过程"><a href="#Kafka生产过程" class="headerlink" title="Kafka生产过程"></a>Kafka生产过程</h3><h4 id="写入方式"><a href="#写入方式" class="headerlink" title="写入方式"></a>写入方式</h4><p>producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。</p>
<h4 id="分区（Partition）"><a href="#分区（Partition）" class="headerlink" title="分区（Partition）"></a>分区（Partition）</h4><p>Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。</p>
<p>Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。</p>
<p>消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：<br><img src="/2019/04/13/Kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/anatomy-of-a-topic.png" alt="Topic的结构"><br>上图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响。<br><img src="/2019/04/13/Kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/partition-producer-consumer.png" alt="Partition的消息是有序的"><br>我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。</p>
<p>发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。</p>
<ol>
<li>分区的原因<ol>
<li>方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</li>
<li>可以提高并发，因为可以以Partition为单位读写了。传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。<br>Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。</li>
</ol>
</li>
<li>分区的原则<ol>
<li>指定了partition，则直接使用；</li>
<li>未指定partition但指定key，通过对key的value进行hash出一个partition；</li>
<li>partition和key都未指定，使用轮询选出一个partition。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DefaultPartitioner类</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h4 id="副本（Replication）"><a href="#副本（Replication）" class="headerlink" title="副本（Replication）"></a>副本（Replication）</h4><p>同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。</p>
<h4 id="写入流程"><a href="#写入流程" class="headerlink" title="写入流程"></a>写入流程</h4><p>producer写入消息流程如下：</p>
<ol>
<li>producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader</li>
<li>producer将消息发送给该leader</li>
<li>leader将消息写入本地log</li>
<li>followers从leader pull消息，写入本地log后向leader发送ACK</li>
<li>leader收到所有ISR（In Sync Replicas）中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK</li>
</ol>
<h4 id="Broker保存信息"><a href="#Broker保存信息" class="headerlink" title="Broker保存信息"></a>Broker保存信息</h4><h5 id="存储方式"><a href="#存储方式" class="headerlink" title="存储方式"></a>存储方式</h5><p>物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[emerk@hadoop001 logs]$ ll</span><br><span class="line">drwxrwxr-x. 2 emerk emerk  4096 3月   6 14:37 first-0</span><br><span class="line">drwxrwxr-x. 2 emerk emerk  4096 3月   6 14:35 first-1</span><br><span class="line">drwxrwxr-x. 2 emerk emerk  4096 3月   6 14:37 first-2</span><br><span class="line">[emerk@hadoop001 logs]$ cd first-0</span><br><span class="line">[emerk@hadoop001 first-0]$ ll</span><br><span class="line">-rw-rw-r--. 1 emerk emerk 10485760 3月   6 14:33 00000000000000000000.index</span><br><span class="line">-rw-rw-r--. 1 emerk emerk      219 3月   6 15:07 00000000000000000000.log</span><br><span class="line">-rw-rw-r--. 1 emerk emerk 10485756 3月   6 14:33 00000000000000000000.timeindex</span><br><span class="line">-rw-rw-r--. 1 emerk emerk        8 3月   6 14:37 leader-epoch-checkpoint</span><br></pre></td></tr></table></figure>
<h5 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h5><p>无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：</p>
<ol>
<li>基于时间：log.retention.hours=168</li>
<li>基于大小：log.retention.bytes=1073741824<br>需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。</li>
</ol>
<h4 id="Kafka消费过程"><a href="#Kafka消费过程" class="headerlink" title="Kafka消费过程"></a>Kafka消费过程</h4><p>kafka提供了两套consumer API：高级Consumer API和低级API。</p>
<h5 id="消费模型"><a href="#消费模型" class="headerlink" title="消费模型"></a>消费模型</h5><p>消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）。</p>
<p>基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的。</p>
<p>Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。<br><img src="/2019/04/13/Kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/consumer-pull.png" alt="consumer的pull拉取模式"><br>在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉。</p>
<h5 id="高级API"><a href="#高级API" class="headerlink" title="高级API"></a>高级API</h5><p>高级API优点：</p>
<ol>
<li>写起来简单</li>
<li>不需要自行去管理offset，系统通过zookeeper自行管理</li>
<li>不需要管理分区、副本等情况，系统自动管理</li>
<li>消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset）</li>
<li>可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）</li>
</ol>
<p>高级API缺点：</p>
<ol>
<li>不能自行控制offset（对于某些特殊需求来说）</li>
<li>不能细化控制如分区、副本、zk等</li>
</ol>
<h5 id="低级API"><a href="#低级API" class="headerlink" title="低级API"></a>低级API</h5><p>低级API优点：</p>
<ol>
<li>能够让开发者自己控制offset，想从哪里读取就从哪里读取</li>
<li>自行控制连接分区，对分区自定义进行负载均衡</li>
<li>对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）</li>
</ol>
<p>低级API缺点：</p>
<ol>
<li>太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等</li>
</ol>
<h5 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h5><p>消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。</p>
<p>在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。</p>
<h5 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h5><p>consumer采用pull（拉）模式从broker中读取数据。</p>
<p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p>
<p>对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p>
<p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx的简介、安装、常用操作</title>
    <url>/2018/03/12/Nginx%E7%9A%84%E7%AE%80%E4%BB%8B%E3%80%81%E5%AE%89%E8%A3%85%E3%80%81%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h3 id="Nginx的功能介绍"><a href="#Nginx的功能介绍" class="headerlink" title="Nginx的功能介绍"></a>Nginx的功能介绍</h3><p>Nginx是一个轻量级、高性能、稳定性高、并发性好的HTTP和反向代理服务器。也是由于其的特性，其应用非常广。</p>
<a id="more"></a>
<hr>
<h4 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h4><p>在说反向代理之前说一下正向代理：某些情况下，代理服务器代理我们用户去访问服务器，需要用户手动的设置代理服务器的ip和端口号。正向代理的最大特点是代理服务器在客户端。<br>反向代理是用来代理服务器的，代理服务器代理我们要访问的目标服务器。代理服务器接受请求，然后将请求转发给内部网络的服务器(集群化)，并将从服务器上得到的结果返回给客户端，此时代理服务器对外就表现为一个服务器。反向代理的最大特点是代理服务器在服务器端。<br><img src="/2018/03/12/Nginx%E7%9A%84%E7%AE%80%E4%BB%8B%E3%80%81%E5%AE%89%E8%A3%85%E3%80%81%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/Nginx01.jpg" alt="Nginx反向代理"></p>
<h4 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h4><p>负载均衡是在高并发情况下需要使用。其原理就是将数据流量分摊到多个服务器执行，减轻每台服务器的压力，多台服务器(集群)共同完成工作任务，从而提高了数据的吞吐量。<br>Nginx可使用的负载均衡策略有：轮询（默认）、权重、ip_hash、url_hash(第三方)、fair(第三方)<br><img src="/2018/03/12/Nginx%E7%9A%84%E7%AE%80%E4%BB%8B%E3%80%81%E5%AE%89%E8%A3%85%E3%80%81%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/Nginx02.jpg" alt="Nginx负载均衡策略"><br>Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。<br><img src="/2018/03/12/Nginx%E7%9A%84%E7%AE%80%E4%BB%8B%E3%80%81%E5%AE%89%E8%A3%85%E3%80%81%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/Nginx03.jpg" alt="Nginx负载均衡策略"></p>
<h4 id="动静分离"><a href="#动静分离" class="headerlink" title="动静分离"></a>动静分离</h4><p>Nginx提供的动静分离是指把动态请求和静态请求分离开，合适的服务器处理相应的请求，使整个服务器系统的性能、效率更高。<br>Nginx可以根据配置对不同的请求做不同转发，这是动态分离的基础。静态请求对应的静态资源可以直接放在Nginx上做缓冲，更好的做法是放在相应的缓冲服务器上。动态请求由相应的后端服务器处理。<br>常见的静态文件有js、css、html，常见的动态文件有jsp、servlet</p>
<h3 id="Nginx的安装"><a href="#Nginx的安装" class="headerlink" title="Nginx的安装"></a>Nginx的安装</h3><h4 id="安装所需环境"><a href="#安装所需环境" class="headerlink" title="安装所需环境"></a>安装所需环境</h4><p>一. gcc 安装<br>安装 nginx 需要先将官网下载的源码进行编译，编译依赖 gcc 环境，如果没有 gcc 环境，则需要安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install gcc-c++</span><br></pre></td></tr></table></figure>
<p>二. PCRE pcre-devel 安装<br>PCRE(Perl Compatible Regular Expressions) 是一个Perl库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式，所以需要在 linux 上安装 pcre 库，pcre-devel 是使用 pcre 开发的一个二次开发库。nginx也需要此库。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y pcre pcre-devel</span><br></pre></td></tr></table></figure>
<p>三. zlib 安装<br>zlib 库提供了很多种压缩和解压缩的方式， nginx 使用 zlib 对 http 包的内容进行 gzip ，所以需要在 Centos 上安装 zlib 库。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y zlib zlib-devel</span><br></pre></td></tr></table></figure>
<p>四. OpenSSL 安装<br>OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。<br>nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y openssl openssl-devel</span><br></pre></td></tr></table></figure>
<p>五. 综合操作下载命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel -y</span><br></pre></td></tr></table></figure>
<h4 id="下载安装Nginx"><a href="#下载安装Nginx" class="headerlink" title="下载安装Nginx"></a>下载安装Nginx</h4><ol>
<li>官网下载.tar.gz安装包，地址：<a href="https://nginx.org/en/download.html" target="_blank" rel="noopener">https://nginx.org/en/download.html</a></li>
<li>使用wget命令下载（推荐）。确保系统已经安装了wget，如果没有安装，执行 yum install wget 安装。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget -c https://nginx.org/download/nginx-1.12.0.tar.gz</span><br></pre></td></tr></table></figure></li>
<li>解压<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf nginx-1.12.0.tar.gz -C ../app</span><br><span class="line">cd nginx-1.12.0</span><br></pre></td></tr></table></figure></li>
<li>默认配置(默认是安装在/usr/local/nginx，不推介指定安装目录)<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure</span><br></pre></td></tr></table></figure></li>
<li>编译安装(root用户)<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure></li>
<li>查找安装路径<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">whereis nginx</span><br></pre></td></tr></table></figure>
结果：/usr/local/nginx、</li>
</ol>
<h4 id="Nginx常用操作"><a href="#Nginx常用操作" class="headerlink" title="Nginx常用操作"></a>Nginx常用操作</h4><ol>
<li>启动、停止nginx<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/nginx/sbin/</span><br><span class="line">./nginx 	        #启动</span><br><span class="line">./nginx -s stop		#停止</span><br><span class="line">./nginx -s quit		#退出</span><br><span class="line">./nginx -s reload	#重新加载配置文件</span><br></pre></td></tr></table></figure>
启动时报80端口被占用:nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)<br>解决办法：安装net-tool 包：yum install net-tools<br>命令详解：<br>./nginx -s quit:此方式停止步骤是待nginx进程处理任务完毕进行停止。<br>./nginx -s stop:此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程。</li>
<li>重启Nginx<br>对 nginx 进行重启相当于先停止再启动，即先执行停止命令再执行启动命令。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx -s quit</span><br><span class="line">./nginx</span><br></pre></td></tr></table></figure></li>
<li>重新加载配置文件<br>当 ngin x的配置文件 nginx.conf 修改后，要想让配置生效需要重启 nginx，使用-s reload不用先停止 ngin x再启动 nginx 即可将配置信息在 nginx 中生效。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx -s reload</span><br></pre></td></tr></table></figure>
启动成功后，可打开浏览器访问，默认端口80</li>
<li>配置开机自启<br>即在rc.local增加启动代码就可以了。<br>vi /etc/rc.local<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/nginx/sbin/nginx</span><br></pre></td></tr></table></figure>
设置执行权限：chmod 755 rc.local</li>
</ol>
]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx 负载均衡配置</title>
    <url>/2021/08/02/Nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>需要进行负载均衡的配置</p>
<p>以下是经过测试的 Nginx 负载均衡的配置方法</p>
<a id="more"></a>
<hr>
<h4 id="conf-文件修改"><a href="#conf-文件修改" class="headerlink" title="conf 文件修改"></a>conf 文件修改</h4><p>1.修改nginx.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/nginx/conf</span><br><span class="line">vi nginx.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在 http 配置最后增加以下内容</span></span><br><span class="line">http &#123;</span><br><span class="line">   ...</span><br><span class="line"><span class="meta">   #</span><span class="bash">包含server配置文件</span></span><br><span class="line">   include /usr/local/nginx/conf/server/*.conf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2.创建负载均衡配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/nginx/conf</span><br><span class="line">mkdir server</span><br><span class="line">vi ubt_data_collection.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#负载均衡</span></span><br><span class="line"><span class="comment">#upstream 的命名可以随意配置</span></span><br><span class="line"><span class="attr">upstream</span> <span class="string">ubt_data_collection &#123;</span></span><br><span class="line"><span class="comment">    #负载均衡的机器、端口、权重</span></span><br><span class="line">    <span class="attr">server</span> <span class="string">cdh8:8001 weight=5;</span></span><br><span class="line">    <span class="attr">server</span> <span class="string">cdh8:8002 weight=5;</span></span><br><span class="line"><span class="attr">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">server</span> <span class="string">&#123;</span></span><br><span class="line"><span class="comment">    #监听的本地请求端口，对外暴漏的请求端口</span></span><br><span class="line">    <span class="attr">listen</span>       <span class="string">8003;</span></span><br><span class="line"><span class="comment">    #server_name和proxy_set_header Host对应，不然就配置localhost</span></span><br><span class="line"><span class="comment">    #可以有多个，空格分开</span></span><br><span class="line">    <span class="attr">server_name</span>  <span class="string">localhost;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">    #web代理到应用</span></span><br><span class="line">    <span class="attr">location</span> <span class="string">/&#123;</span></span><br><span class="line"><span class="comment">        #proxy_pass 配置对应的 upstream 信息</span></span><br><span class="line">        <span class="attr">proxy_pass</span> <span class="string">http://ubt_data_collection;</span></span><br><span class="line">        <span class="attr">proxy_set_header</span> <span class="string">Host $host;</span></span><br><span class="line">        <span class="attr">proxy_set_header</span> <span class="string">X-Real-IP $remote_addr;</span></span><br><span class="line">        <span class="attr">proxy_set_header</span> <span class="string">X-Forwarded-For $proxy_add_x_forwarded_for;</span></span><br><span class="line">    <span class="attr">&#125;</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure>

<h4 id="重新加载配置文件"><a href="#重新加载配置文件" class="headerlink" title="重新加载配置文件"></a>重新加载配置文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/nginx/sbin</span><br><span class="line">./nginx -s reload</span><br></pre></td></tr></table></figure>



<h4 id="测试用-SpringBoot-代码"><a href="#测试用-SpringBoot-代码" class="headerlink" title="测试用 SpringBoot 代码"></a>测试用 SpringBoot 代码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.offlineClass;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NginxtestApplication</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;server.port:&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String port;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(NginxtestApplication<span class="class">.<span class="keyword">class</span>, <span class="title">args</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">""</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"call me "</span> + port);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"i am "</span> + port;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>启动命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java -jar nginx_test.jar --server.port=8001</span><br><span class="line">java -jar nginx_test.jar --server.port=8002</span><br></pre></td></tr></table></figure>

<p>请求地址</p>
<figure class="highlight http"><table><tr><td class="code"><pre><span class="line"><span class="attribute">http://cdh8:8003/</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala 模式匹配</title>
    <url>/2018/05/06/Scala_%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/</url>
    <content><![CDATA[<h3 id="Scala-模式匹配"><a href="#Scala-模式匹配" class="headerlink" title="Scala 模式匹配"></a>Scala 模式匹配</h3><p>类似于java的switch case<br>可以匹配内容、集合、类型…</p>
<a id="more"></a>
<hr>
<h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">变量 <span class="keyword">match</span>&#123;</span><br><span class="line">	<span class="keyword">case</span> 值<span class="number">1</span> =&gt; ...</span><br><span class="line">	<span class="keyword">case</span> 值<span class="number">2</span> =&gt; ...</span><br><span class="line">	<span class="keyword">case</span> 值<span class="number">3</span> =&gt; ...</span><br><span class="line">	_ =&gt; ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//*scala不需要break</span></span><br></pre></td></tr></table></figure>
<h5 id="根据集合匹配："><a href="#根据集合匹配：" class="headerlink" title="根据集合匹配："></a>根据集合匹配：</h5><p>例1：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> a1 = <span class="type">Array</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a11 = a1(<span class="type">Random</span>.nextInt(a1.length))</span><br><span class="line"></span><br><span class="line">a11 <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="string">"a"</span> =&gt; print(<span class="string">"这是a"</span>)</span><br><span class="line">  <span class="keyword">case</span> <span class="string">"b"</span> =&gt; print(<span class="string">"这是b"</span>)</span><br><span class="line">  <span class="keyword">case</span> <span class="string">"c"</span> =&gt; print(<span class="string">"这是c"</span>)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; print(<span class="string">"这是哪个？"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>例2：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greeting</span></span>(array: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  array <span class="keyword">match</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> <span class="type">Array</span>(<span class="string">"a"</span>) =&gt; println(<span class="string">"这是只有一个a"</span>)</span><br><span class="line">	<span class="keyword">case</span> <span class="type">Array</span>(x,y) =&gt; println(<span class="string">"这是"</span> + x + <span class="string">", "</span> + y)</span><br><span class="line">	<span class="keyword">case</span> <span class="type">Array</span>(<span class="string">"a"</span>,_*) =&gt; println(<span class="string">"这是a开头的多个"</span>)</span><br><span class="line">	<span class="keyword">case</span> _ =&gt; println(<span class="string">"都没有匹配上"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">greeting(<span class="type">Array</span>(<span class="string">"a"</span>))</span><br><span class="line">greeting(<span class="type">Array</span>(<span class="string">"a"</span>,<span class="string">"b"</span>))</span><br><span class="line">greeting(<span class="type">Array</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>))</span><br><span class="line">greeting(<span class="type">Array</span>(<span class="string">"c"</span>))</span><br></pre></td></tr></table></figure>
<h6 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">这是只有一个a</span><br><span class="line">这是a, b</span><br><span class="line">这是a开头的多个</span><br><span class="line">都没有匹配上</span><br></pre></td></tr></table></figure>

<p>例3：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greeting1</span></span>(array: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  array <span class="keyword">match</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> <span class="string">"a"</span>:: <span class="type">Nil</span> =&gt; println(<span class="string">"有且只有一个a"</span>)</span><br><span class="line">	<span class="keyword">case</span> x::y::<span class="type">Nil</span> =&gt; println(<span class="string">"这是"</span> + x + <span class="string">", "</span> + y)</span><br><span class="line">	<span class="keyword">case</span> <span class="string">"a"</span>::tail =&gt; println(<span class="string">"这是a开头的多个"</span>)</span><br><span class="line">	<span class="keyword">case</span> _ =&gt; println(<span class="string">"都没有匹配上"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">greeting1(<span class="type">List</span>(<span class="string">"a"</span>))</span><br><span class="line">greeting1(<span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>))</span><br><span class="line">greeting1(<span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>))</span><br><span class="line">greeting1(<span class="type">List</span>(<span class="string">"c"</span>))</span><br></pre></td></tr></table></figure>
<h6 id="结果：-1"><a href="#结果：-1" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">这是只有一个a</span><br><span class="line">这是a, b</span><br><span class="line">这是a开头的多个</span><br><span class="line">都没有匹配上</span><br></pre></td></tr></table></figure>
<h5 id="根据类型匹配："><a href="#根据类型匹配：" class="headerlink" title="根据类型匹配："></a>根据类型匹配：</h5><p>例1：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matchType</span></span>(obj: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  obj <span class="keyword">match</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> i:<span class="type">Int</span> =&gt; println(<span class="string">"这是Int类型"</span>)</span><br><span class="line">	<span class="keyword">case</span> s:<span class="type">String</span> =&gt; println(<span class="string">"这是String类型"</span>)</span><br><span class="line">	<span class="keyword">case</span> m:<span class="type">Map</span>[_,_] =&gt; println(<span class="string">"这是Map类型"</span>)</span><br><span class="line">	<span class="keyword">case</span> _ =&gt; println(<span class="string">"未找到的类型"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">matchType(<span class="number">1</span>)</span><br><span class="line">matchType(<span class="string">"a"</span>)</span><br><span class="line">matchType(<span class="type">Map</span>(<span class="string">"a"</span> -&gt; <span class="number">1</span>))</span><br><span class="line">matchType(<span class="number">1</span>L)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="结果：-2"><a href="#结果：-2" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">这是<span class="type">Int</span>类型</span><br><span class="line">这是<span class="type">String</span>类型</span><br><span class="line">这是<span class="type">Map</span>类型</span><br><span class="line">未找到的类型</span><br></pre></td></tr></table></figure>

<p>例2：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> aas = <span class="type">Map</span>(<span class="string">"a"</span> -&gt; <span class="string">"a1"</span>, <span class="string">"b"</span> -&gt; <span class="string">"b1"</span>, <span class="string">"c"</span> -&gt; <span class="string">"c1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matchaa</span></span>(key:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  <span class="keyword">val</span> aa = aas.get(key)</span><br><span class="line">  aa <span class="keyword">match</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> <span class="type">Some</span>(aa) =&gt; println(<span class="string">"key对应的value为"</span> + aa)</span><br><span class="line">	<span class="keyword">case</span> <span class="type">None</span> =&gt; println(<span class="string">"key没有匹配上..."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">matchaa(<span class="string">"a"</span>)</span><br><span class="line">matchaa(<span class="string">"b"</span>)</span><br><span class="line">matchaa(<span class="string">"c"</span>)</span><br><span class="line">matchaa(<span class="string">"d"</span>)</span><br></pre></td></tr></table></figure>
<h6 id="结果：-3"><a href="#结果：-3" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">key对应的value为a1</span><br><span class="line">key对应的value为b1</span><br><span class="line">key对应的value为c1</span><br><span class="line">key没有匹配上...</span><br></pre></td></tr></table></figure>
<h5 id="模式匹配在工作中主要用于异常处理："><a href="#模式匹配在工作中主要用于异常处理：" class="headerlink" title="模式匹配在工作中主要用于异常处理："></a>模式匹配在工作中主要用于异常处理：</h5><p>IO操作、数据库操作<br>1）open<br>2）todo…<br>3）release</p>
<p>例1：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">  <span class="keyword">val</span> i = <span class="number">1</span>/<span class="number">0</span></span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> e:<span class="type">ArithmeticException</span> =&gt; println(<span class="string">"除数不能为0..."</span>)</span><br><span class="line">  <span class="comment">//case e:Exception =&gt; println(e.getMessage)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="结果：-4"><a href="#结果：-4" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">除数不能为<span class="number">0.</span>..</span><br></pre></td></tr></table></figure>

<p>例2：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">  <span class="keyword">val</span> i = <span class="number">1</span>/<span class="number">0</span></span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="comment">//case e:ArithmeticException =&gt; println("除数不能为0...")</span></span><br><span class="line">  <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt; println(e.getMessage)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="结果：-5"><a href="#结果：-5" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">/ by zero</span><br></pre></td></tr></table></figure>

<p>例3：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">  <span class="comment">//open</span></span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt; println(e.getMessage)</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="comment">//关闭文件流或释放数据库连接</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala 使用ScalikeJDBC操作MySQL</title>
    <url>/2018/05/13/Scala%20%E4%BD%BF%E7%94%A8ScalikeJDBC%E6%93%8D%E4%BD%9CMySQL/</url>
    <content><![CDATA[<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li>简介</li>
<li>配置</li>
<li>操作数据库</li>
</ol>
<a id="more"></a>
<hr>
<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>ScalikeJDBC是一款给Scala开发者使用的简介访问类库，它是基于SQL的，使用者只需要关注SQL逻辑的编写，所有的数据库操作都交给ScalikeJDBC。<br>这个类库内置包含了JDBCAPI，并且给用户提供了简单易用并且非常灵活的API。<br>并且，QueryDSl（通用查询查询框架）使你的代码类型安全，半年过去可重复使用。我们可以在生产环境大胆地使用这款DB访问类库。</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>解决依赖</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;scala.version&gt;<span class="number">2.11</span><span class="number">.8</span>&lt;/scala.version&gt;</span><br><span class="line">    &lt;scalikejdbc.version&gt;<span class="number">3.3</span><span class="number">.2</span>&lt;/scalikejdbc.version&gt;</span><br><span class="line">    &lt;mysql.jdbc.version&gt;<span class="number">5.1</span><span class="number">.38</span>&lt;/mysql.jdbc.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&lt;!--<span class="type">Scala</span>相关依赖--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--scalikejdbc相关依赖--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc_2<span class="number">.11</span>&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;scalikejdbc.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scalikejdbc-config_2<span class="number">.11</span>&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;scalikejdbc.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;mysql.jdbc.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h4 id="解决配置"><a href="#解决配置" class="headerlink" title="解决配置"></a>解决配置</h4><p>在src的main目录下配置一个resource文件夹，文件夹下再创建一个application.conf</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">db.<span class="keyword">default</span>.driver=<span class="string">"com.mysql.jdbc.Driver"</span></span><br><span class="line">db.<span class="keyword">default</span>.url=<span class="string">"jdbc:mysql://hadoop001/ruoze_d6?characterEncoding=utf-8"</span></span><br><span class="line">db.<span class="keyword">default</span>.user=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.password=<span class="string">"123456"</span></span><br><span class="line"></span><br><span class="line"># <span class="type">Connection</span> <span class="type">Pool</span> settings</span><br><span class="line">db.<span class="keyword">default</span>.poolInitialSize=<span class="number">10</span></span><br><span class="line">db.<span class="keyword">default</span>.poolMaxSize=<span class="number">20</span></span><br><span class="line">db.<span class="keyword">default</span>.connectionTimeoutMillis=<span class="number">1000</span></span><br></pre></td></tr></table></figure>

<h4 id="操作数据库"><a href="#操作数据库" class="headerlink" title="操作数据库"></a>操作数据库</h4><ol>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> Employer(</span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">10</span>),</span><br><span class="line">    age <span class="built_in">varchar</span>(<span class="number">4</span>),</span><br><span class="line">    salary  <span class="built_in">varchar</span>(<span class="number">10</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li><p>scala编程实现增删改查操作</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.wsk.bigdata.scala.scalikejdbc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scalikejdbc._</span><br><span class="line"><span class="keyword">import</span> scalikejdbc.config._</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义样例类获取数据</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employer</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">JdbcTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">DBs</span>.setupAll()	<span class="comment">//初始化配置</span></span><br><span class="line">	</span><br><span class="line">    <span class="comment">//数据</span></span><br><span class="line">    <span class="keyword">val</span> employers = <span class="type">List</span>(<span class="type">Employer</span>(<span class="string">"zhangsan"</span>, <span class="number">20</span>, <span class="number">18000</span>), <span class="type">Employer</span>(<span class="string">"zhangliu"</span>, <span class="number">50</span>, <span class="number">300000</span>), <span class="type">Employer</span>(<span class="string">"lisi"</span>, <span class="number">22</span>, <span class="number">22000</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//批量插入</span></span><br><span class="line">    insert(employers)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//查询出结果</span></span><br><span class="line">    <span class="keyword">val</span> results = select()</span><br><span class="line">    <span class="keyword">for</span> (employer &lt;- results) &#123;</span><br><span class="line">      println(employer.name, employer.age, employer.salary)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//修改</span></span><br><span class="line">    update(<span class="number">1000</span>, <span class="string">"zhangsan"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//根据条件删除</span></span><br><span class="line">    deleteByname(<span class="string">"zhangliu"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//删除所有</span></span><br><span class="line">    deleteAll()</span><br><span class="line"></span><br><span class="line">   	<span class="comment">//关闭资源</span></span><br><span class="line">    <span class="type">DBs</span>.closeAll()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">//插入数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(employers: <span class="type">List</span>[<span class="type">Employer</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//事物插入</span></span><br><span class="line">    <span class="type">DB</span>.localTx &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      		<span class="keyword">for</span> (employer &lt;- employers) &#123;</span><br><span class="line">        		<span class="type">SQL</span>(<span class="string">"insert into wsktest(name,age,salary) values(?,?,?)"</span>)</span><br><span class="line">          		.bind(employer.name, employer.age, employer.salary)</span><br><span class="line">          		.update()	<span class="comment">//更新操作</span></span><br><span class="line">                .apply()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//查询操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(): <span class="type">List</span>[<span class="type">Employer</span>] = &#123;</span><br><span class="line">    <span class="type">DB</span>.readOnly &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      		<span class="type">SQL</span>(<span class="string">"select * from wsktest"</span>)</span><br><span class="line">        	.map(rs =&gt; <span class="type">Employer</span>(rs.string(<span class="string">"name"</span>), rs.int(<span class="string">"age"</span>), rs.long(<span class="string">"salary"</span>)))</span><br><span class="line">        	.list()  <span class="comment">//结果转换成list</span></span><br><span class="line">        	.apply() </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//更新操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(age: <span class="type">Int</span>, name: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      		<span class="type">SQL</span>(<span class="string">"update wsktest set age = ? where name = ?"</span>)</span><br><span class="line">        	.bind(age, name)</span><br><span class="line">        	.update()	<span class="comment">//更新操作</span></span><br><span class="line">        	.apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//根据条件删除</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteByname</span></span>(name: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      		<span class="type">SQL</span>(<span class="string">"delete from wsktest where name = ?"</span>)</span><br><span class="line">        	.bind(name)	<span class="comment">//更新操作</span></span><br><span class="line">        	.update()</span><br><span class="line">        	.apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//删除所有</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteAll</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      		<span class="type">SQL</span>(<span class="string">"delete from wsktest "</span>)</span><br><span class="line">        	.update()	<span class="comment">//更新操作</span></span><br><span class="line">        	.apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>ScalikeJDBC</tag>
      </tags>
  </entry>
  <entry>
    <title>RowKey设计原则、热点问题、设计实践</title>
    <url>/2020/06/30/RowKey%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E3%80%81RowKey%E7%83%AD%E7%82%B9%E9%97%AE%E9%A2%98%E3%80%81RowKey%E8%AE%BE%E8%AE%A1%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>HBase由于其存储和读写的高性能，在OLAP即时分析中越来越发挥重要的作用。</p>
<p>作为Nosql数据库的一员，HBase查询只能通过其Rowkey来查询(Rowkey用来表示唯一一行记录)，Rowkey设计的优劣直接影响读写性能。Rowkey的ASCII字典顺序进行全局排序的，下面举例说明：</p>
<a id="more"></a>
<p>假如有5个Rowkey：”012”, “0”, “123”, “234”, “3”，按ASCII字典排序后的结果为：”0”, “012”, “123”, “234”, “3”。（注：文末附常用ASCII码表）<br>Rowkey排序时会先比对两个Rowkey的第一个字节，如果相同，然后会比对第二个字节，依次类推… 对比到第X个字节时，已经超出了其中一个Rowkey的长度，短的Rowkey排在前面。<br>由于HBase是通过Rowkey查询的，一般Rowkey上都会存一些比较关键的检索信息，我们需要提前想好数据具体需要如何查询，根据查询方式进行数据存储格式的设计，要避免做全表扫描，因为效率特别低。</p>
<hr>
<h4 id="Rowkey设计原则"><a href="#Rowkey设计原则" class="headerlink" title="Rowkey设计原则"></a>Rowkey设计原则</h4><h5 id="Rowkey的唯一原则"><a href="#Rowkey的唯一原则" class="headerlink" title="Rowkey的唯一原则"></a>Rowkey的唯一原则</h5><p>必须在设计上保证其唯一性。由于在HBase中数据存储是Key-Value形式，若HBase中同一表插入相同Rowkey，则原先的数据会被覆盖掉(如果表的version设置为1的话)，所以务必保证Rowkey的唯一性</p>
<h5 id="Rowkey的排序原则"><a href="#Rowkey的排序原则" class="headerlink" title="Rowkey的排序原则"></a>Rowkey的排序原则</h5><p>HBase的Rowkey是按照ASCII有序设计的，我们在设计Rowkey时要充分利用这点。比如视频网站上对影片《泰坦尼克号》的弹幕信息，这个弹幕是按照时间倒排序展示视频里，这个时候我们设计的Rowkey要和时间顺序相关。可以使用”Long.MAX_VALUE - 弹幕发表时间”的 long 值作为 Rowkey 的前缀</p>
<h5 id="Rowkey的散列原则"><a href="#Rowkey的散列原则" class="headerlink" title="Rowkey的散列原则"></a>Rowkey的散列原则</h5><p>我们设计的Rowkey应均匀的分布在各个HBase节点上。拿常见的时间戳举例，假如Rowkey是按系统时间戳的方式递增，Rowkey的第一部分如果是时间戳信息的话将造成所有新数据都在一个RegionServer上堆积的热点现象，也就是通常说的Region热点问题， 热点发生在大量的client直接访问集中在个别RegionServer上（访问可能是读，写或者其他操作），导致单个RegionServer机器自身负载过高，引起性能下降甚至Region不可用，常见的是发生jvm full gc或者显示region too busy异常情况，当然这也会影响同一个RegionServer上的其他Region。</p>
<h5 id="Rowkey的长度原则"><a href="#Rowkey的长度原则" class="headerlink" title="Rowkey的长度原则"></a>Rowkey的长度原则</h5><p>Rowkey长度设计原则：Rowkey是一个二进制，Rowkey的长度被很多开发者建议说设计在10~100个字节，建议是越短越好。<br>原因有两点：<br>其一是HBase的持久化文件HFile是按照KeyValue存储的，如果Rowkey过长比如500个字节，1000万列数据光Rowkey就要占用500*1000万=50亿个字节，将近1G数据，这会极大影响HFile的存储效率<br>其二是MemStore缓存部分数据到内存，如果Rowkey字段过长内存的有效利用率会降低，系统无法缓存更多的数据，这会降低检索效率<br>需要指出的是不仅Rowkey的长度是越短越好，而且列族名、列名等尽量使用短名字，因为HBase属于列式数据库，这些名字都是会写入到HBase的持久化文件HFile中去，过长的Rowkey、列族、列名都会导致整体的存储量成倍增加。</p>
<hr>
<h4 id="Region热点"><a href="#Region热点" class="headerlink" title="Region热点"></a>Region热点</h4><h5 id="Reverse反转"><a href="#Reverse反转" class="headerlink" title="Reverse反转"></a>Reverse反转</h5><p>针对固定长度的Rowkey反转后存储，这样可以使Rowkey中经常改变的部分放在最前面，可以有效的随机Rowkey。<br>反转Rowkey的例子通常以手机举例，可以将手机号反转后的字符串作为Rowkey，这样的就避免了以手机号那样比较固定开头(137x、15x等)导致热点问题，<br>这样做的缺点是牺牲了Rowkey的有序性。</p>
<h5 id="Salt加盐"><a href="#Salt加盐" class="headerlink" title="Salt加盐"></a>Salt加盐</h5><p>Salting是将每一个Rowkey加一个前缀，前缀使用一些随机字符，使得数据分散在多个不同的Region，达到Region负载均衡的目标。<br>比如在一个有4个Region(注：以 [ ,a)、[a,b)、[b,c)、[c, )为Region起至)的HBase表中，<br>加Salt前的Rowkey：abc001、abc002、abc003<br>我们分别加上a、b、c前缀，加Salt后Rowkey为：a-abc001、b-abc002、c-abc003<br>可以看到，加盐前的Rowkey默认会在第2个region中，加盐后的Rowkey数据会分布在3个region中，理论上处理后的吞吐量应是之前的3倍。由于前缀是随机的，读这些数据时需要耗费更多的时间，所以Salt增加了写操作的吞吐量，不过缺点是同时增加了读操作的开销。</p>
<h5 id="Hash散列或者Mod"><a href="#Hash散列或者Mod" class="headerlink" title="Hash散列或者Mod"></a>Hash散列或者Mod</h5><p>用Hash散列来替代随机Salt前缀的好处是能让一个给定的行有相同的前缀，这在分散了Region负载的同时，使读操作也能够推断。确定性Hash(比如md5后取前4位做前缀)能让客户端重建完整的RowKey，可以使用get操作直接get想要的行。<br>例如将上述的原始Rowkey经过hash处理，此处我们采用md5散列算法取前4位做前缀，结果如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">9</span>bf0-abc001 （abc001在md5后是<span class="number">9</span>bf049097142c168c38a94c626eddf3d，取前<span class="number">4</span>位是<span class="number">9</span>bf0）</span><br><span class="line"><span class="number">7006</span>-abc002</span><br><span class="line"><span class="number">95e6</span>-abc003</span><br></pre></td></tr></table></figure>
<p>若以前4个字符作为不同分区的起止，上面几个Rowkey数据会分布在3个region中。实际应用场景是当数据量越来越大的时候，这种设计会使得分区之间更加均衡。<br>如果Rowkey是数字类型的，也可以考虑Mod方法。</p>
<hr>
<h4 id="Rowkey设计实践"><a href="#Rowkey设计实践" class="headerlink" title="Rowkey设计实践"></a>Rowkey设计实践</h4><p>在实际的设计中我们可能更多的是结合多种设计方法来实现Rowkey的最优化设计，比如设计订单状态表时使用：Rowkey: reverse(order_id) + (Long.MAX_VALUE – timestamp)，这样设计的好处一是通过reverse订单号避免Region热点，二是可以按时间倒排显示。<br>结合使用HBase作为事件(事件指的的终端在APP中发生的行为，比如登录、下单等等统称事件(event))的临时存储(HBase只存储了最近10分钟的热数据)来举例：<br>设计event事件的Rowkey为：两位随机数Salt + eventId + Date + kafka的Offset<br>这样设计的好处是：<br>设计加盐的目的是为了增加查询的并发性，假如Salt的范围是0~n，那我们在查询的时候，可以将数据分为n个split同时做scan操作。经过我们的多次测试验证，增加并发度能够将整体的查询速度提升5～20倍以上。随后的eventId和Date是用来做范围Scan使用的。在我们的查询场景中，大部分都是指定了eventId的，因此我们把eventId放在了第二个位置上，同时呢，eventId的取值有几十个，通过Salt + eventId的方式可以保证不会形成热点。在单机部署版本中，HBase会存储所有的event数据，所以我们把date放在rowkey的第三个位置上以实现按date做scan，批量Scan性能甚至可以做到毫秒级返回。<br>这样的rowkey设计能够很好的支持如下几个查询场景：</p>
<ol>
<li>全表scan: 在这种情况下，我们仍然可以将全表数据切分成n份并发查询，从而实现查询的实时响应。</li>
<li>只按照event_id查询</li>
<li>按照event_id和date查询</li>
</ol>
<p>此外使用HBase做用户画像的标签存储方案，存储每个app的用户的人口学属性和商业属性等标签信息，由于其设计的更为复杂，后续会另起篇幅详细展开。<br>最后我们顺带提下HBase的表设计，HBase表设计通常可以是宽表（wide table）模式，即一行包括很多列。同样的信息也可以用高表（tall table）形式存储，通常高表的性能比宽表要高出 50%以上，所以推荐大家使用高表来完成表设计。表设计时，我们也应该要考虑HBase数据库的一些特性：</p>
<ol>
<li>在HBase表中是通过Rowkey的字典序来进行数据排序的</li>
<li>所有存储在HBase表中的数据都是二进制的字节</li>
<li>原子性只在行内保证，HBase不支持跨行事务</li>
<li>列族(Column Family)在表创建之前就要定义好</li>
<li>列族中的列标识(Column Qualifier)可以在表创建完以后动态插入数据时添加</li>
</ol>
<hr>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>在做Rowkey设计时，请先考虑业务是读比写多、还是读比写少，HBase本身是为写优化的，即便是这样，也可能会出现热点问题，而如果我们读比较多的话，除了考虑以上Rowkey设计原则外，还可以考虑HBase的Coprocessor甚至elastic search结合的方法，无论哪种方式，都建议做实际业务场景下数据的压力测试以得到最优结果。</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala 集合 List</title>
    <url>/2018/05/02/Scala_%E9%9B%86%E5%90%88_List/</url>
    <content><![CDATA[<h3 id="Scala-集合"><a href="#Scala-集合" class="headerlink" title="Scala 集合"></a>Scala 集合</h3><p>Scala集合部分包括List Set Tuple Map<br>集合的操作需要掌握增、删、迭代的方法<br>该部分为集合中List的说明</p>
<a id="more"></a>
<hr>
<h4 id="List"><a href="#List" class="headerlink" title="List"></a>List</h4><p>Scala List 可以有重复值</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> scala.collection.mutable.<span class="type">Queue</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> q1 = <span class="keyword">new</span> <span class="type">Queue</span>[<span class="type">Int</span>]</span><br></pre></td></tr></table></figure>
<h5 id="放入数据："><a href="#放入数据：" class="headerlink" title="放入数据："></a>放入数据：</h5><h6 id="单个数据"><a href="#单个数据" class="headerlink" title="单个数据"></a>单个数据</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; q1 += <span class="number">10</span></span><br><span class="line">res0: q1<span class="class">.<span class="keyword">type</span> </span>= <span class="type">Queue</span>(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h6 id="集合数据"><a href="#集合数据" class="headerlink" title="集合数据"></a>集合数据</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; q1 ++= <span class="type">List</span>(<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>)</span><br><span class="line">res1: q1<span class="class">.<span class="keyword">type</span> </span>= <span class="type">Queue</span>(<span class="number">10</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<h6 id="多个数据"><a href="#多个数据" class="headerlink" title="多个数据"></a>多个数据</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; q1.enqueue(<span class="number">2</span>,<span class="number">6</span>)</span><br><span class="line">scala&gt; scala.collection.mutable.<span class="type">Queue</span>[<span class="type">Int</span>] = <span class="type">Queue</span>(<span class="number">10</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<h5 id="取数据："><a href="#取数据：" class="headerlink" title="取数据："></a>取数据：</h5><h6 id="只读取数据，不删除数据"><a href="#只读取数据，不删除数据" class="headerlink" title="只读取数据，不删除数据"></a>只读取数据，不删除数据</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; q1.head</span><br><span class="line">res5: <span class="type">Int</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<h6 id="读取数据后，删除数据"><a href="#读取数据后，删除数据" class="headerlink" title="读取数据后，删除数据"></a>读取数据后，删除数据</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; q1.dequeue</span><br><span class="line">res6: <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">scala&gt; q1</span><br><span class="line">res7: scala.collection.mutable.<span class="type">Queue</span>[<span class="type">Int</span>] = <span class="type">Queue</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<h6 id="读取最后一位数据"><a href="#读取最后一位数据" class="headerlink" title="读取最后一位数据"></a>读取最后一位数据</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; q1.last</span><br><span class="line">res8: <span class="type">Int</span> = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">scala&gt; q1</span><br><span class="line">res9: scala.collection.mutable.<span class="type">Queue</span>[<span class="type">Int</span>] = <span class="type">Queue</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>

<h6 id="读取第一位以后的所有值"><a href="#读取第一位以后的所有值" class="headerlink" title="读取第一位以后的所有值"></a>读取第一位以后的所有值</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; q1.tail</span><br><span class="line">res10: scala.collection.mutable.<span class="type">Queue</span>[<span class="type">Int</span>] = <span class="type">Queue</span>(<span class="number">4</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; q1</span><br><span class="line">res11: scala.collection.mutable.<span class="type">Queue</span>[<span class="type">Int</span>] = <span class="type">Queue</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; q1.tail.tail</span><br><span class="line">res12: scala.collection.mutable.<span class="type">Queue</span>[<span class="type">Int</span>] = <span class="type">Queue</span>(<span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; q1</span><br><span class="line">res13: scala.collection.mutable.<span class="type">Queue</span>[<span class="type">Int</span>] = <span class="type">Queue</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala 集合 Map</title>
    <url>/2018/05/03/Scala_%E9%9B%86%E5%90%88_Map/</url>
    <content><![CDATA[<h3 id="Scala-集合"><a href="#Scala-集合" class="headerlink" title="Scala 集合"></a>Scala 集合</h3><p>Scala集合部分包括List Set Tuple Map<br>集合的操作需要掌握增、删、迭代的方法<br>该部分为集合中Map的说明</p>
<a id="more"></a>
<hr>
<h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><p>Scala Map</p>
<h5 id="创建不可修改的Map："><a href="#创建不可修改的Map：" class="headerlink" title="创建不可修改的Map："></a>创建不可修改的Map：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = <span class="type">Map</span>(<span class="string">"a"</span>-&gt;<span class="number">10</span>,<span class="string">"b"</span>-&gt;<span class="number">20</span>)</span><br><span class="line">a: scala.collection.immutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>] = <span class="type">Map</span>(a -&gt; <span class="number">10</span>, b -&gt; <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<h6 id="取值方法："><a href="#取值方法：" class="headerlink" title="取值方法："></a>取值方法：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; a(<span class="string">"a"</span>)</span><br><span class="line">res1: <span class="type">Int</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<h5 id="创建可修改的Map："><a href="#创建可修改的Map：" class="headerlink" title="创建可修改的Map："></a>创建可修改的Map：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> b = scala.collection.mutable.<span class="type">Map</span>(<span class="string">"a"</span>-&gt;<span class="number">10</span>,<span class="string">"b"</span>-&gt;<span class="number">20</span>)</span><br><span class="line">b: scala.collection.mutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>] = <span class="type">Map</span>(b -&gt; <span class="number">20</span>, a -&gt; <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h6 id="修改方法："><a href="#修改方法：" class="headerlink" title="修改方法："></a>修改方法：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; b(<span class="string">"a"</span>) = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; b</span><br><span class="line">res3: scala.collection.mutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>] = <span class="type">Map</span>(b -&gt; <span class="number">20</span>, a -&gt; <span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<h5 id="为了避免key不存在报错，使用以下方法获取value值："><a href="#为了避免key不存在报错，使用以下方法获取value值：" class="headerlink" title="为了避免key不存在报错，使用以下方法获取value值："></a>为了避免key不存在报错，使用以下方法获取value值：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; b.getOrElse(<span class="string">"c"</span>,<span class="number">30</span>)</span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">30</span></span><br></pre></td></tr></table></figure>
<h5 id="增加key-value："><a href="#增加key-value：" class="headerlink" title="增加key-value："></a>增加key-value：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; b += (<span class="string">"d"</span>-&gt;<span class="number">40</span>)</span><br><span class="line">res1: b<span class="class">.<span class="keyword">type</span> </span>= <span class="type">Map</span>(b -&gt; <span class="number">20</span>, d -&gt; <span class="number">40</span>, a -&gt; <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h5 id="减少key-value-只需要减少key就可以了："><a href="#减少key-value-只需要减少key就可以了：" class="headerlink" title="减少key-value 只需要减少key就可以了："></a>减少key-value 只需要减少key就可以了：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; b -= <span class="string">"b"</span></span><br><span class="line">res5: b<span class="class">.<span class="keyword">type</span> </span>= <span class="type">Map</span>(d -&gt; <span class="number">40</span>, a -&gt; <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h5 id="确认key是否存在："><a href="#确认key是否存在：" class="headerlink" title="确认key是否存在："></a>确认key是否存在：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; b.contains(<span class="string">"a"</span>)</span><br><span class="line">res7: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h5 id="Map的遍历："><a href="#Map的遍历：" class="headerlink" title="Map的遍历："></a>Map的遍历：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> c = scala.collection.mutable.<span class="type">Map</span>(<span class="string">"a"</span>-&gt;<span class="number">10</span>,<span class="string">"b"</span>-&gt;<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<h6 id="遍历value1："><a href="#遍历value1：" class="headerlink" title="遍历value1："></a>遍历value1：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ((k, v) &lt;- c)&#123;</span><br><span class="line">  println(k + <span class="string">"--&gt;"</span> + v)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">b--&gt;<span class="number">20</span></span><br><span class="line">a--&gt;<span class="number">10</span></span><br></pre></td></tr></table></figure>
<h6 id="遍历value2："><a href="#遍历value2：" class="headerlink" title="遍历value2："></a>遍历value2：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (k &lt;- c.keySet)&#123;</span><br><span class="line">  println(k + <span class="string">"--&gt;"</span> + c.get(k))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="结果：-1"><a href="#结果：-1" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">b--&gt;<span class="type">Some</span>(<span class="number">20</span>)</span><br><span class="line">a--&gt;<span class="type">Some</span>(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h6 id="获取所有key："><a href="#获取所有key：" class="headerlink" title="获取所有key："></a>获取所有key：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (v &lt;- c.values) &#123;</span><br><span class="line">  println(v)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="结果：-2"><a href="#结果：-2" class="headerlink" title="结果："></a>结果：</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>
<h5 id="some的解释："><a href="#some的解释：" class="headerlink" title="some的解释："></a>some的解释：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = <span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="number">2</span>)</span><br><span class="line">a: scala.collection.immutable.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">Int</span>] = <span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; a.get(<span class="number">1</span>)</span><br><span class="line">res10: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">Some</span>(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; a.get(<span class="number">2</span>)</span><br><span class="line">res11: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.get(<span class="number">1</span>).get</span><br><span class="line">res12: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.get(<span class="number">2</span>).getOrElse(<span class="number">99</span>)</span><br><span class="line">res2: <span class="type">Int</span> = <span class="number">99</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala 集合 Set</title>
    <url>/2018/05/04/Scala_%E9%9B%86%E5%90%88_Set/</url>
    <content><![CDATA[<h3 id="Scala-集合"><a href="#Scala-集合" class="headerlink" title="Scala 集合"></a>Scala 集合</h3><p>Scala集合部分包括List Set Tuple Map<br>集合的操作需要掌握增、删、迭代的方法<br>该部分为集合中Set的说明</p>
<a id="more"></a>
<hr>
<h4 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h4><p>Scala Set 数据不会重复</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> scala.collection.mutable.<span class="type">Set</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Set</span></span><br><span class="line"><span class="comment">//导包了是可mutable（可变的）</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> s = <span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">s: scala.collection.mutable.<span class="type">Set</span>[<span class="type">Int</span>] = <span class="type">Set</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h5 id="增加一个值，成功了返回值为true："><a href="#增加一个值，成功了返回值为true：" class="headerlink" title="增加一个值，成功了返回值为true："></a>增加一个值，成功了返回值为true：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; s.add(<span class="number">10</span>)</span><br><span class="line">res14: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h5 id="多种增加值的方法："><a href="#多种增加值的方法：" class="headerlink" title="多种增加值的方法："></a>多种增加值的方法：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; s += <span class="number">11</span></span><br><span class="line">res15: s<span class="class">.<span class="keyword">type</span> </span>= <span class="type">Set</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; s += (<span class="number">12</span>)</span><br><span class="line">res16: s<span class="class">.<span class="keyword">type</span> </span>= <span class="type">Set</span>(<span class="number">12</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; s += <span class="number">12</span></span><br><span class="line">res17: s<span class="class">.<span class="keyword">type</span> </span>= <span class="type">Set</span>(<span class="number">12</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; s.add(<span class="number">12</span>)</span><br><span class="line">res18: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="comment">//此处因为12存在，所以不能增加了</span></span><br></pre></td></tr></table></figure>
<h5 id="去掉一个值"><a href="#去掉一个值" class="headerlink" title="去掉一个值"></a>去掉一个值</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; s -= <span class="number">12</span></span><br><span class="line">res19: s<span class="class">.<span class="keyword">type</span> </span>= <span class="type">Set</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; s.remove(<span class="number">10</span>)</span><br><span class="line">res20: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">scala&gt; s</span><br><span class="line">res21: scala.collection.mutable.<span class="type">Set</span>[<span class="type">Int</span>] = <span class="type">Set</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">11</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark DRR转DF的两种方式</title>
    <url>/2018/10/16/Spark%20DRR%E8%BD%ACDF%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<p>Spark DRR转DF的两种方式分为编程方式和反射方式</p>
<a id="more"></a>

<p>序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。</p>
<p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。</p>
<h3 id="编程方式将RDD转成DF"><a href="#编程方式将RDD转成DF" class="headerlink" title="编程方式将RDD转成DF"></a>编程方式将RDD转成DF</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">programmatically</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// RDD=&gt;DF时需要的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd = spark.sparkContext.textFile(<span class="string">"spark-sql/data/info.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// STEP1: RDD[String] ==&gt; RDD[Row]</span></span><br><span class="line">    <span class="keyword">val</span> infoRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> splits = x.split(<span class="string">","</span>)</span><br><span class="line">      <span class="keyword">val</span> id = splits(<span class="number">0</span>).trim.toInt</span><br><span class="line">      <span class="keyword">val</span> name = splits(<span class="number">1</span>).trim</span><br><span class="line">      <span class="keyword">val</span> age = splits(<span class="number">2</span>).trim.toInt</span><br><span class="line">      <span class="type">Row</span>(id, name, age)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// STEP2: schema</span></span><br><span class="line">    <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>) ::</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">false</span>) ::</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">false</span>) :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// STEP3: createDataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(infoRDD, schema)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="反射方式将RDD转成DF"><a href="#反射方式将RDD转成DF" class="headerlink" title="反射方式将RDD转成DF"></a>反射方式将RDD转成DF</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reflection</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// RDD=&gt;DF时需要的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd = spark.sparkContext.textFile(<span class="string">"spark-sql/data/info.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// RDD[String] =&gt; case class</span></span><br><span class="line">    <span class="keyword">val</span> infoDF = rdd.map(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> splits = x.split(<span class="string">","</span>)</span><br><span class="line">      <span class="keyword">val</span> id = splits(<span class="number">0</span>).trim.toInt</span><br><span class="line">      <span class="keyword">val</span> name = splits(<span class="number">1</span>).trim</span><br><span class="line">      <span class="keyword">val</span> age = splits(<span class="number">2</span>).trim.toInt</span><br><span class="line">      <span class="type">Info</span>(id, name, age)</span><br><span class="line">    &#125;).toDF() <span class="comment">// 最终转成DF</span></span><br><span class="line"></span><br><span class="line">    infoDF.printSchema()</span><br><span class="line">    infoDF.show()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Info</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala 集合 Tuple</title>
    <url>/2018/05/05/Scala_%E9%9B%86%E5%90%88_Tuple/</url>
    <content><![CDATA[<h3 id="Scala-集合"><a href="#Scala-集合" class="headerlink" title="Scala 集合"></a>Scala 集合</h3><p>Scala集合部分包括List Set Tuple Map<br>集合的操作需要掌握增、删、迭代的方法<br>该部分为集合中Tuple的说明</p>
<a id="more"></a>
<hr>
<h4 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h4><p>Scala Tuple 能放很多你想放的数据（不管数据类型）</p>
<h5 id="创建Tuple："><a href="#创建Tuple：" class="headerlink" title="创建Tuple："></a>创建Tuple：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">a: (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>) = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//会使用以下这种方式就可以了</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="string">"a"</span>)</span><br><span class="line">b: (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">String</span>) = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,a)</span><br></pre></td></tr></table></figure>
<h5 id="取值方法1：（下标从1开始）"><a href="#取值方法1：（下标从1开始）" class="headerlink" title="取值方法1：（下标从1开始）"></a>取值方法1：（下标从1开始）</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; b._1</span><br><span class="line">res22: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; b._4</span><br><span class="line">res23: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">cala&gt; <span class="keyword">val</span> t = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>,<span class="number">21</span>,<span class="number">22</span>)</span><br><span class="line">t: (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>) = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>,<span class="number">21</span>,<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> t = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>,<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>)</span><br><span class="line">&lt;console&gt;:<span class="number">1</span>: error: too many elements <span class="keyword">for</span> tuple: <span class="number">23</span>, allowed: <span class="number">22</span></span><br><span class="line"><span class="keyword">val</span> t = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>,<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>)</span><br><span class="line"><span class="comment">//*tuple 最多只能放22个值</span></span><br></pre></td></tr></table></figure>
<h5 id="取值方法2："><a href="#取值方法2：" class="headerlink" title="取值方法2："></a>取值方法2：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> t = ((<span class="string">"a"</span>,<span class="number">10</span>),<span class="number">20</span>,<span class="number">30</span>)</span><br><span class="line">t: ((<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Int</span>, <span class="type">Int</span>) = ((a,<span class="number">10</span>),<span class="number">20</span>,<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> t = ((<span class="string">"a"</span>,<span class="number">10</span>),<span class="number">20</span>,<span class="number">30</span>)</span><br><span class="line">println(t._1._1 + <span class="string">"\t"</span> + t._1._2 + <span class="string">"\t"</span> + t._2 + <span class="string">"\t"</span> + t._3)</span><br><span class="line">结果：</span><br><span class="line">a	<span class="number">10</span>	<span class="number">20</span>	<span class="number">30</span></span><br></pre></td></tr></table></figure>
<h5 id="以下的用法没太多用处，都是奇怪的用法："><a href="#以下的用法没太多用处，都是奇怪的用法：" class="headerlink" title="以下的用法没太多用处，都是奇怪的用法："></a>以下的用法没太多用处，都是奇怪的用法：</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//限制值数量的tuple</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> t = <span class="keyword">new</span> <span class="type">Tuple3</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="number">3</span>)</span><br><span class="line">t: (<span class="type">String</span>, <span class="type">String</span>, <span class="type">Int</span>) = (a,b,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> t0 = <span class="number">0</span> -&gt; <span class="string">"zero"</span></span><br><span class="line">t0: (<span class="type">Int</span>, <span class="type">String</span>) = (<span class="number">0</span>,zero)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> t1 = <span class="number">1</span> -&gt; <span class="string">"one"</span></span><br><span class="line">t1: (<span class="type">Int</span>, <span class="type">String</span>) = (<span class="number">1</span>,one)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> t2 = <span class="number">2</span> -&gt; <span class="string">"two"</span> -&gt; <span class="string">"second"</span></span><br><span class="line">t2: ((<span class="type">Int</span>, <span class="type">String</span>), <span class="type">String</span>) = ((<span class="number">2</span>,two),second)</span><br></pre></td></tr></table></figure>
<h5 id="tuple遍历"><a href="#tuple遍历" class="headerlink" title="tuple遍历"></a>tuple遍历</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> b = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="string">"a"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until b.productArity)&#123;</span><br><span class="line">  println(b.productElement(i))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h6><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<h5 id="Tuple2-对偶（调换位置）"><a href="#Tuple2-对偶（调换位置）" class="headerlink" title="Tuple2 对偶（调换位置）"></a>Tuple2 对偶（调换位置）</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> t = (<span class="string">"a"</span>,<span class="string">"b"</span>)</span><br><span class="line">t: (<span class="type">String</span>, <span class="type">String</span>) = (a,b)</span><br><span class="line"></span><br><span class="line">scala&gt; t.swap</span><br><span class="line">res0: (<span class="type">String</span>, <span class="type">String</span>) = (b,a)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Transformations、Action</title>
    <url>/2018/09/16/Spark%20Transformations%E3%80%81Action/</url>
    <content><![CDATA[<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li>Transformations</li>
<li>Action</li>
</ol>
<a id="more"></a>
<hr>
<h4 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h4><p>Transformations的特点是lazy的，和Scala中的lazy该念一致——延迟/懒加载，<br>不会立刻执行，只有等待遇到第一个action才会去提交作业到Spark上。</p>
<h5 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h5><p><strong>map</strong> 作用到每一个元素<br>输入:任意类型的函数，输出:泛型U类型的函数，返回RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<p><strong>mapPartitions</strong> 作用到每一个分区<br>输入:一个可迭代的类型T，输出:一个可迭代的类型U，返回RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<p><strong>mapPartitionsWithIndex</strong> 作用到每一个分区并打印分区数<br>输入:分区索引，可迭代的类型T，输出:可迭代的类型U，返回RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<p><strong>glom()</strong> 按分区返回数组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.glom().collect().foreach(f=&gt;f.foreach(x =&gt; println(_)))</span><br></pre></td></tr></table></figure>

<p><strong>filter()</strong> 过滤<br>输入:输入一个函数T，输出:一个布尔值，返回一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p><strong>sample()</strong> 取样<br>输入:是否放回的布尔值，抽出来的概率，返回一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">      withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">      fraction: <span class="type">Double</span>,</span><br><span class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p><strong>distinct(x)</strong> 去重 ==&gt; numPartitions可指定分区<br>输入的必须是RDD，返回的也是一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dist: <span class="type">RDD</span>[<span class="type">Int</span>] = listRDD3.distinct()</span><br></pre></td></tr></table></figure>

<p><strong>coalesce(x)</strong> 重点(小文件相关场景大量使用): ==&gt; reduce数量决定最终输出的文件数，coalesce的作用是减少到指定分区数(x)，减少分区是窄依赖<br>==&gt; Spark作业遇到shuffle 会切分stage<br>输入一个分区数，返回一个重分区后的RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">               partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">              (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>
<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD2.coalesce(<span class="number">1</span>).getNumPartitions</span><br></pre></td></tr></table></figure>

<h5 id="双value算子"><a href="#双value算子" class="headerlink" title="双value算子"></a>双value算子</h5><p><strong>zip()</strong> 拉链 ==&gt; 不同分区和不同元素都不能用</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zip</span></span>[<span class="type">A1</span> &gt;: <span class="type">A</span>, <span class="type">B</span>, <span class="type">That</span>](that: <span class="type">GenIterable</span>[<span class="type">B</span>])(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, (<span class="type">A1</span>, <span class="type">B</span>), <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure>

<p><strong>zipWithIndex()</strong> 打印拉链所在的分区</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zipWithIndex</span></span>[<span class="type">A1</span> &gt;: <span class="type">A</span>, <span class="type">That</span>](<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, (<span class="type">A1</span>, <span class="type">Int</span>), <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> name = <span class="type">List</span>(<span class="string">"张三"</span>, <span class="string">"李四"</span>, <span class="string">"王五"</span>)</span><br><span class="line"><span class="keyword">val</span> age = <span class="type">List</span>(<span class="number">19</span>, <span class="number">26</span>, <span class="number">38</span>)</span><br><span class="line"><span class="keyword">val</span> zipRDD: <span class="type">List</span>[((<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Int</span>)] = name.zip(age).zipWithIndex</span><br></pre></td></tr></table></figure>

<p><strong>union()</strong> 并集 ==&gt; 分区数相加</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">union</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>, <span class="type">That</span>](that: <span class="type">GenSeq</span>[<span class="type">B</span>])(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, <span class="type">B</span>, <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">val</span> ints: <span class="type">List</span>[<span class="type">Int</span>] = list1.union(list2)</span><br></pre></td></tr></table></figure>

<p><strong>intersection()</strong> 交集</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>](that: <span class="type">GenSeq</span>[<span class="type">B</span>]): <span class="type">Repr</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> inter: <span class="type">List</span>[<span class="type">Int</span>] = list1.intersect(list2)</span><br></pre></td></tr></table></figure>

<p><strong>subtract()</strong> 差集<br>输入的必须是RDD，返回的也是一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sub: <span class="type">RDD</span>[<span class="type">Int</span>] = listRDD2.subtract(listRDD3)</span><br></pre></td></tr></table></figure>

<p><strong>cartesian()</strong> 笛卡尔积<br>输入的必须是RDD，返回的也是一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cartesian</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> car = listRDD2.cartesian(listRDD3)</span><br></pre></td></tr></table></figure>

<h5 id="kv算子"><a href="#kv算子" class="headerlink" title="kv算子"></a>kv算子</h5><p><strong>mapValues</strong> 得到所有ky的函数<br>输入:一个函数V，输出:一个值U，返回key为K，value为U的键函数对RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapValues</span></span>[<span class="type">U</span>](f: <span class="type">V</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.groupByKey().mapValues(_.sum).print()</span><br></pre></td></tr></table></figure>

<p><strong>sortBy(x)</strong> 降序指定-x，指定任意参数<br>输入键值对，指定排序的值，默认升序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">      f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">      ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">      numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">      (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p><strong>sortByKey(true|false)</strong> 只能根据key排序<br>默认升序为true，可指定降序为false</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.map(x=&gt;(x._2,x._1)).sortByKey(<span class="literal">false</span>).map(x=&gt;(x._2,x._1)).print()</span><br></pre></td></tr></table></figure>

<p><strong>groupByKey</strong> 返回的kv对中的函数可迭代<br>==&gt;每个数据都经过shuffle，到reduce聚合，数据量大</p>
<p>可指定分区数，返回一个PariRDD，包含一个Key和一个可迭代的Value</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.groupByKey().mapValues(_.sum).print()</span><br></pre></td></tr></table></figure>

<p><strong>reduceByKey()</strong> 对value做指定的操作，直接返回函数<br>==&gt;map端有Combiner先进行了一次预聚合操作，减少了网络IO传输的数据量，所以比groupByKey快<br>==&gt;groupByKey的shuffle数据量明显多于reduceByKey，所以建议使用reduceByKey</p>
<p>输入两个值，输出一个值，返回一个PariRDD，包含一个明确的Key和一个明确的Value</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>

<p><strong>join()</strong><br>两个RDDjoin，返回一个PariRDD包含一个key，两个Value</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> mapRDD2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zhaoliu"</span>, <span class="number">18</span>), (<span class="string">"zhangsan"</span>, <span class="number">22</span>), (<span class="string">"list"</span>, <span class="number">21</span>), (<span class="string">"wangwu"</span>, <span class="number">26</span>)))</span><br><span class="line"><span class="keyword">val</span> mapRDD3 = sc.parallelize(<span class="type">List</span>((<span class="string">"hongqi"</span>, <span class="string">"男"</span>), (<span class="string">"zhangsan"</span>, <span class="string">"男"</span>), (<span class="string">"list"</span>, <span class="string">"女"</span>), (<span class="string">"wangwu"</span>, <span class="string">"男"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br></pre></td></tr></table></figure>

<p><strong>leftOuterJoin</strong><br>两个RDDjoin，返回一个PariRDD包含一个key，一个确定的左表Value值，一个Option类型的右表Value值，即可能为空</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure>

<p><strong>rightOuterJoin</strong><br>两个RDDjoin，返回一个PariRDD包含一个key，一个确定的右表Value值，一个Option类型的左表Value值，即可能为空</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">W</span>))]</span><br></pre></td></tr></table></figure>

<p><strong>fullOuterJoin</strong><br>两个RDDjoin，返回一个PariRDD包含一个key，一个Option类型的右表Value值，一个Option类型的左表Value值，即都可能为空</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fullOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure>

<p><strong>cogroup</strong><br>作用和join类似，不同的是返回的结果是可迭代的，而join返回的是值，原因是join底层调用了cogroup</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure>

<p>面试题：Spark Core 不使用distinct去重</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> listRDD3 = sc.parallelize(<span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">listRDD3.map(x=&gt;(x,<span class="literal">null</span>)).reduceByKey((x,y)=&gt;x).map(_._1).print()</span><br></pre></td></tr></table></figure>


<h4 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h4><p><strong>first()</strong><br>返回第一个元素，等于take(1)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p><strong>take()</strong><br>拿出指定的前N个元素,返回一个数组，结果为原始顺序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p><strong>count()</strong><br>返回元素数量，是个Long型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>

<p><strong>sum</strong><br>求和，返回一个Double型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(): <span class="type">Double</span></span><br></pre></td></tr></table></figure>

<p><strong>max</strong><br>返回最大值，结果通过隐式转换排序过</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p><strong>min</strong><br>返回最小值，结果通过隐式转换排序过</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p><strong>top()</strong><br>先排降序再返回前N个元素组成的数组，字典序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>案例：升序排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.top(<span class="number">3</span>)(<span class="type">Ordering</span>.by(x =&gt; -x)).foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>takeOrdered</strong><br>先排降序再返回N个元素组成的数组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>案例：升序排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.takeOrdered(<span class="number">3</span>)(<span class="type">Ordering</span>.by(x =&gt; x)).foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>reduce</strong><br>聚合，输入两个元素输出一个元素，类型相同</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p><strong>foreach</strong><br>循环输出</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure>

<p><strong>foreachPartition</strong><br>分区循环输出<br>输入的是一个可迭代的类型T，输出Unit</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.foreachPartition(x=&gt;x.foreach(println))</span><br></pre></td></tr></table></figure>

<p><strong>countByKey</strong><br>根据key统计个数，用作检测数据倾斜</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>]</span><br></pre></td></tr></table></figure>

<p><strong>lookup</strong><br>根据map中的键来取出相应的值的，</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(key: <span class="type">K</span>): <span class="type">Seq</span>[<span class="type">V</span>]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.lookup(<span class="string">"zhangsan"</span>).foreach(println)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark算子</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 监控模块</title>
    <url>/2019/03/12/Spark%20%E7%9B%91%E6%8E%A7%E6%A8%A1%E5%9D%97/</url>
    <content><![CDATA[<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li>Spark自带监控</li>
<li>Spark接口监控</li>
<li>Spark自定义监控</li>
</ol>
<a id="more"></a>
<hr>
<h4 id="Spark自带监控"><a href="#Spark自带监控" class="headerlink" title="Spark自带监控"></a>Spark自带监控</h4><p><a href="http://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options</a></p>
<p>第一种监控方式是Spark自带的，由于Spark Web UI界面只在sc的生命周期内有效，所以我们需要存储日志，在Spark sc 生命周期结束后重构UI界面。</p>
<p>首先看官方文档配置，这里只是简单配置</p>
<ol>
<li><p>修改spark.default.xml</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">#开启日志存储</span><br><span class="line">spark.eventLog.enabled <span class="literal">true</span></span><br><span class="line">#指定日志存储的<span class="type">HDFS</span>目录</span><br><span class="line">spark.eventLog.dir hdfs:<span class="comment">//fushuaidate:9000/spark-logs</span></span><br><span class="line">#开启日志存储<span class="number">7</span>天自动删除</span><br><span class="line">spark.history.fs.cleaner.enabled <span class="literal">true</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改spark.env.xml</p>
</li>
<li><p>在 sc 的生命周期外打开历史UI界面</p>
</li>
</ol>
<h4 id="Spark接口监控"><a href="#Spark接口监控" class="headerlink" title="Spark接口监控"></a>Spark接口监控</h4><p><a href="http://spark.apache.org/docs/latest/monitoring.html#rest-api" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/monitoring.html#rest-api</a></p>
<p>metrics: 数据信息</p>
<p>spark 提供了一系列整个任务生命周期中各个阶段变化的事件监听机制，通过这一机制可以在任务的各个阶段做一些自定义的各种动作。SparkListener便是这些阶段的事件监听接口类 通过实现这个类中的各种方法便可实现自定义的事件处理动作。</p>
<p>自定义监听sparListener后的注册方式有两种：<br>方法1：conf 配置中指定</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">setAppName(getClass.getSimpleName)</span><br><span class="line">setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">getOrCreate()</span><br></pre></td></tr></table></figure>
<p>方法2：sparkContext 类中指定</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.addSparkListener(<span class="keyword">new</span> <span class="type">MySparkAppListener</span>)</span><br></pre></td></tr></table></figure>

<h4 id="SparkListerner"><a href="#SparkListerner" class="headerlink" title="SparkListerner"></a>SparkListerner</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//SparkListener 下各个事件对应的函数名非常直白，即如字面所表达意思。</span></span><br><span class="line"><span class="comment">//想对哪个阶段的事件做一些自定义的动作，变继承SparkListener实现对应的函数即可</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkListener</span> <span class="keyword">extends</span> <span class="title">SparkListenerInterface</span> </span>&#123;</span><br><span class="line">  <span class="comment">//阶段完成时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageCompleted</span></span>(stageCompleted: <span class="type">SparkListenerStageCompleted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//阶段提交时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageSubmitted</span></span>(stageSubmitted: <span class="type">SparkListenerStageSubmitted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务启动时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskStart</span></span>(taskStart: <span class="type">SparkListenerTaskStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//下载任务结果的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskGettingResult</span></span>(taskGettingResult: <span class="type">SparkListenerTaskGettingResult</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobEnd</span></span>(jobEnd: <span class="type">SparkListenerJobEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//环境变量被更新的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEnvironmentUpdate</span></span>(environmentUpdate: <span class="type">SparkListenerEnvironmentUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//块管理被添加的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerAdded</span></span>(blockManagerAdded: <span class="type">SparkListenerBlockManagerAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerRemoved</span></span>(</span><br><span class="line">      blockManagerRemoved: <span class="type">SparkListenerBlockManagerRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//取消rdd缓存的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onUnpersistRDD</span></span>(unpersistRDD: <span class="type">SparkListenerUnpersistRDD</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationStart</span></span>(applicationStart: <span class="type">SparkListenerApplicationStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app结束的事件 [以下各事件也如同函数名所表达各个阶段被触发的事件不在一一标注]</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationEnd</span></span>(applicationEnd: <span class="type">SparkListenerApplicationEnd</span>): <span class="type">Unit</span> = &#123; &#125; </span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorMetricsUpdate</span></span>(</span><br><span class="line">      executorMetricsUpdate: <span class="type">SparkListenerExecutorMetricsUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorAdded</span></span>(executorAdded: <span class="type">SparkListenerExecutorAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorRemoved</span></span>(executorRemoved: <span class="type">SparkListenerExecutorRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorBlacklisted</span></span>(</span><br><span class="line">      executorBlacklisted: <span class="type">SparkListenerExecutorBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorUnblacklisted</span></span>(</span><br><span class="line">      executorUnblacklisted: <span class="type">SparkListenerExecutorUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeBlacklisted</span></span>(</span><br><span class="line">      nodeBlacklisted: <span class="type">SparkListenerNodeBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeUnblacklisted</span></span>(</span><br><span class="line">      nodeUnblacklisted: <span class="type">SparkListenerNodeUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockUpdated</span></span>(blockUpdated: <span class="type">SparkListenerBlockUpdated</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onOtherEvent</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h4><p>首先看官方文档配置，这里只是简单案例</p>
<ol>
<li>自定义监控类，继承SparkListener</li>
<li>重写onTaskEnd方法，拿到taskMetrics</li>
<li>从taskMetrics获取各种数据信息</li>
<li>注册到被监听的类</li>
</ol>
<p>第1-3步代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListener</span>(<span class="params">conf:<span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> metricsObject = <span class="type">Metrics</span>(appName,taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.shuffleReadMetrics.totalBytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line">        <span class="comment">//输出字符串类型的metricsObject</span></span><br><span class="line">        logError(metricsObject.toString)</span><br><span class="line">        <span class="comment">//输出Json类型的metricsObject</span></span><br><span class="line">        logError(<span class="type">Json</span>(<span class="type">DefaultFormats</span>).write(metricsObject))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//定义case class对象</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Metrics</span>(<span class="params">appName:<span class="type">String</span>,stageId:<span class="type">Long</span>,taskId:<span class="type">Long</span>,bytesRead:<span class="type">Long</span>,bytesWritten:<span class="type">Long</span>,shuffleReadMetrics:<span class="type">Long</span>,shuffleWriteMetrics:<span class="type">Long</span></span>)</span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"appName:<span class="subst">$appName</span>,stageId:<span class="subst">$stageId</span>,taskId:<span class="subst">$taskId</span>,bytesRead:<span class="subst">$bytesRead</span>,bytesWritten:<span class="subst">$bytesWritten</span>,shuffleReadMetrics:<span class="subst">$shuffleReadMetrics</span>,shuffleWriteMetrics:<span class="subst">$shuffleWriteMetrics</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>第4步代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//输入、输出路径</span></span><br><span class="line">        <span class="keyword">val</span> (in,out) = (args(<span class="number">0</span>),args(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">//配置conf</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">            .setAppName(getClass.getSimpleName)</span><br><span class="line">            .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">            <span class="comment">//监听类注册</span></span><br><span class="line">            .set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">        <span class="comment">//拿到sc</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//删除输出目录</span></span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(<span class="keyword">new</span> <span class="type">Configuration</span>(),out)</span><br><span class="line">        <span class="comment">//操作算子</span></span><br><span class="line">        <span class="keyword">val</span> result = sc.textFile(in).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//保存文件</span></span><br><span class="line">        result.saveAsTextFile(out)</span><br><span class="line">        <span class="comment">//关闭sc</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>taskEnd.taskInfo.status 该参数决定作业的成功或者失败</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">status</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (running) &#123;</span><br><span class="line">      <span class="keyword">if</span> (gettingResult) &#123;</span><br><span class="line">        <span class="string">"GET RESULT"</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="string">"RUNNING"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (failed) &#123;</span><br><span class="line">      <span class="string">"FAILED"</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (killed) &#123;</span><br><span class="line">      <span class="string">"KILLED"</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (successful) &#123;</span><br><span class="line">      <span class="string">"SUCCESS"</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="string">"UNKNOWN"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>小demo: </p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> status: <span class="type">String</span> = taskEnd.taskInfo.status</span><br><span class="line">    <span class="keyword">if</span>(<span class="string">"SUCCESS"</span>.equals(status))&#123;</span><br><span class="line">        println(<span class="string">"作业完成"</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>注意:<br>如果是大范围的实现某个abstract或者trait，最好加一层中间的代码过度达到解耦，我们实现的是中间的过度接口，与原来的接口隔绝来，这样如果升级版本的时候源码有所改动我们只需要改中间层的继承接口。</p>
<h4 id="Spark自定义监控案例"><a href="#Spark自定义监控案例" class="headerlink" title="Spark自定义监控案例"></a>Spark自定义监控案例</h4><p>把监控参数写入到MySQL</p>
<p>需求：应用程序名字、jobID号、stageID号、taskID号、读取数据量、写入数据量、shuffle读取数据量、shuffle写入数据量。</p>
<ol>
<li><p>建表</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">create table wc2mysql(</span><br><span class="line">	app_name varchar(<span class="number">32</span>),</span><br><span class="line">	job_id bigint,</span><br><span class="line">	stage_id bigInt,</span><br><span class="line">	task_id bigint,</span><br><span class="line">	file_read_byte bigint,</span><br><span class="line">	file_write_byte bigint,</span><br><span class="line">	shuffle_read_byte bigint,</span><br><span class="line">	shuffle_write_byte bigint</span><br><span class="line">);</span><br></pre></td></tr></table></figure></li>
<li><p>实现监控类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListenerV2</span>(<span class="params">conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义JobID</span></span><br><span class="line">    <span class="keyword">var</span> jobId:<span class="type">Long</span> = _</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        jobId = jobStart.jobId</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"============准备插入数据============"</span>)</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> listener = <span class="type">Listener</span>(appName, jobId, taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleReadMetrics.totalBytesRead, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//结果插入到MySQL</span></span><br><span class="line">        <span class="type">ListenerCURD</span>.insert(listener)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//发送监控邮件</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"true"</span> == conf.get(<span class="string">"spark.send.mail.enabled"</span>))&#123;</span><br><span class="line">            <span class="type">MsgUtils</span>.send(<span class="string">"971118017@qq.com"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">"============成功插入数据============"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现监控数据写入MySQL<br>使用的是scalikejdbc框架实现的,<a href="https://emerkfu.github.io/2018/05/13/Scala%20使用ScalikeJDBC操作MySQL/">ScalikeJDBC操作MySQL</a></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Listener</span>(<span class="params">app_name: <span class="type">String</span>, job_id: <span class="type">Long</span>, stage_id: <span class="type">Long</span>, task_id: <span class="type">Long</span>, file_read_byte: <span class="type">Long</span>, file_write_byte: <span class="type">Long</span>, shuffle_read_byte: <span class="type">Long</span>, shuffle_write_byte: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ListenerCURD</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Before</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化配置</span></span><br><span class="line">        <span class="type">DBs</span>.setupAll()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(listener: <span class="type">Listener</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">Before</span>()</span><br><span class="line">        <span class="comment">//事物插入</span></span><br><span class="line">        <span class="type">DB</span>.localTx &#123;</span><br><span class="line">            <span class="keyword">implicit</span> session =&gt; &#123;</span><br><span class="line">                <span class="type">SQL</span>(<span class="string">"insert into wc2mysql(app_name,job_id,stage_id,task_id,file_read_byte,file_write_byte,shuffle_read_byte,shuffle_write_byte) values(?,?,?,?,?,?,?,?)"</span>)</span><br><span class="line">                    .bind(listener.app_name,listener.job_id, listener.stage_id, listener.task_id, listener.file_read_byte, listener.file_write_byte, listener.shuffle_read_byte, listener.shuffle_write_byte)</span><br><span class="line">                    .update()</span><br><span class="line">                    .apply()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">After</span>()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">After</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//关闭资源</span></span><br><span class="line">        <span class="type">DBs</span>.closeAll()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动被监控类<br>被监控的类还是我们上面的WordCount的类，关键在于在SparkConf()中注册</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//配置conf</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListenerV2"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在数据库中查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> wc2mysql;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark监控</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark RDD浅谈</title>
    <url>/2018/09/06/SparkRDD%E6%B5%85%E8%B0%88/</url>
    <content><![CDATA[<p>对于RDD的理解，RDD是做弹性分布式数据集，是Spark的核心概念</p>
<p>它是分布式的，可以分布在多台机器上，进行计算</p>
<p>它是弹性的，计算过程中内存不够时它会和磁盘进行数据交换</p>
<p>它表示已被分区，不可变的并能够被并行操作的数据集合</p>
<p>这些特性，大家都能说出来，觉得很简单，仅看过几篇文章的人，也能有同样的回答，但是对于RDD有独到的理解吗？</p>
<a id="more"></a>
<hr>
<h4 id="对RDD的理解"><a href="#对RDD的理解" class="headerlink" title="对RDD的理解"></a>对RDD的理解</h4><h5 id="官方在源码中对RDD的注释："><a href="#官方在源码中对RDD的注释：" class="headerlink" title="官方在源码中对RDD的注释："></a>官方在源码中对RDD的注释：</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">* Internally, each RDD is characterized by five main properties:</span><br><span class="line">*   </span><br><span class="line">*  - A list of partitions  一个分区的列表</span><br><span class="line">*  - A function <span class="keyword">for</span> computing each split  一个计算每个分区的函数，即RDD的分片函数</span><br><span class="line">*  - A list of dependencies on other RDDs    RDD之间的依赖关系 </span><br><span class="line">*  - Optionally, a Partitioner <span class="keyword">for</span> key-<span class="function">value <span class="title">RDDs</span> <span class="params">(e.g. to say that the RDD is hash-partitioned)</span>    可选：一个Partitioner</span></span><br><span class="line"><span class="function">*  - Optionally, a list of preferred locations to compute each split <span class="title">on</span> <span class="params">(e.g. block locations <span class="keyword">for</span></span></span></span><br><span class="line"><span class="function"><span class="params">*    an HDFS file)</span>    可选：一个列表，存储存取每个Partition的优先位置</span></span><br><span class="line"><span class="function">*</span></span><br><span class="line"><span class="function">* All of the scheduling and execution in Spark is done based on these methods, allowing each RDD</span></span><br><span class="line"><span class="function">* to implement its own way of computing itself. Indeed, users can implement custom <span class="title">RDDs</span> <span class="params">(e.g. <span class="keyword">for</span></span></span></span><br><span class="line"><span class="function"><span class="params">* reading data from a new storage system)</span> by overriding these functions. Please refer to the</span></span><br><span class="line"><span class="function">* [[http:<span class="comment">//www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf Spark paper]] for more details</span></span></span><br><span class="line"><span class="function">* on RDD internals.</span></span><br><span class="line"><span class="function">*/</span></span><br></pre></td></tr></table></figure>
<p>通过官方的注释，我们可以了解到，每个RDD都有五个重要的属性。</p>
<ol>
<li><p>一个分区的列表。我们可以理解为是一组分片，分片就是数据集的基本组成单位，对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。<br>用户创建RDD时，可以指定RDD的分片个数，若没有指定，则采用默认（默认值：程序所分配的CPU Cores数目），下图描述了分区存储的计算模型，每个分配的存储是有BlockManager实现的。<br>每个分区都会被逻辑映射成BlockManager的一个block，而这个block会被一个Task负责计算。<br><img src="/2018/09/06/SparkRDD%E6%B5%85%E8%B0%88/RDD-Partiton.png" alt="RDD Partiton 的存储和计算模型"></p>
</li>
<li><p>一个计算每个分区的函数。即RDD的计算分片compute函数，Spark中的RDD计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复核，不需要保存每次计算的结果。</p>
</li>
<li><p>RDD之间的依赖关系。这里要说一下RDD的依赖关系<br>RDD和它的依赖的parent RDD(s)的关系有两种不同的类型，即宽依赖和窄依赖</p>
</li>
</ol>
<ul>
<li>窄依赖指的是每一个parent RDD的partition最多被子RDD的一个partition使用</li>
<li>宽依赖是指多个子RDD的partition会依赖同一个parent RDD的partition<br>接下来，从不同类型的转换来进一步理解RDD的窄依赖和宽依赖的区别，如下图所示。<br><img src="/2018/09/06/SparkRDD%E6%B5%85%E8%B0%88/20170920031301_87301.png" alt="RDD 宽依赖 窄依赖"><br>对于map和filter形式的转换来说，它们只是将Partition的数据根据转换的规则进行转化，并不涉及其他的处理，可以简单地认为只是将数据从一个形式转换到另一个形式。<br>对于union，只是将多个RDD合并成一个，parent RDD的Partition(s)不会有任何的变化，可以认为只是把parent RDD的Partition(s)简单进行复制与合并。<br>对于join，如果每个Partition仅仅和已知的、特定的Partition进行join，那么这个依赖关系也是窄依赖。对于这种有规则的数据的join，并不会引入昂贵的Shuffle。<br>对于窄依赖，由于RDD每个Partition依赖固定数量的parent RDD(s)的Partition(s)，因此可以通过一个计算任务来处理这些Partition，并且这些Partition相互独立，这些计算任务也就可以并行执行了。<br>对于groupByKey，子RDD的所有Partition(s)会依赖于parent RDD的所有Partition(s)，子RDD的Partition是parent RDD的所有Partition Shuffle的结果，因此这两个RDD是不能通过一个计算任务来完成的。同样，对于需要parent RDD的所有Partition进行join的转换，也是需要Shuffle，这类join的依赖就是宽依赖而不是前面提到的窄依赖了。<br>不同的操作依据其特性，可能会产生不同的依赖。例如map、filter操作会产生 narrow dependency 。reduceBykey操作会产生 wide / shuffle dependency。<br>通俗点来说，RDD的每个Partition，仅仅依赖于父RDD中的一个Partition，这才是窄。 就这么简单！<br>反正，子Rdd的partition和父Rdd的Partition如果是一对一就是窄依赖，这样理解就好区分了 　　　　　　　　　<br>捋一下这里的源码：<br>所有的依赖都要实现<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">trait Dependency[T]：</span><br><span class="line">abstract class Dependency[T] extends Serializable &#123;</span><br><span class="line">    def rdd: RDD[T]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
其中rdd就是依赖的parent RDD。<br>对于窄依赖的实现（有两种）<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123;</span><br><span class="line">    <span class="comment">//返回子RDD的partitionId依赖的所有的parent RDD的Partition(s)</span></span><br><span class="line">    <span class="function">def <span class="title">getParents</span><span class="params">(partitionId: Int)</span>: Seq[Int]</span></span><br><span class="line"><span class="function">    override def rdd: RDD[T] </span>= _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
窄依赖是有两种具体实现，分别如下：<br>一种是一对一的依赖，即OneToOneDependency：<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123;</span><br><span class="line">    <span class="function">override def <span class="title">getParents</span><span class="params">(partitionId: Int)</span> </span>= List(partitionId)</span><br><span class="line">    通过getParents的实现不难看出，RDD仅仅依赖于parent RDD相同ID的Partition。</span><br></pre></td></tr></table></figure>
还有一个是范围的依赖，即RangeDependency，它仅仅被org.apache.spark.rdd.UnionRDD使用。UnionRDD是把多个RDD合成一个RDD，这些RDD是被拼接而成，即每个parent RDD的Partition的相对顺序不会变，只不过每个parent RDD在UnionRDD中的Partition的起始位置不同。因此它的getPartents如下：<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">override def <span class="title">getParents</span><span class="params">(partitionId: Int)</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">if</span>(partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">       List(partitionId - outStart + inStart)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       Nil</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
其中，inStart是parent RDD中Partition的起始位置，outStart是在UnionRDD中的起始位置，length就是parent RDD中Partition的数量。<br>对于宽依赖的实现（只有一种）<br>宽依赖的实现只有一种：ShuffleDependency。子RDD依赖于parent RDD的所有Partition，因此需要Shuffle过程：<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">class ShuffleDependency[K, V, C](</span><br><span class="line">    <span class="meta">@transient</span> _rdd: RDD[_ &lt;: Product2[K, V]],</span><br><span class="line">    val partitioner: Partitioner,</span><br><span class="line">    val serializer: Option[Serializer] = None,</span><br><span class="line">    val keyOrdering: Option[Ordering[K]] = None,</span><br><span class="line">    val aggregator: Option[Aggregator[K, V, C]] = None,</span><br><span class="line">    val mapSideCombine: Boolean = <span class="keyword">false</span>)</span><br><span class="line">extends Dependency[Product2[K, V]] &#123;</span><br><span class="line"> </span><br><span class="line">override def rdd = _rdd.asInstanceOf[RDD[Product2[K, V]]]</span><br><span class="line"><span class="comment">//获取新的shuffleId</span></span><br><span class="line">val shuffleId: Int = _rdd.context.newShuffleId()</span><br><span class="line"><span class="comment">//向ShuffleManager注册Shuffle的信息</span></span><br><span class="line">val shuffleHandle: ShuffleHandle =</span><br><span class="line">_rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.size, <span class="keyword">this</span>)</span><br><span class="line">    _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(<span class="keyword">this</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
注意：宽依赖支持两种Shuffle Manager。<br>即org.apache.spark.shuffle.hash.HashShuffleManager（基于Hash的Shuffle机制）和<br>org.apache.spark.shuffle.sort.SortShuffleManager（基于排序的Shuffle机制）。</li>
</ul>
<ol start="4">
<li><p>可选：KV RDD可重分区。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于key-value的RDD才会有Partitioner，非key-value的RDD的Partitioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p>
</li>
<li><p>可选：移动计算，存储存取每个Partition的优先位置。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在块的位置。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark RDD</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSQL 做统计分析</title>
    <url>/2018/10/11/SparkSQL%E5%81%9A%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>SparkSQL 做统计分析的方法、存储格式的转换</p>
<a id="more"></a>

<hr>
<h4 id="SparkSQL做统计分析"><a href="#SparkSQL做统计分析" class="headerlink" title="SparkSQL做统计分析"></a>SparkSQL做统计分析</h4><ol>
<li>数据</li>
<li>需求：求每个国家的每个域名的访问流量排名前2</li>
<li>SQL实现<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupTopN</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"data/data.json"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">		<span class="comment">//读取数据</span></span><br><span class="line">        <span class="keyword">val</span> ds = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//为生成需要的表格做准备</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">3</span>), words(<span class="number">12</span>), words(<span class="number">15</span>).toLong)</span><br><span class="line">        &#125;).toDF(<span class="string">"country"</span>, <span class="string">"domain"</span>, <span class="string">"traffic"</span>)</span><br><span class="line"></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"access"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 每个国家的域名流量前2</span></span><br><span class="line">        <span class="keyword">val</span> topNSQL=<span class="string">""</span><span class="string">"select</span></span><br><span class="line"><span class="string">                      |	*</span></span><br><span class="line"><span class="string">                      |from (</span></span><br><span class="line"><span class="string">                      |		select</span></span><br><span class="line"><span class="string">                      |			t.*,row_number() over(partition by country order by sum_traffic desc) r</span></span><br><span class="line"><span class="string">                      |		from</span></span><br><span class="line"><span class="string">                      |			(</span></span><br><span class="line"><span class="string">                      |				select country,domain,sum(traffic) as sum_traffic from access group by country,domain</span></span><br><span class="line"><span class="string">                      |			) t</span></span><br><span class="line"><span class="string">                      |	    ) rt</span></span><br><span class="line"><span class="string">                      |where rt.r &lt;=2 "</span><span class="string">""</span>.stripMargin</span><br><span class="line">        spark.sql(topNSQL).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>如果只要求traffic的降序，可以使用API直接写出来<br>分组，求和，别名，降序<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//traffic降序排序</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">df.groupBy(<span class="string">"country"</span>,<span class="string">"domain"</span>).agg(sum(<span class="string">"traffic"</span>).as(<span class="string">"sum_traffic"</span>)).sort($<span class="string">"sum_traffic"</span>.desc).show()</span><br></pre></td></tr></table></figure>
注意看源码中案例仿写</li>
<li>结果展示<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----------------+-----------------+-----------+---+</span><br><span class="line">|         country|           domain|sum_traffic|  r|</span><br><span class="line">+----------------+-----------------+-----------+---+</span><br><span class="line">|            中国| www.bilibili.com|   <span class="number">24265886</span>|  <span class="number">1</span>|</span><br><span class="line">|            中国|www.ruozedata.com|    <span class="number">4187637</span>|  <span class="number">2</span>|</span><br><span class="line">|          利比亚| www.bilibili.com|      <span class="number">22816</span>|  <span class="number">1</span>|</span><br><span class="line">|          利比亚|  ruoze.ke.qq.com|      <span class="number">15970</span>|  <span class="number">2</span>|</span><br><span class="line">|            加纳| www.bilibili.com|     <span class="number">138659</span>|  <span class="number">1</span>|</span><br><span class="line">|            加纳|www.ruozedata.com|      <span class="number">17988</span>|  <span class="number">2</span>|</span><br><span class="line">|        利比里亚| www.bilibili.com|      <span class="number">20593</span>|  <span class="number">1</span>|</span><br><span class="line">|        利比里亚|  ruoze.ke.qq.com|       <span class="number">7466</span>|  <span class="number">2</span>|</span><br><span class="line">+----------------+-----------------+-----------+---+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="UDF函数"><a href="#UDF函数" class="headerlink" title="UDF函数"></a>UDF函数</h4><ol>
<li><p>数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">蔡三	唱,跳,rap,篮球</span><br><span class="line">李四	唱</span><br><span class="line">王五	唱,跳</span><br></pre></td></tr></table></figure></li>
<li><p>需求：求出每个人的爱好的个数<br>提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎<br>DataFrame：它可以根据很多源进行构建，包括：结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD</p>
</li>
<li><p>SQL实现</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">practice04</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"data/data.txt"</span></span><br><span class="line">    <span class="keyword">val</span> session = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    session.sparkContext.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    <span class="keyword">import</span> session.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ds = session.read.textFile(in)</span><br><span class="line">    <span class="keyword">val</span> df = ds.map(row =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> words = row.split(<span class="string">"\t"</span>)</span><br><span class="line">      (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">    &#125;).toDF(<span class="string">"name"</span>, <span class="string">"fav"</span>)</span><br><span class="line"></span><br><span class="line">    session.udf.register(<span class="string">"length"</span>,(fav: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      fav.split(<span class="string">","</span>).length</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"udf_fav"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql1 =</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |select</span></span><br><span class="line"><span class="string">        | name,</span></span><br><span class="line"><span class="string">        | fav,</span></span><br><span class="line"><span class="string">        | length(fav) fav_count</span></span><br><span class="line"><span class="string">        |from udf_fav</span></span><br><span class="line"><span class="string">        |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">    session.sql(sql1).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>上面是使用SQL的解决方案，还可以使用API的方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//自定义的udf需要返回值</span></span><br><span class="line"><span class="keyword">val</span> loveLengthUDF: <span class="type">UserDefinedFunction</span> = spark.udf.register(<span class="string">"length"</span>, (love: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">    love.split(<span class="string">","</span>).length</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">//df.select中传入UDF函数</span></span><br><span class="line">df.select($<span class="string">"name"</span>,$<span class="string">"love"</span>,loveLengthUDF($<span class="string">"love"</span>)).show()</span><br></pre></td></tr></table></figure></li>
<li><p>结果展示</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----+--------------+---------+</span><br><span class="line">|name|           fav|fav_count|</span><br><span class="line">+----+--------------+---------+</span><br><span class="line">|蔡三|唱,跳,rap,篮球|        <span class="number">4</span>|</span><br><span class="line">|李四|            唱|        <span class="number">1</span>|</span><br><span class="line">|王五|         唱,跳|        <span class="number">2</span>|</span><br><span class="line">+----+--------------+---------+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="存储格式的转换"><a href="#存储格式的转换" class="headerlink" title="存储格式的转换"></a>存储格式的转换</h4><p>Spark读text文件进行清洗，清洗完以后直接以我们想要的列式存储格式输出，如果按以前的方式要经过很多复杂的步骤</p>
<p>用Spark的时候只需要在df.write.format(“orc”).mode().save()中指定格式即可，如orc，现在就很方便了，想转成什么格式，只要format支持就ok</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">text2orc</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"data/people.txt"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"out"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//对文本文件做处理</span></span><br><span class="line">        df.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>),words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">            .toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)	<span class="comment">//这一步解决了数据没有表头的问题</span></span><br><span class="line">            .write</span><br><span class="line">            .mode(<span class="string">"overwrite"</span>)	<span class="comment">//save mode</span></span><br><span class="line">            .format(<span class="string">"orc"</span>)	<span class="comment">//save format</span></span><br><span class="line">            .save(out)	<span class="comment">//save path</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSQL 基础</title>
    <url>/2018/10/10/SparkSQL/</url>
    <content><![CDATA[<p>SparkSQL 基础内容</p>
<a id="more"></a>

<hr>
<h4 id="认识SparkSQL"><a href="#认识SparkSQL" class="headerlink" title="认识SparkSQL"></a>认识SparkSQL</h4><ol>
<li>SparkSQL的版本变更<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.0以前：</span><br><span class="line">   Shark</span><br><span class="line">1.1.x开始：</span><br><span class="line">   SparkSQL(只是测试性的) SQL</span><br><span class="line">1.3.x:</span><br><span class="line">   SparkSQL(正式版本)+Dataframe</span><br><span class="line">1.5.x:</span><br><span class="line">   SparkSQL 钨丝计划</span><br><span class="line">1.6.x：</span><br><span class="line">   SparkSQL+DataFrame+DataSet(测试版本)</span><br><span class="line">2.x.x:</span><br><span class="line">   SparkSQL+DataFrame+DataSet(正式版本)</span><br><span class="line">   SparkSQL:还有其他的优化</span><br><span class="line">   StructuredStreaming(DataSet)</span><br></pre></td></tr></table></figure></li>
<li>什么是SparkSQL?<br>spark SQL是spark的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame。</li>
<li>SparkSQL的作用<br>提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎<br>DataFrame：它可以根据很多源进行构建，包括：结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD</li>
<li>运行原理<br>将 Spark SQL 转化为 RDD， 然后提交到集群执行</li>
<li>特点</li>
</ol>
<ul>
<li>容易整合</li>
<li>统一的数据访问方式</li>
<li>兼容 Hive</li>
<li>标准的数据连接</li>
</ul>
<ol start="6">
<li>spark sql<br>spark-sql是一个Spark专属的SQL命令行交互工具，在使用spark-sql之前要把hive-site.xml 拷贝到Spark/Conf下，spark-sql和spark-shell用法一样，但是在引入外部依赖的时候，spark-sql需要用–jars和–driver-class-path同时引入依赖才不会报错</li>
<li>持久化<br>在spark-sql中的持久化Table命令是: cache table xxx，清除持久化 uncache table xxx<br>spark-SQL中的cache和uncache都是eager的，立即执行的<br>考点：RDD和SparkSQL的cache有什么区别？<br>RDD中的cache是lazy的 spark-SQL中的cache是eager的</li>
<li>遗留问题<br>–files/–jars 传进去的东西清不掉</li>
</ol>
<h4 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h4><p>SparkSession是Spark 2.0引如的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。</p>
<p>在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于Hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，SQLContext和HiveContext也被保存下来。</p>
<p>SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p>
<p>特点：</p>
<ol>
<li>为用户提供一个统一的切入点使用Spark 各项功能</li>
<li>允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序</li>
<li>减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互</li>
<li>与 Spark 交互之时不需要显示的创建 SparkConf, SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中</li>
</ol>
<h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p>
<p><img src="/2018/10/10/SparkSQL/RDD%E5%92%8CDataFrame%E7%9A%84%E5%AD%98%E5%82%A8%E5%86%85%E5%AE%B9%E6%AF%94%E8%BE%83.png" alt="RDD和DataFrame的存储内容比较"></p>
<h4 id="DataFrame的read和write"><a href="#DataFrame的read和write" class="headerlink" title="DataFrame的read和write"></a>DataFrame的read和write</h4><h5 id="json"><a href="#json" class="headerlink" title="json"></a>json</h5><ol>
<li>数据的读取[DataFrameReader]<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">rdd2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"data/people.json"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">		<span class="comment">//  读取json数据</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"json"</span>).load(in)</span><br><span class="line">        <span class="comment">//  使用$"" 导入隐式转换</span></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//  可以使用UDF</span></span><br><span class="line">        df.select($<span class="string">"name"</span>,$<span class="string">"age"</span>).show(<span class="number">2</span>,<span class="literal">false</span>)</span><br><span class="line">        <span class="comment">//  不可以使用UDF 适合大部分场景</span></span><br><span class="line">        df.select(<span class="string">"name"</span>,<span class="string">"age"</span>).show()</span><br><span class="line">        <span class="comment">//  不推介，写着复杂</span></span><br><span class="line">        df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).show(<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
select方法用于选择要输出的列，推介使用 $”col” 和 “col” 的方法</li>
</ol>
<ul>
<li><ol>
<li>使用select可以选取打印的列，空值为null</li>
</ol>
</li>
<li><ol start="2">
<li>show()默认打印20条数据，可以指定条数</li>
</ol>
</li>
<li><ol start="3">
<li>truncate默认为true，截取长度，可以设置为false<br>select方法有三种不同的写法，fliter也有<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(<span class="symbol">'name</span> === <span class="string">"Andy"</span>).show()	<span class="comment">//推介使用</span></span><br><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(df(<span class="string">"name"</span>) === <span class="string">"Andy"</span>).show()</span><br><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(<span class="string">"name = 'Andy'"</span>).show()</span><br></pre></td></tr></table></figure>
printSchema()方法可以查看数据的Schema信息<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.printSchema()</span><br><span class="line">------------------------------------------------</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- name: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<ol start="2">
<li>数据的存储[DataFrameWriter]<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> selectDf: <span class="type">DataFrame</span> = df.select($<span class="string">"name"</span>, $<span class="string">"age"</span>)</span><br><span class="line"><span class="comment">//  写出json数据</span></span><br><span class="line">selectDf.write.format(<span class="string">"json"</span>).mode(<span class="string">"overwrite"</span>).save(out)</span><br></pre></td></tr></table></figure>
这里需要知道的一个概念是Save Modes<br>Save操作可以选择使用SaveMode，它指定目标如果存在，如何处理现有数据。重要的是要认识到，这些保存模式不利用任何锁定，也不是原子性的。此外，在执行覆盖时，在写入新数据之前将删除数据。<br><img src="/2018/10/10/SparkSQL/SaveMode.png" alt="SaveMode"></li>
</ol>
<h5 id="text"><a href="#text" class="headerlink" title="text"></a>text</h5><ol>
<li>数据的读取<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">text2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"data/people.txt"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"out"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame不能直接split，且调用map返回的是一个Dataset</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"text"</span>).load(in)</span><br><span class="line">        <span class="keyword">val</span> mapDF: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = df.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        mapDF.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame转换为RDD后，再toDF，返回的是一个DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> mapRDD2DF: <span class="type">DataFrame</span> = df.rdd.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).toDF()</span><br><span class="line">        mapRDD2DF.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用textFile方法读取文本文件直接返回的是一个Dataset</span></span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(in)</span><br><span class="line">        <span class="keyword">val</span> mapDs: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        mapDs.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
文本数据读进来的一行在一个字段里面，所以要使用map算子，在map中split</li>
</ol>
<ul>
<li>直接read.format()读进来的是DataFrame，map中不能直接split</li>
<li>DataFrame通过.rdd的方式转换成RDD，map中也不能直接split</li>
<li>通过read.textFile()的方式读进来的是Dataset，map中可以split</li>
</ul>
<ol start="2">
<li>数据的存储<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"text"</span>).load(in)</span><br><span class="line"><span class="keyword">val</span> mapDF = df.map(row =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">    <span class="comment">// 拼接成一列</span></span><br><span class="line">    words(<span class="number">0</span>) +<span class="string">","</span>+words(<span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">mapDF.write.format(<span class="string">"text"</span>).mode(<span class="string">"overwrite"</span>).save(out)</span><br></pre></td></tr></table></figure>
文本数据写出去的时候</li>
</ol>
<ul>
<li>不支持int类型，如果存在int类型，会报错，解决办法是toString，转换成字符串</li>
<li>只能作为一列输出，如果是多列，会报错，解决办法是拼接起来，组成一列<br>文本数据压缩输出，只要是Spark支持的压缩的格式，都可以指定<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapDF.write</span><br><span class="line">    .format(<span class="string">"text"</span>)</span><br><span class="line">    <span class="comment">// 添加压缩操作</span></span><br><span class="line">    .option(<span class="string">"compression"</span>,<span class="string">"gzip"</span>)</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>)</span><br><span class="line">    .save(out)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h5><ol>
<li>数据的读取<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">csv2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"data/people.csv"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"out"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">            .format(<span class="string">"csv"</span>)</span><br><span class="line">            .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">            .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</span><br><span class="line">            .option(<span class="string">"interSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">            .load(in)</span><br><span class="line">        df.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
csv读取数据注意使用几个参数</li>
</ol>
<ul>
<li>指定表头：option(“header”, “true”)</li>
<li>指定分隔符：option(“sep”, “;”)</li>
<li>类型自动推测：option(“interSchema”,”true”)</li>
</ul>
<h5 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h5><p>在操作jdbc之前要导入两个依赖，一个是mysql-jdbc，用来连接mysql，一个是config，用来解决硬编码的问题</p>
<p>依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.typesafe<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>config<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>application.conf文件</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">db.<span class="keyword">default</span>.driver=<span class="string">"com.mysql.jdbc.Driver"</span></span><br><span class="line">db.<span class="keyword">default</span>.url=<span class="string">"jdbc:mysql://hadoop/listener?characterEncoding=utf-8&amp;useSSL=false"</span></span><br><span class="line">db.<span class="keyword">default</span>.user=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.password=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.source=<span class="string">"dws_ad_phone_type_dist"</span></span><br><span class="line">db.<span class="keyword">default</span>.target=<span class="string">"dws_ad_phone_type_dist_1"</span></span><br><span class="line">db.<span class="keyword">default</span>.db=<span class="string">"access_dw"</span></span><br><span class="line"></span><br><span class="line"># Connection Pool settings</span><br><span class="line">db.<span class="keyword">default</span>.poolInitialSize=<span class="number">10</span></span><br><span class="line">db.<span class="keyword">default</span>.poolMaxSize=<span class="number">20</span></span><br><span class="line">db.<span class="keyword">default</span>.connectionTimeoutMillis=<span class="number">1000</span></span><br></pre></td></tr></table></figure>

<ol>
<li><p>数据的读取</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mysql2df</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">		<span class="comment">//获取配置文件中的值，db.default开头</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="type">ConfigFactory</span>.load()</span><br><span class="line">        <span class="keyword">val</span> driver = conf.getString(<span class="string">"db.default.driver"</span>)</span><br><span class="line">        <span class="keyword">val</span> url = conf.getString(<span class="string">"db.default.url"</span>)</span><br><span class="line">        <span class="keyword">val</span> user = conf.getString(<span class="string">"db.default.user"</span>)</span><br><span class="line">        <span class="keyword">val</span> password = conf.getString(<span class="string">"db.default.password"</span>)</span><br><span class="line">        <span class="keyword">val</span> source = conf.getString(<span class="string">"db.default.source"</span>)</span><br><span class="line">        <span class="keyword">val</span> target = conf.getString(<span class="string">"db.default.target"</span>)</span><br><span class="line">        <span class="keyword">val</span> db = conf.getString(<span class="string">"db.default.db"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取数据库的内容</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">            .format(<span class="string">"jdbc"</span>)</span><br><span class="line">            .option(<span class="string">"url"</span>, url)</span><br><span class="line">            .option(<span class="string">"dbtable"</span>, <span class="string">s"<span class="subst">$db</span>.<span class="subst">$source</span>"</span>)	<span class="comment">//库名.源表</span></span><br><span class="line">            .option(<span class="string">"user"</span>, user)</span><br><span class="line">            .option(<span class="string">"password"</span>, password)</span><br><span class="line">            .option(<span class="string">"driver"</span>, driver)</span><br><span class="line">            .load()</span><br><span class="line">        <span class="comment">//使用DataFrame创建临时表提供spark.sql查询</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"phone_type_dist"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//spark.sql写SQL返回一个DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> sqlDF: <span class="type">DataFrame</span> = spark.sql(<span class="string">"select * from phone_type_dist where phoneSystemType = 'IOS'"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用df.createOrReplaceTempView()方法创建一个DataFrame数据生成的临时表，提供spark.sql()使用SQL操作数据，返回的也是一个DataFrame</p>
</li>
<li><p>数据的存储</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//接着上面返回的sqlDF: DataFrame</span></span><br><span class="line">sqlDF.write</span><br><span class="line">    .format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, url)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, <span class="string">s"<span class="subst">$db</span>.<span class="subst">$target</span>"</span>)	<span class="comment">//库名.目标表</span></span><br><span class="line">    .option(<span class="string">"user"</span>, user)</span><br><span class="line">    .option(<span class="string">"password"</span>, password)</span><br><span class="line">    .option(<span class="string">"driver"</span>,driver)</span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkStreaming 基础</title>
    <url>/2019/03/16/SparkStreaming%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li>SparkStreaming简介</li>
<li>SparkStreaming的内部结构</li>
<li>StreamingContext对象</li>
<li>离散流（DStream）</li>
<li>IDEA开发Spark Streaming</li>
</ol>
<a id="more"></a>
<hr>
<h4 id="SparkStreaming简介"><a href="#SparkStreaming简介" class="headerlink" title="SparkStreaming简介"></a>SparkStreaming简介</h4><p>Spark Streaming是核心Spark API的扩展，实现可扩展、高吞吐量、可容错的实时数据流处理。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等众多来源获取，并且可以使用由高级函数（如map，reduce，join和window）开发的复杂算法进行流数据处理。<br>最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。<br>而且还可以在数据流上应用Spark提供的机器学习和图处理算法。</p>
<p><img src="/2019/03/16/SparkStreaming%E5%9F%BA%E7%A1%80/what-is-spark-streaming.png" alt="what is SparkStreaming"></p>
<h4 id="SparkStreaming的内部结构"><a href="#SparkStreaming的内部结构" class="headerlink" title="SparkStreaming的内部结构"></a>SparkStreaming的内部结构</h4><p>在内部，它的工作原理如下。<br>Spark Streaming接收实时输入数据流，并将数据切分成批，然后由Spark引擎对其进行处理，最后生成“批”形式的结果流。</p>
<p><img src="/2019/03/16/SparkStreaming%E5%9F%BA%E7%A1%80/streaming-batch-process.png" alt="streaming batch process"></p>
<p>Spark Streaming将连续的数据流抽象为discretizedstream(DStream)。在内部，DStream由一个RDD序列表示。</p>
<h4 id="StreamingContext对象"><a href="#StreamingContext对象" class="headerlink" title="StreamingContext对象"></a>StreamingContext对象</h4><p>初始化StreamingContext：<br>方式一，从SparkConf对象中创建：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建一个Context对象：StreamingContext</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"MyNetworkWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//指定批处理的时间间隔</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<p>方式二，从现有的SparkContext实例中创建：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li>appName参数是应用程序在集群UI上显示的名称。</li>
<li>master是Spark，Mesos或YARN集群的URL，或者一个特殊的“local [*]”字符串来让程序以本地模式运行。</li>
<li>当在集群上运行程序时，不需要在程序中硬编码master参数，而是使用spark-submit提交应用程序并将master的URL以脚本参数的形式传入。但是，对于本地测试和单元测试，您可以通过“local[*]”来运行Spark Streaming程序（请确保本地系统中的cpu核心数够用）。</li>
<li>StreamingContext会内在的创建一个SparkContext的实例（所有Spark功能的起始点），你可以通过ssc.sparkContext访问到这个实例。</li>
<li>批处理的时间窗口长度必须根据应用程序的延迟要求和可用的集群资源进行设置。</li>
</ul>
<p>注意：</p>
<ul>
<li>一旦一个StreamingContext开始运作，就不能设置或添加新的流计算。</li>
<li>一旦一个上下文被停止，它将无法重新启动。</li>
<li>同一时刻，一个JVM中只能有一个StreamingContext处于活动状态。</li>
<li>StreamingContext上的stop()方法也会停止SparkContext。 要仅停止StreamingContext（保持SparkContext活跃），请将stop() 方法的可选参数stopSparkContext设置为false。</li>
<li>只要前一个StreamingContext在下一个StreamingContext被创建之前停止（不停止SparkContext），SparkContext就可以被重用来创建多个StreamingContext。</li>
</ul>
<h4 id="离散流（DStream）"><a href="#离散流（DStream）" class="headerlink" title="离散流（DStream）"></a>离散流（DStream）</h4><p>DiscretizedStream(DStream) 是Spark Streaming对流式数据的基本抽象。<br>它表示连续的数据流，这些连续的数据流可以是从数据源接收的输入数据流，也可以是通过对输入数据流执行转换操作而生成的经处理的数据流。<br>在内部，DStream由一系列连续的RDD表示，如下图：</p>
<p><img src="/2019/03/16/SparkStreaming%E5%9F%BA%E7%A1%80/streaming-dstream-1.png" alt="streaming dstream 1"></p>
<p>我们将一行行文本组成的流转换为单词流，具体做法为：将flatMap操作应用于名为lines的 DStream中的每个RDD上，以生成words DStream的RDD。如下图所示：<br><img src="/2019/03/16/SparkStreaming%E5%9F%BA%E7%A1%80/streaming-dstream-2.png" alt="streaming dstream 2"></p>
<p>但是DStream和RDD也有区别，下面画图说明：<br><img src="/2019/03/16/SparkStreaming%E5%9F%BA%E7%A1%80/streaming-dstream-3.png" alt="streaming dstream 3"></p>
<p><img src="/2019/03/16/SparkStreaming%E5%9F%BA%E7%A1%80/streaming-dstream-4.png" alt="streaming dstream 4"></p>
<h4 id="IDEA开发SparkStreaming"><a href="#IDEA开发SparkStreaming" class="headerlink" title="IDEA开发SparkStreaming"></a>IDEA开发SparkStreaming</h4><p>要编写自己的Spark流程序，必须将以下依赖项添加到Maven项目中。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="SocketFile简单的单词计数"><a href="#SocketFile简单的单词计数" class="headerlink" title="SocketFile简单的单词计数"></a>SocketFile简单的单词计数</h5><ol>
<li>实现代scala代码逻辑<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到StreamingContext对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启StreamingContext</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(ssc: <span class="type">StreamingContext</span>) = &#123;</span><br><span class="line">    <span class="comment">//输入记录</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//逻辑处理</span></span><br><span class="line">    <span class="keyword">val</span> words = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> pair = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> result = pair.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//输出记录</span></span><br><span class="line">    result.print()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>使用nc发送消息<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -lk 9100</span></span><br><span class="line"></span><br><span class="line">hello world</span><br></pre></td></tr></table></figure></li>
<li>客户端接收消息<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1357008430000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,1)</span><br><span class="line">(world,1)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>DStreams 是表示从源端接收的输入数据的数据流。<br>在这个简单的示例中，行是一个输入DStream，因为它表示从netcat服务器接收到的数据流。<br>每个输入DStream(本节后面讨论的文件流除外)都与接收方(Scala doc、Java doc)对象相关联，接收方接收来自源的数据并将其存储在Spark内存中进行处理。</p>
<p>注意：<br>Spark流应用程序需要分配足够的Core来处理接收到的数据，以及运行接收方。<br>设置core的数量要大于Receivers的数量。</p>
<h5 id="Checkpoint维护State"><a href="#Checkpoint维护State" class="headerlink" title="Checkpoint维护State"></a>Checkpoint维护State</h5><p>什么是updateStateByKey?</p>
<ul>
<li>updateStateByKey(func)可以返回一个新“state”的DStream，其中通过对键的前一个状态和键的新值应用给定的函数来更新每个键的状态。<br>这可以用来维护每个键的任意状态数据。</li>
</ul>
<p>什么是Checkpoint?</p>
<ul>
<li>Checkpoint可以通过在一个容错的、可靠的文件系统中设置一个目录来启用，Checkpoint信息将被保存到这个目录中。<br>这是通过使用streamingContext.checkpoint(checkpointDirectory)实现的。</li>
</ul>
<ol>
<li><p>下面案例也是单词计数，只不过需求变成了求当天到现在为止的单词计数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理逻辑</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(ssc: <span class="type">StreamingContext</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置checkpoint目录，保存offset</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"./chk"</span>)</span><br><span class="line">    <span class="comment">// updateStateByKey：维护记录的state</span></span><br><span class="line">    lines.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).updateStateByKey(updateFunction)</span><br><span class="line">    .print()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现对新值和旧值的累加</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], oldValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> curr = newValues.sum</span><br><span class="line">    <span class="keyword">val</span> old = oldValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> count = curr + old</span><br><span class="line">    <span class="type">Some</span>(count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用nc发送消息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -lk 9100</span></span><br><span class="line"></span><br><span class="line">a a a b b c</span><br><span class="line">a a a b b c</span><br></pre></td></tr></table></figure>
</li>
<li><p>客户端接收消息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: 1587439170000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,2)</span><br><span class="line">(a,3)</span><br><span class="line">(c,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1587439175000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,6)</span><br><span class="line">(c,2)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>上面的代码一直运行，结果可以一直累加，但是代码一旦停止运行，再次运行时，结果会不会接着上一次进行计算，上一次的计算结果丢失了，主要原因上每次程序运行都会初始化一个程序入口，而2次运行的程序入口不是同一个入口，所以会导致第一次计算的结果丢失。</p>
<p>第一次的运算结果状态保存在Driver里面，所以我们如果想用上一次的计算结果，我们需要将上一次的Driver里面的运行结果状态取出来，而上面的代码有一个checkpoint方法，它会把上一次Driver里面的运算结果状态保存在checkpoint的目录里面，我们在第二次启动程序时，从checkpoint里面取出上一次的运行结果状态，把这次的Driver状态恢复成和上一次Driver一样的状态。</p>
<h5 id="Checkpoint维护State-HA"><a href="#Checkpoint维护State-HA" class="headerlink" title="Checkpoint维护State HA"></a>Checkpoint维护State HA</h5><p>以下代码参考<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing" target="_blank" rel="noopener">官网</a><br>如果想让应用程序从驱动程序故障中恢复，我们应该重写代码，让它具备下面的功能</p>
<ul>
<li>当程序第一次启动时，它将创建一个新的StreamingContext，设置所有的流，然后调用start()。</li>
<li>当程序在失败后重新启动时，它将从Checkpoint目录中的Checkpoint数据重新创建一个StreamingContext。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> checkpoint = <span class="string">"./chk_v2"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc  = <span class="type">StreamingContext</span>.getOrCreate(checkpoint, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建StreamingContext</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">50000</span>))   <span class="comment">// new context</span></span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    ssc.checkpoint(checkpoint)   <span class="comment">// set checkpoint directory</span></span><br><span class="line">    ssc</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理具题的业务逻辑</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(ssc: <span class="type">StreamingContext</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>) <span class="comment">// create DStreams</span></span><br><span class="line"></span><br><span class="line">    lines</span><br><span class="line">    .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    .map((_, <span class="number">1</span>))</span><br><span class="line">    .updateStateByKey(updateFunction)</span><br><span class="line">    .print()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更新state</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], oldValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> curr = newValues.sum</span><br><span class="line">    <span class="keyword">val</span> old = oldValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> count = curr + old</span><br><span class="line">    <span class="type">Some</span>(count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="源码中维护State的方法"><a href="#源码中维护State的方法" class="headerlink" title="源码中维护State的方法"></a>源码中维护State的方法</h5><p>在阅读源码中的Example模块下Streaming下的StatefulNetworkWordCount object时，发现了一种维护State的写法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 保存checkpoint</span></span><br><span class="line"><span class="keyword">val</span> checkpoint = <span class="string">"./chk_v3"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 拿到 StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(checkpoint, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 StreamingContext</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.checkpoint(checkpoint)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对记录做累加操作</span></span><br><span class="line">    <span class="keyword">val</span> mappingFunc = (word: <span class="type">String</span>, one: <span class="type">Option</span>[<span class="type">Int</span>], state: <span class="type">State</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span>(state.isTimingOut())&#123;</span><br><span class="line">            println(<span class="string">"超时3秒没拿到数据"</span>)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">val</span> sum = one.getOrElse(<span class="number">0</span>) + state.getOption.getOrElse(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">val</span> output = (word, sum)</span><br><span class="line">            state.update(sum)</span><br><span class="line">            output</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 逻辑处理</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line">    lines</span><br><span class="line">    .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    .map((_,<span class="number">1</span>))</span><br><span class="line">    .mapWithState(<span class="type">StateSpec</span>.function(mappingFunc)</span><br><span class="line">                  .timeout(<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">                 ).print()</span><br><span class="line"></span><br><span class="line">    ssc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 多目录输出</title>
    <url>/2018/12/01/Spark%E5%A4%9A%E7%9B%AE%E5%BD%95%E8%BE%93%E5%87%BA/</url>
    <content><![CDATA[<p>实现多目录输出自定义类</p>
<a id="more"></a>

<hr>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.<span class="type">NullWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.lib.<span class="type">MultipleTextOutputFormat</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMultipleTextOutputFormat</span> <span class="keyword">extends</span> <span class="title">MultipleTextOutputFormat</span>[<span class="type">Any</span>,<span class="type">Any</span>] </span>&#123;</span><br><span class="line">    <span class="comment">//生成最终生成的key的类型，这里不要，给Null</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualKey</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>): <span class="type">Any</span> = <span class="type">NullWritable</span>.get()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生成最终生成的value的类型，这里是String</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualValue</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">        value.asInstanceOf[<span class="type">String</span>]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生成文件名</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateFileNameForKeyValue</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>, name: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">s"<span class="subst">$key</span>/<span class="subst">$name</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="主类，使用saveAsHadoopFile-path-keyClass-valueClass-fm-runtimeClass-asInstanceOf-Class-F-方法保存数据，指定参数"><a href="#主类，使用saveAsHadoopFile-path-keyClass-valueClass-fm-runtimeClass-asInstanceOf-Class-F-方法保存数据，指定参数" class="headerlink" title="主类，使用saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])方法保存数据，指定参数"></a>主类，使用saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])方法保存数据，指定参数</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MultipleDirectory</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"spark-core/out"</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(sc.hadoopConfiguration,out)</span><br><span class="line">        <span class="comment">//读取数组，转换成键值对的格式</span></span><br><span class="line">        <span class="keyword">val</span> lines = sc.textFile(<span class="string">"spark-core/ip/access-result/*"</span>)</span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = line.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">12</span>), line)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//多目录保存文件</span></span><br><span class="line">        mapRDD.saveAsHadoopFile(out,</span><br><span class="line">                                classOf[<span class="type">String</span>],</span><br><span class="line">                                classOf[<span class="type">String</span>],</span><br><span class="line">                                classOf[<span class="type">MyMultipleTextOutputFormat</span>])</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p><img src="/2018/12/01/Spark%E5%A4%9A%E7%9B%AE%E5%BD%95%E8%BE%93%E5%87%BA/%E5%A4%9A%E7%9B%AE%E5%BD%95%E8%BE%93%E5%87%BA.jpg" alt="多目录输出"></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkStreaming 进阶</title>
    <url>/2019/04/17/SparkStreaming%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li>黑名单管理</li>
<li>窗口</li>
<li>闭包</li>
<li>SS对接Kafka</li>
<li>KafkaRDD</li>
</ol>
<a id="more"></a>
<hr>
<h4 id="黑名单管理"><a href="#黑名单管理" class="headerlink" title="黑名单管理"></a>黑名单管理</h4><p>Spark Streaming在计算流式数据时，有时候需要过滤一些数据，比如一些特殊的字段，或者利用爬虫爬取数据的恶意ip，又或者那些帮助某些无良商家刷广告的人，那么我们有一个黑名单，来过滤或者禁止他们的访问<br>思路：</p>
<ol>
<li>准备一个管理黑名单的文件，读进RDD作为key，并添加一个value值为true</li>
<li>Spark Streaming接收流式数据，使用transform转换为RDD，拿到key用来做join，value为数据内容</li>
<li>两个RDD做left join，并过滤掉value值为true的数据</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取黑名单数据，并做简单处理</span></span><br><span class="line">    <span class="keyword">val</span> blacks = <span class="type">List</span>(<span class="string">"tunan"</span>)</span><br><span class="line">    <span class="keyword">val</span> blackRDD = ssc.sparkContext.parallelize(blacks)</span><br><span class="line">    <span class="keyword">val</span> blackMapRDD = blackRDD.map((_,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从socket拿到流式数据</span></span><br><span class="line">    <span class="keyword">val</span> stream = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 数据转换成RDD</span></span><br><span class="line">    stream.transform( rdd =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> mapRDD= rdd.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            <span class="comment">// 拿到做join的key，value为数据内容</span></span><br><span class="line">            (words(<span class="number">0</span>), row)</span><br><span class="line">        &#125;)</span><br><span class="line">		<span class="comment">// 流式数据为基表，做left join</span></span><br><span class="line">        <span class="keyword">val</span> joinRDD = mapRDD.leftOuterJoin(blackMapRDD)</span><br><span class="line">		<span class="comment">// 过滤掉黑名单数据</span></span><br><span class="line">        <span class="keyword">val</span> filterRDD = joinRDD.filter(_._2._2.getOrElse(<span class="literal">false</span>) != <span class="literal">true</span>)</span><br><span class="line">        <span class="comment">// 返回数据内容</span></span><br><span class="line">        filterRDD.map(_._2._1)</span><br><span class="line">    &#125;).print()</span><br><span class="line">    </span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="SparkStreaming窗口"><a href="#SparkStreaming窗口" class="headerlink" title="SparkStreaming窗口"></a>SparkStreaming窗口</h4><p>Spark Streaming还提供了窗口计算功能，允许在数据的滑动窗口上应用转换操作。下图说明了滑动窗口的工作方式：</p>
<p><img src="/2019/04/17/SparkStreaming%E8%BF%9B%E9%98%B6/streaming-dstream-window.png" alt="streaming dstream window"></p>
<p>如图所示，每当窗口滑过originalDStream时，落在窗口内的源RDD被组合并被执行操作以产生windowedDStream的RDD。<br>在上面的例子中，操作应用于最近3个时间单位的数据，并以2个时间单位滑动。<br>这表明任何窗口操作都需要指定两个参数：</p>
<ul>
<li>窗口长度（windowlength） 窗口的时间长度（上图的示例中为：3）。</li>
<li>滑动间隔（slidinginterval） 两次相邻的窗口操作的间隔（即每次滑动的时间长度）（上图示例中为：2）。<br>这两个参数必须是源DStream的批间隔的倍数（上图示例中为：1）。</li>
</ul>
<p>我们以一个例子来说明窗口操作。<br>对之前的单词计数的示例进行扩展，每10秒钟对过去30秒的数据进行wordcount。<br>为此，我们必须在最近30秒的pairs DStream数据中对(word, 1)键值对应用reduceByKey操作。<br>这是通过使用reduceByKeyAndWindow操作完成的。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 执行wordcount</span></span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordPair = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line"><span class="comment">//val wordCountResult = wordPair.reduceByKey(_ + _)</span></span><br><span class="line"><span class="keyword">val</span> wordCountResult = wordPair.reduceByKeyAndWindow(</span><br><span class="line">    (a: <span class="type">Int</span>, b: <span class="type">Int</span>) =&gt; (a + b), <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>一些常见的窗口操作如下表所示。所有这些操作都用到了上述两个参数：windowLength和slideInterval。</p>
<ul>
<li><p>window(windowLength, slideInterval)<br>基于源DStream产生的窗口化的批数据计算一个新的DStream</p>
</li>
<li><p>countByWindow(windowLength, slideInterval)<br>返回流中元素的一个滑动窗口数</p>
</li>
<li><p>reduceByWindow(func, windowLength, slideInterval)<br>返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数必须是相关联的以使计算能够正确的并行计算。</p>
</li>
<li><p>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])<br>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每一个key的值均由给定的reduce函数聚集起来。<br>注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数</p>
</li>
<li><p>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])<br>上述reduceByKeyAndWindow()的更高效的版本，其中使用前一窗口的reduce计算结果递增地计算每个窗口的reduce值。<br>这是通过对进入滑动窗口的新数据进行reduce操作，以及“逆减（inverse reducing）”离开窗口的旧数据来完成的。<br>一个例子是当窗口滑动时对键对应的值进行“一加一减”操作。<br>但是，它仅适用于“可逆减函数（invertible reduce functions）”，即具有相应“反减”功能的减函数（作为参数invFunc）。<br>像reduceByKeyAndWindow一样，通过可选参数可以配置reduce任务的数量。请注意，使用此操作必须启用检查点。</p>
</li>
<li><p>countByValueAndWindow(windowLength, slideInterval, [numTasks])<br>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。<br>每个key的值都是它们在滑动窗口中出现的频率。</p>
</li>
</ul>
<h4 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h4><p>Spark的难点之一是理解跨集群执行代码时变量和方法的范围和生命周期。在范围之外修改变量的RDD操作可能经常引起混淆。在下面的示例中，我们将查看使用foreach()递增计数器的代码，但是其他操作也可能出现类似的问题。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">var</span> rdd = sc.parallelize(data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wrong: Don't do this!!</span></span><br><span class="line">rdd.foreach(x =&gt; counter += x)</span><br><span class="line"></span><br><span class="line">println(<span class="string">"Counter value: "</span> + counter)</span><br></pre></td></tr></table></figure>

<p>上述代码的行为是未定义的，可能无法按预期工作。为了执行作业，Spark将RDD操作的处理分解为tasks，每个任务由executor执行。在执行之前，Spark计算task的闭包。闭包是那些executor在RDD上执行其计算时必须可见的变量和方法(在本例中为foreach())。这个闭包被序列化并发送给每个executor 。</p>
<p>闭包中发送给每个executor 的变量现在都是副本，因此，当在foreach函数中引用counter时，它不再是driver 上的计数器。在executors的内存中仍然有一个计数器，但它对executor不再可见!executor只看到来自序列化闭包的副本。因此，counter的最终值仍然是零，因为counter上的所有操作都引用了序列化闭包中的值。</p>
<p>一般来说，像循环或局部定义方法这样的闭包结构不应该用来改变全局状态。Spark不保证闭包外部引用的对象的突变行为。一些这样做的代码可能在本地模式下工作，但那只是偶然的，而且这样的代码在分布式模式下不会像预期的那样工作。如果需要全局聚合，则使用Accumulator。</p>
<h4 id="SparkStreaming对接Kafka"><a href="#SparkStreaming对接Kafka" class="headerlink" title="SparkStreaming对接Kafka"></a>SparkStreaming对接Kafka</h4><p>Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据。这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。</p>
<p>SS是Spark上的一个流式处理框架，可以面向海量数据实现高吞吐量、高容错的实时计算。SS支持多种类型数据源，包括Kafka、Flume、twitter、zeroMQ、Kinesis以及TCP sockets等。SS实时接收数据流，并按照一定的时间间隔将连续的数据流拆分成一批批离散的数据集；然后应用诸如map、reduce、join和window等丰富的API进行复杂的数据处理；最后提交给Spark引擎进行运算，得到批量结果数据，因此其也被称为准实时处理系统。而结果也能保存在很多地方，如HDFS，数据库等。另外SS也能和MLlib（机器学习）以及GraphX（图计算）完美融合。</p>
<p>下面我们就来一个SS对接Kafka的案例</p>
<p>Kafka Product API</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 设置配置文件，这些配置文件都是源码中找的</span></span><br><span class="line">    <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop:9090,hadoop:9091,hadoop:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    <span class="comment">// 创建producer</span></span><br><span class="line">    <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 循环发送数据</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">100</span>)&#123;</span><br><span class="line">        <span class="keyword">val</span> par = i%<span class="number">3</span> <span class="comment">// 数组走的分区</span></span><br><span class="line">        producer.send(<span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"test"</span>,par,<span class="string">""</span>,<span class="type">Integer</span>.toString(i)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 关闭producer</span></span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Spark Streaming Consumer</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建ssc</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接kafka配置参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">        <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop:9090,hadoop:9091,hadoop:9092"</span>,</span><br><span class="line">        <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"group.id"</span> -&gt; <span class="string">"use_a_separate_group_id_for_each_stream"</span>,</span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">        <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 可以设置多个topic</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"test"</span>)</span><br><span class="line">    <span class="comment">// 创建DirectStream</span></span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 业务逻辑</span></span><br><span class="line">    stream.map(x =&gt; (x.value(),<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">    .foreachRDD(rdd =&gt; &#123;</span><br><span class="line">        <span class="comment">// 分区操作</span></span><br><span class="line">        rdd.foreachPartition(partition =&gt; &#123;</span><br><span class="line">            <span class="comment">// 一个分区连一个jedis</span></span><br><span class="line">            <span class="keyword">val</span> jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line">            partition.foreach(fields =&gt;&#123;</span><br><span class="line">                <span class="comment">// 保存到Hash中</span></span><br><span class="line">                jedis.hincrBy(<span class="string">"wc_redis"</span>,fields._1,fields._2)</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="comment">// 关闭连接</span></span><br><span class="line">            jedis.close()</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// 启动程序</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在，无论在哪里写进Kafka的数据，都可以从Spark Streaming的客户端写出来，我们这里保存的是Redis，保存在MySQL是同样的思路。</p>
<h4 id="KafkaRDD"><a href="#KafkaRDD" class="headerlink" title="KafkaRDD"></a>KafkaRDD</h4><p>最后我们看一下如何在代码中拿到Kafka的Offset</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">        <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop:9090,hadoop:9091,hadoop:9092"</span>,</span><br><span class="line">        <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"group.id"</span> -&gt; <span class="string">"use_a_separate_group_id_for_each_stream"</span>,</span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">        <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"test"</span>)</span><br><span class="line">    <span class="comment">// stream不能做任何操作，否则得到的不是一个KafkaRDD</span></span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 必须先拿到HasOffsetRanges，才能开始业务逻辑</span></span><br><span class="line">    stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">        <span class="comment">// 通过rdd.asInstanceOf[HasOffsetRanges]拿到KafkaRDD，它保存了每个分区的offset</span></span><br><span class="line">        <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">        <span class="comment">// KafkaRDD维护了topic、partition、fromOffset、untilOffset</span></span><br><span class="line">        offsetRanges.foreach &#123; o =&gt;</span><br><span class="line">            println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span> <span class="subst">$&#123;o.partition&#125;</span> <span class="subst">$&#123;o.fromOffset&#125;</span> <span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 启动程序</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ul>
<li>对HasOffsetRanges的类型转换只有在对createDirectStream的结果调用的第一个方法中完成时才会成功，而不是在随后的方法链中。</li>
<li>RDD分区和Kafka分区之间的一一映射会在RDD发生shuffle或者repartition操作之后改变，比如reduceByKey或window</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 分组TopN</title>
    <url>/2018/09/23/Spark%E5%88%86%E7%BB%84TopN/</url>
    <content><![CDATA[<p>使用Spark Core解决TopN问题，想写出性能好的代码也不是件容易的事。<br>下面我们尝试使用多种方式解决TopN问题。</p>
<a id="more"></a>
<hr>
<h5 id="方法1，直接reduceByKey完成分组求和排序"><a href="#方法1，直接reduceByKey完成分组求和排序" class="headerlink" title="方法1，直接reduceByKey完成分组求和排序"></a>方法1，直接reduceByKey完成分组求和排序</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"file:///home/hadoop/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)	<span class="comment">//((domain,url),1)</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = mapRDD.reduceByKey(_ + _).groupBy(x =&gt; x._1._1).mapValues( x=&gt; x.toList.sortBy(x =&gt; -x._2).map(x =&gt; (x._1._1,x._1._2,x._2)).take(<span class="number">2</span>))</span><br><span class="line">    result.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法虽然直接，但是在reduceByKey和groupBy分别进过了shuffle，而且x.toList是一个非常吃内存的操作，如果数据量大，直接OOM。</p>
<h5 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = <span class="type">Array</span>(<span class="string">"www.google.com"</span>, <span class="string">"www.ruozedata.com"</span>, <span class="string">"www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (domain &lt;- domains)&#123;</span><br><span class="line">        mapRDD.filter(x =&gt; x._1._1.equals(domain)).reduceByKey(_+_).sortBy(x =&gt; -x._2).take(<span class="number">2</span>).foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>核心思想：把需要分组分类的数据提前拿出来，在filter中过滤，每次执行一个分组，虽然减少了一次shuffle，但是我们不可能每次都把需要的数据都能提前拿到数据。</p>
<h5 id="方法3，使用ditinct-collect返回的数组替换人为创建的数组"><a href="#方法3，使用ditinct-collect返回的数组替换人为创建的数组" class="headerlink" title="方法3，使用ditinct.collect返回的数组替换人为创建的数组"></a>方法3，使用ditinct.collect返回的数组替换人为创建的数组</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (domain &lt;- domains)&#123;</span><br><span class="line">        mapRDD.filter( x =&gt; domain.equals(x._1._1)).reduceByKey(_+_).sortBy(x =&gt; -x._2).take(<span class="number">2</span>).foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="方法4，使用分区执行替换for循环"><a href="#方法4，使用分区执行替换for循环" class="headerlink" title="方法4，使用分区执行替换for循环"></a>方法4，使用分区执行替换for循环</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapPartRDD = mapRDD.reduceByKey(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(domains), _ + _).mapPartitions(partition =&gt; &#123;</span><br><span class="line">        partition.toList.sortBy(x =&gt; -x._2).take(<span class="number">2</span>).iterator</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    mapPartRDD.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义的分区类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">domains:<span class="type">Array</span>[<span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">Partitioner</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> map = mutable.<span class="type">HashMap</span>[<span class="type">String</span>,<span class="type">Int</span>]()</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until (domains.length))&#123;</span><br><span class="line">        map(domains(i)) = i</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = domains.length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> domain = key.asInstanceOf[(<span class="type">String</span>, <span class="type">String</span>)]._1</span><br><span class="line">        map(domain)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="方法5，使用TreeSet替换toList实现最终的排序"><a href="#方法5，使用TreeSet替换toList实现最终的排序" class="headerlink" title="方法5，使用TreeSet替换toList实现最终的排序"></a>方法5，使用TreeSet替换toList实现最终的排序</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">        <span class="comment">//连接SparkMaster</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">            ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> ord: <span class="type">Ordering</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = <span class="keyword">new</span> <span class="type">Ordering</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)]() &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: ((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>), y: ((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">                <span class="keyword">if</span> (!x._1.equals(y._1) &amp;&amp; x._2 == y._2) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//  降序排</span></span><br><span class="line">                y._2 - x._2</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> treeSort = mapRDD.reduceByKey(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(domains), _ + _).mapPartitions(partition =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> set = mutable.<span class="type">TreeSet</span>.empty(ord)</span><br><span class="line">            partition.foreach(x =&gt; &#123;</span><br><span class="line">                set.add(x)</span><br><span class="line">                <span class="keyword">if</span> (set.size &gt; <span class="number">2</span>) &#123;</span><br><span class="line">                    set.remove(set.lastKey) <span class="comment">//移除最后一个</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            set.toIterator</span><br><span class="line">        &#125;).collect()</span><br><span class="line">        treeSort.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用TreeSet实现自定义排序器，使之每次维护的只有需要的极少量数据，这样占用内存少，效率最高。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark算子</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 各个版本特性</title>
    <url>/2018/11/09/Spark%E5%90%84%E4%B8%AA%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<p>Spark各个版本特性</p>
<a id="more"></a>

<h3 id="各个版本特性（官方文档）"><a href="#各个版本特性（官方文档）" class="headerlink" title="各个版本特性（官方文档）"></a>各个版本特性（官方文档）</h3><p><a href="https://spark.apache.org/releases/" target="_blank" rel="noopener">https://spark.apache.org/releases/</a><br><a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">https://spark.apache.org/news/index.html</a></p>
<hr>
<h3 id="Spark-0-6-x"><a href="#Spark-0-6-x" class="headerlink" title="Spark 0.6.x"></a>Spark 0.6.x</h3><p>Standalone部署模式进行了简化</p>
<hr>
<h3 id="Spark-0-7"><a href="#Spark-0-7" class="headerlink" title="Spark 0.7"></a>Spark 0.7</h3><p>Python API<br>增加Spark Streaming<br>支持maven build</p>
<hr>
<h3 id="Spark-0-8"><a href="#Spark-0-8" class="headerlink" title="Spark 0.8"></a>Spark 0.8</h3><p>支持MLlib库<br>hadoop yarn正式支持</p>
<hr>
<h3 id="Spark-0-9"><a href="#Spark-0-9" class="headerlink" title="Spark 0.9"></a>Spark 0.9</h3><p>用SparkConf类来配置SparkContext<br>spark streaming正式版发布<br>GraphX的测试版出现<br>mllib库升级，支持python<br>core升级</p>
<hr>
<h3 id="Spark-1-0"><a href="#Spark-1-0" class="headerlink" title="Spark 1.0"></a>Spark 1.0</h3><p>提出spark-submit脚本和history-server<br>yarn安全模式整合<br>spark sql被提出<br>java8的支持</p>
<hr>
<h3 id="Spark-1-1"><a href="#Spark-1-1" class="headerlink" title="Spark 1.1"></a>Spark 1.1</h3><p>spark增强了磁盘（非内存）的排序的速率</p>
<hr>
<h3 id="Spark-1-2"><a href="#Spark-1-2" class="headerlink" title="Spark 1.2"></a>Spark 1.2</h3><p>shuffle大升级<br>Graphx正式版发布</p>
<hr>
<h3 id="Spark-1-3"><a href="#Spark-1-3" class="headerlink" title="Spark 1.3"></a>Spark 1.3</h3><p>新增DataFrame API<br>Spark SQL正式脱离alpha版本</p>
<hr>
<h3 id="Spark-1-4"><a href="#Spark-1-4" class="headerlink" title="Spark 1.4"></a>Spark 1.4</h3><p>正式引入SparkR<br>Spark Core为应用提供了REST API来获取各种信息</p>
<hr>
<h3 id="Spark-1-5"><a href="#Spark-1-5" class="headerlink" title="Spark 1.5"></a>Spark 1.5</h3><p>Hive支持</p>
<hr>
<h3 id="Spark-1-6"><a href="#Spark-1-6" class="headerlink" title="Spark 1.6"></a>Spark 1.6</h3><p>新增Dataset API</p>
<hr>
<h3 id="Spark-2-0"><a href="#Spark-2-0" class="headerlink" title="Spark 2.0"></a>Spark 2.0</h3><p>用sparksession实现hivecontext和sqlcontext统一<br>合并dataframe和datasets</p>
<hr>
<h3 id="Spark-2-1"><a href="#Spark-2-1" class="headerlink" title="Spark 2.1"></a>Spark 2.1</h3><p>提升ORC格式文件的读写性能</p>
<hr>
<h3 id="Spark-2-2"><a href="#Spark-2-2" class="headerlink" title="Spark 2.2"></a>Spark 2.2</h3><p>Structured Streaming的生产环境支持已经就绪</p>
<hr>
<h3 id="Spark-2-3"><a href="#Spark-2-3" class="headerlink" title="Spark 2.3"></a>Spark 2.3</h3><p>Structured Streaming 引入了低延迟的连续处理<br>支持 stream-to-stream joins</p>
<hr>
<h3 id="Spark-2-4"><a href="#Spark-2-4" class="headerlink" title="Spark 2.4"></a>Spark 2.4</h3><p>Scala 2.12<br>添加了35个高阶函数</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 序列化</title>
    <url>/2018/10/26/Spark%E5%BA%8F%E5%88%97%E5%8C%96/</url>
    <content><![CDATA[<h5 id="官方文档：https-spark-apache-org-docs-latest-tuning-html"><a href="#官方文档：https-spark-apache-org-docs-latest-tuning-html" class="headerlink" title="官方文档：https://spark.apache.org/docs/latest/tuning.html"></a>官方文档：<a href="https://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/tuning.html</a></h5><a id="more"></a>

<p>序列化在分布式应用的性能中扮演着重要的角色。格式化对象缓慢，或者消耗大量的字节格式化，会大大降低计算性能。通常这是在spark应用中第一件需要优化的事情。Spark的目标是在便利与性能中取得平衡，所以提供2种序列化的选择。</p>
<h3 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h3><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。</p>
<h3 id="Kryo-serialization"><a href="#Kryo-serialization" class="headerlink" title="Kryo serialization"></a>Kryo serialization</h3><p>Spark还可以使用Kryo库（版本4）来更快地序列化对象。Kryo比Java串行化（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。</p>
<p>您可以通过使用SparkConf初始化作业并调用conf.set（“ spark.serializer”，“ org.apache.spark.serializer.KryoSerializer”）来切换为使用Kryo。</p>
<p>要向Kryo注册您自己的自定义类，请使用registerKryoClasses方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 广播变量</title>
    <url>/2018/10/04/Spark%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/</url>
    <content><![CDATA[<h5 id="为什么要将变量定义成广播变量？"><a href="#为什么要将变量定义成广播变量？" class="headerlink" title="为什么要将变量定义成广播变量？"></a>为什么要将变量定义成广播变量？</h5><p>如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。</p>
<a id="more"></a>
<hr>
<h5 id="广播变量图解"><a href="#广播变量图解" class="headerlink" title="广播变量图解"></a>广播变量图解</h5><p>错误的，不使用广播变量<br><img src="/2018/10/04/Spark%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/%E4%B8%8D%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="不使用广播变量"></p>
<p>正确的，使用广播变量的情况<br><img src="/2018/10/04/Spark%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="使用广播变量"></p>
<h5 id="小表广播案例"><a href="#小表广播案例" class="headerlink" title="小表广播案例"></a>小表广播案例</h5><p>使用广播变量的场景很多， 我们都知道spark 一种常见的优化方式就是小表广播， 使用 map join 来代替 reduce join， 我们通过把小的数据集广播到各个节点上，节省了一次特别 expensive 的 shuffle 操作。</p>
<p>比如driver 上有一张数据量很小的表， 其他节点上的task 都需要 lookup 这张表， 那么 driver 可以先把这张表 copy 到这些节点，这样 task 就可以在本地查表了。</p>
<ol>
<li><p>Fact table 航线(起点机场, 终点机场, 航空公司, 起飞时间)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">SEA</span>,<span class="type">JFK</span>,<span class="type">DL</span>,<span class="number">7</span>:<span class="number">00</span></span><br><span class="line"><span class="type">SFO</span>,<span class="type">LAX</span>,<span class="type">AA</span>,<span class="number">7</span>:<span class="number">05</span></span><br><span class="line"><span class="type">SFO</span>,<span class="type">JFK</span>,<span class="type">VX</span>,<span class="number">7</span>:<span class="number">05</span></span><br><span class="line"><span class="type">JFK</span>,<span class="type">LAX</span>,<span class="type">DL</span>,<span class="number">7</span>:<span class="number">10</span></span><br><span class="line"><span class="type">LAX</span>,<span class="type">SEA</span>,<span class="type">DL</span>,<span class="number">7</span>:<span class="number">10</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Dimension table 机场(简称, 全称, 城市, 所处城市简称)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">JFK</span>,<span class="type">John</span> <span class="type">F</span>. <span class="type">Kennedy</span> <span class="type">International</span> <span class="type">Airport</span>,<span class="type">New</span> <span class="type">York</span>,<span class="type">NY</span></span><br><span class="line"><span class="type">LAX</span>,<span class="type">Los</span> <span class="type">Angeles</span> <span class="type">International</span> <span class="type">Airport</span>,<span class="type">Los</span> <span class="type">Angeles</span>,<span class="type">CA</span></span><br><span class="line"><span class="type">SEA</span>,<span class="type">Seattle</span>-<span class="type">Tacoma</span> <span class="type">International</span> <span class="type">Airport</span>,<span class="type">Seattle</span>,<span class="type">WA</span></span><br><span class="line"><span class="type">SFO</span>,<span class="type">San</span> <span class="type">Francisco</span> <span class="type">International</span> <span class="type">Airport</span>,<span class="type">San</span> <span class="type">Francisco</span>,<span class="type">CA</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Dimension table 航空公司(简称,全称)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">AA</span>,<span class="type">American</span> <span class="type">Airlines</span></span><br><span class="line"><span class="type">DL</span>,<span class="type">Delta</span> <span class="type">Airlines</span></span><br><span class="line"><span class="type">VX</span>,<span class="type">Virgin</span> <span class="type">America</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>思路：将机场维度表和航空公司维度表进行广播，生成Map，航线事实表从广播变量中通过key拿到value(计算在每个executor上)</p>
</li>
<li><p>代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcastApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Fact table  航线(起点机场, 终点机场, 航空公司, 起飞时间)</span></span><br><span class="line">        <span class="keyword">val</span> flights = sc.textFile(<span class="string">"tunan-spark-core/broadcast/flights.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Dimension table 机场(简称, 全称, 城市, 所处城市简称)</span></span><br><span class="line">        <span class="keyword">val</span> airports: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"tunan-spark-core/broadcast/airports.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Dimension table  航空公司(简称,全称)</span></span><br><span class="line">        <span class="keyword">val</span> airlines = sc.textFile(<span class="string">"tunan-spark-core/broadcast/airlines.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 最终统计结果：</span></span><br><span class="line"><span class="comment">         * 出发城市           终点城市        航空公司名称             起飞时间</span></span><br><span class="line"><span class="comment">         * Seattle           New York       Delta Airlines          7:00</span></span><br><span class="line"><span class="comment">         * San Francisco     Los Angeles    American Airlines       7:05</span></span><br><span class="line"><span class="comment">         * San Francisco     New York       Virgin America          7:05</span></span><br><span class="line"><span class="comment">         * New York          Los Angeles    Delta Airlines          7:10</span></span><br><span class="line"><span class="comment">         * Los Angeles       Seattle        Delta Airlines          7:10</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播Dimension Table airport，生成Map</span></span><br><span class="line">        <span class="keyword">val</span> airportsBC = sc.broadcast(airports.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">2</span>))</span><br><span class="line">        &#125;).collectAsMap())</span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播Dimension Table airlines，生成Map</span></span><br><span class="line">        <span class="keyword">val</span> airlinesBC = sc.broadcast(airlines.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).collectAsMap())</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过key获取value</span></span><br><span class="line">        flights.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> a = airportsBC.value.get(words(<span class="number">0</span>)).get</span><br><span class="line">            <span class="keyword">val</span> b = airportsBC.value.get(words(<span class="number">1</span>)).get</span><br><span class="line">            <span class="keyword">val</span> c = airlinesBC.value.get(words(<span class="number">2</span>)).get</span><br><span class="line">            a+<span class="string">"    "</span>+b+<span class="string">"    "</span>+c+<span class="string">"    "</span>+words(<span class="number">3</span>)</span><br><span class="line">        &#125;).foreach(println)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">New</span> <span class="type">York</span>        <span class="type">Los</span> <span class="type">Angeles</span>     <span class="type">Delta</span> <span class="type">Airlines</span>    <span class="number">7</span>:<span class="number">10</span></span><br><span class="line"><span class="type">Los</span> <span class="type">Angeles</span>     <span class="type">Seattl</span>          <span class="type">Delta</span> <span class="type">Airlines</span>    <span class="number">7</span>:<span class="number">10</span></span><br><span class="line"><span class="type">Seattle</span>         <span class="type">New</span> <span class="type">York</span>    	<span class="type">Delta</span> <span class="type">Airlines</span>    <span class="number">7</span>:<span class="number">00</span></span><br><span class="line"><span class="type">San</span> <span class="type">Francisco</span>   <span class="type">Los</span> <span class="type">Angeles</span>     <span class="type">American</span> <span class="type">Airlines</span> <span class="number">7</span>:<span class="number">05</span></span><br><span class="line"><span class="type">San</span> <span class="type">Francisco</span>   <span class="type">New</span> <span class="type">York</span>    	<span class="type">Virgin</span> <span class="type">America</span>    <span class="number">7</span>:<span class="number">05</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="为什么只能-broadcast-只读的变量"><a href="#为什么只能-broadcast-只读的变量" class="headerlink" title="为什么只能 broadcast 只读的变量"></a>为什么只能 broadcast 只读的变量</h5><p>这就涉及一致性的问题，如果变量可以被更新，那么一旦变量被某个节点更新，其他节点要不要一块更新？如果多个节点同时在更新，更新顺序是什么？怎么做同步？ 仔细想一下， 每个都很头疼， spark 目前就索性搞成了只读的。 因为分布式强一致性。</p>
<h5 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h5><ol>
<li><p>变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改</p>
</li>
<li><p>能不能将一个RDD使用广播变量广播出去？因为RDD是不存储数据的。可以将RDD的结果广播出去。</p>
</li>
<li><p>广播变量只能在Driver端定义，不能在Executor端定义。</p>
</li>
<li><p>在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。</p>
</li>
<li><p>如果executor端用到了Driver的变量，不使用广播变量在Executor有多少task就有多少Driver端的变量副本。</p>
</li>
<li><p>如果Executor端用到了Driver的变量，使用广播变量在每个Executor中只有一份Driver端的变量副本。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 核心概念RDD</title>
    <url>/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/</url>
    <content><![CDATA[<p>转载来源:<br><a href="http://sharkdtu.com/posts/spark-rdd.html" target="_blank" rel="noopener">http://sharkdtu.com/posts/spark-rdd.html</a></p>
<p>RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)，它是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。<br>基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，该DAG描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)一气呵成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。<br>另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系统，简单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。</p>
<a id="more"></a>
<hr>
<h4 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h4><p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。<br>RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p>
<h5 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h5><p>如下图所示，RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。<br>如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。<br><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/rdd-partition.png" alt="rdd partition"></p>
<h5 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h5><p>如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。<br><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/rdd-readonly.png" alt="rdd readonly"></p>
<p>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。<br><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/rdd-transform.png" alt="rdd transform"></p>
<p>RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。<br><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/rdd-transformations-actions.png" alt="rdd transformations actions"></p>
<h5 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h5><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。<br>如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。<br><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/rdd-dependency.png" alt="rdd dependency"></p>
<p>通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。<br><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/rdd-dag.png" alt="rdd dag"></p>
<h5 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h5><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。<br>如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。<br><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/rdd-cache.png" alt="rdd cache"></p>
<h5 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h5><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。<br>但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。<br>为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p>
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>总结起来，给定一个RDD我们至少可以知道如下几点信息：<br>1、分区数以及分区方式；<br>2、由父RDDs衍生而来的相关依赖信息；<br>3、计算每个分区的数据，计算步骤为：<br>    1）如果被缓存，则从缓存中取的分区的数据；<br>    2）如果被checkpoint，则从checkpoint处恢复数据；<br>    3）根据血缘关系计算分区的数据。</p>
<h4 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h4><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。<br>经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。<br>在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。</p>
<p>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。<br>Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。<br><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/spark-runtime.png" alt="spark runtime"></p>
<h5 id="应用举例"><a href="#应用举例" class="headerlink" title="应用举例"></a>应用举例</h5><p>下面介绍一个简单的spark应用程序实例WordCount，统计一个数据集中每个单词出现的次数，首先将从hdfs中加载数据得到原始RDD-0，其中每条记录为数据中的一行句子，经过一个flatMap操作，将一行句子切分为多个独立的词，得到RDD-1，再通过map操作将每个词映射为key-value形式，其中key为词本身，value为初始计数值1，得到RDD-2，将RDD-2中的所有记录归并，统计每个词的计数，得到RDD-3，最后将其保存到hdfs。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> <span class="type">SparkContext</span>._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: WordCount &lt;inputfile&gt; &lt;outputfile&gt;"</span>);</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> result = sc.textFile(args(<span class="number">0</span>))</span><br><span class="line">                   .flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line">                   .map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">                   .reduceByKey(_ + _)</span><br><span class="line">    result.saveAsTextFile(args(<span class="number">1</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/2018/09/04/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5RDD/spark-wordcount.png" alt="spark wordcount"></p>
<h5 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h5><p>基于RDD实现的Spark相比于传统的Hadoop MapReduce有什么优势呢？<br>总结起来应该至少有三点：<br>    1）RDD提供了丰富的操作算子，不再是只有map和reduce两个操作了，对于描述应用程序来说更加方便；<br>    2）通过RDDs之间的转换构建DAG，中间结果不用落地；<br>    3）RDD支持缓存，可以在内存中快速完成计算。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark RDD</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 持久化</title>
    <url>/2018/10/05/Spark%E6%8C%81%E4%B9%85%E5%8C%96/</url>
    <content><![CDATA[<h5 id="为什么要将变量定义成广播变量？"><a href="#为什么要将变量定义成广播变量？" class="headerlink" title="为什么要将变量定义成广播变量？"></a>为什么要将变量定义成广播变量？</h5><p>Spark中最重要的功能之一是跨操作在内存中持久化数据集。持久化一个RDD时，每个节点在内存中存储它计算的任何分区，并在该数据集(或从中派生的数据集)的其他操作中重构它们。这使得将来的操作要快得多(通常超过10倍)。缓存是迭代算法和快速交互使用的关键工具。</p>
<p>可以使用其上的persist()或cache()方法将RDD标记为持久的。第一次在操作中计算它时，它将保存在节点的内存中。Spark的缓存是容错的——如果一个RDD的任何分区丢失了，它将使用最初创建它的转换自动重新计算。</p>
<a id="more"></a>
<hr>
<p>持久化的存储级别很多，常用的是MEMORY_ONLY、MEMORY_ONLY_SER、MEMORY_AND_DISK<br><img src="/2018/10/05/Spark%E6%8C%81%E4%B9%85%E5%8C%96/%E5%AD%98%E5%82%A8%E7%BA%A7%E5%88%AB.png" alt="存储级别"></p>
<h5 id="如何选择它们？"><a href="#如何选择它们？" class="headerlink" title="如何选择它们？"></a>如何选择它们？</h5><p>Storage Level的选择是内存和CPU的权衡</p>
<ol>
<li>内存多：MEMORY_ONLY (不进行序列化)</li>
<li>CPU跟的上：MEMORY_ONLY_SER (进行了序列化，推介)</li>
<li>不建议写Disk<br>使用cache()和persist()进行持久化操作，它们都是lazy的，需要action才能触发，默认使用MEMORY_ONLY。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; forRDD.cache</span><br><span class="line">res18: forRDD<span class="class">.<span class="keyword">type</span> </span>= <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; forRDD.count</span><br><span class="line">res19: <span class="type">Long</span> = <span class="number">8</span></span><br></pre></td></tr></table></figure>

<p>结果可以在Web UI的Storage中查看</p>
<p>如果需要清除缓存，使用unpersist()，清除缓存数据是立即执行的</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; forRDD.unpersist()</span><br><span class="line">res8: forRDD<span class="class">.<span class="keyword">type</span> </span>= <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">28</span></span><br></pre></td></tr></table></figure>

<p>怎么修改存储级别？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="comment">//计数器做累加</span></span><br><span class="line">    acc.add(<span class="number">1</span>L)</span><br><span class="line">&#125;).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br></pre></td></tr></table></figure>

<p>StorageLevel是个object，需要的级别都可以从里面拿出来</p>
<p>考点：cache和persist有什么区别？</p>
<ul>
<li>cache调用的persist，persist调用的persist(storage level)<br>考点：序列化和非序列化有什么区别？</li>
<li>序列化将对象转换成字节数组了，节省空间，占CPU</li>
</ul>
<h5 id="Removing-Data"><a href="#Removing-Data" class="headerlink" title="Removing Data"></a>Removing Data</h5><p>Spark自动监视每个节点上的缓存使用情况，并以最近最少使用(LRU)的方式删除旧的数据分区。如果想要手动删除一个RDD，而不是等待它从缓存中消失，那么可以使用RDD.unpersist()方法。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 经典案例</title>
    <url>/2018/10/01/Spark%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/</url>
    <content><![CDATA[<p>一个Spark的经典案例</p>
<a id="more"></a>
<hr>
<h5 id="经典案例"><a href="#经典案例" class="headerlink" title="经典案例"></a>经典案例</h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 用户     节目            展示 点击</span></span><br><span class="line"><span class="comment"> * 001,一起看|电视剧|军旅|亮剑,1,1</span></span><br><span class="line"><span class="comment"> * 001,一起看|电视剧|军旅|亮剑,1,0</span></span><br><span class="line"><span class="comment"> * 002,一起看|电视剧|军旅|士兵突击,1,1</span></span><br><span class="line"><span class="comment"> * ==&gt;</span></span><br><span class="line"><span class="comment"> * 001,一起看,2,1</span></span><br><span class="line"><span class="comment"> * 001,电视剧,2,1</span></span><br><span class="line"><span class="comment"> * 001,军旅,2,1</span></span><br><span class="line"><span class="comment"> * 001,亮剑,2,1</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">exercise02</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"tunan-spark-core/data/test2.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用map返回的是一个数组，我不要数组，就使用flatMap</span></span><br><span class="line">        <span class="keyword">import</span> com.tunan.spark.utils.<span class="type">ImplicitAspect</span>.rdd2RichRDD</span><br><span class="line">        <span class="keyword">val</span> map2RDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = linesRDD.flatMap(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> programs: <span class="type">Array</span>[<span class="type">String</span>] = words(<span class="number">1</span>).split(<span class="string">"\\|"</span>)</span><br><span class="line">            <span class="keyword">val</span> mapRDD: <span class="type">Array</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = programs.map(program =&gt; ((words(<span class="number">0</span>), program), (words(<span class="number">2</span>).toInt, words(<span class="number">3</span>).toInt)))</span><br><span class="line">            mapRDD</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Iterable</span>[(<span class="type">Int</span>, <span class="type">Int</span>)])] = map2RDD.groupByKey()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//这里是mapValues很好的一个使用案例</span></span><br><span class="line">        <span class="keyword">val</span> mapVRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = groupRDD.mapValues(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> imps: <span class="type">Int</span> = x.map(_._1).sum</span><br><span class="line">            <span class="keyword">val</span> check: <span class="type">Int</span> = x.map(_._2).sum</span><br><span class="line">            (imps, check)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//格式化输出</span></span><br><span class="line">        mapVRDD.map(x =&gt; &#123;</span><br><span class="line">            (x._1._1,x._1._2,x._2._1,x._2._1)</span><br><span class="line">        &#125;).print()</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark算子</tag>
      </tags>
  </entry>
  <entry>
    <title>Tableau 备份还原</title>
    <url>/2021/03/23/Tableau%E5%A4%87%E4%BB%BD%E8%BF%98%E5%8E%9F/</url>
    <content><![CDATA[<p>对Linux Tableau Server 进行备份、还原操作。</p>
<a id="more"></a>
<hr>
<p><a href="https://help.tableau.com/current/server-linux/zh-cn/backup_restore.htm" target="_blank" rel="noopener">Tableau 备份-还原 官方文档</a></p>
<h4 id="Tableau-备份"><a href="#Tableau-备份" class="headerlink" title="Tableau 备份"></a>Tableau 备份</h4><p>Tableau Server 可以生成两种类型的备份数据。</p>
<p>如果必须在恢复方案中还原服务器，建议对两种类型都执行定期备份：</p>
<h5 id="Tableau-Server-管理的数据"><a href="#Tableau-Server-管理的数据" class="headerlink" title="Tableau Server 管理的数据"></a>Tableau Server 管理的数据</h5><p>Tableau Server 管理的数据包含 Tableau PostgreSQL 数据库或存储库以及文件存储，其中包含工作簿和用户元数据、数据提取文件以及配置数据。使用 TSM 创建备份时，所有这些数据都保存在一个扩展名为 <code>.tsbak</code>的文件中。</p>
<p>管理的数据使用以下命令进行备份，扩展名为<code>.tsbak</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm maintenance backup -f &lt;filename&gt;.tsbak</span><br><span class="line">/var/opt/tableau/tableau_server/data/tabsvc/files/backups/&lt;filename&gt;.tsbak</span><br></pre></td></tr></table></figure>

<h5 id="配置和拓扑数据"><a href="#配置和拓扑数据" class="headerlink" title="配置和拓扑数据"></a>配置和拓扑数据</h5><p>包括完全恢复服务器所需的大多数服务器配置信息。SMTP、通知、某些身份验证资产都是可导出进行备份的配置数据的示例。拓扑数据定义 Tableau Server 进程在单服务器和多节点部署中的配置方式。</p>
<p>配置和拓扑数据使用以下命令进行备份，命令生成的<code>.json</code> 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm settings export -f &lt;filename&gt;.json</span><br></pre></td></tr></table></figure>



<h4 id="Tableau-还原"><a href="#Tableau-还原" class="headerlink" title="Tableau 还原"></a>Tableau 还原</h4><h5 id="还原拓扑和配置数据"><a href="#还原拓扑和配置数据" class="headerlink" title="还原拓扑和配置数据"></a>还原拓扑和配置数据</h5><p>将拓扑和配置 json 备份文件复制到计算机。</p>
<p>通过运行以下命令来导入 <code>.json</code> 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm settings import -f &lt;filename&gt;.json</span><br></pre></td></tr></table></figure>

<p>重启Tableau </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm pending-changes apply</span><br></pre></td></tr></table></figure>

<h5 id="从备份文件还原-Tableau-Server"><a href="#从备份文件还原-Tableau-Server" class="headerlink" title="从备份文件还原 Tableau Server"></a>从备份文件还原 Tableau Server</h5><h6 id="将-tsbak-文件复制到默认文件位置"><a href="#将-tsbak-文件复制到默认文件位置" class="headerlink" title="将 .tsbak 文件复制到默认文件位置"></a>将 <code>.tsbak</code> 文件复制到默认文件位置</h6><p><code>restore</code>命令需要 TSM <code>basefilepath.backuprestore</code> 变量定义的目录中有备份文件。</p>
<p>默认文件位置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/var/opt/tableau/tableau_server/data/tabsvc/files/backups/</span><br></pre></td></tr></table></figure>

<h6 id="停止服务器"><a href="#停止服务器" class="headerlink" title="停止服务器"></a>停止服务器</h6><p>在命令提示符处，键入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm stop</span><br></pre></td></tr></table></figure>

<h6 id="从备份文件进行还原。"><a href="#从备份文件进行还原。" class="headerlink" title="从备份文件进行还原。"></a>从备份文件进行还原。</h6><p>在命令提示符处，键入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm maintenance restore --file &lt;file_name&gt;</span><br></pre></td></tr></table></figure>

<h6 id="重新启动服务器"><a href="#重新启动服务器" class="headerlink" title="重新启动服务器"></a>重新启动服务器</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm start</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 计数器</title>
    <url>/2018/10/02/Spark%E8%AE%A1%E6%95%B0%E5%99%A8/</url>
    <content><![CDATA[<h5 id="为什么要定义计数器？"><a href="#为什么要定义计数器？" class="headerlink" title="为什么要定义计数器？"></a>为什么要定义计数器？</h5><p>在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。</p>
<a id="more"></a>
<hr>
<h5 id="图解计数器"><a href="#图解计数器" class="headerlink" title="图解计数器"></a>图解计数器</h5><p>错误的图解<br><img src="/2018/10/02/Spark%E8%AE%A1%E6%95%B0%E5%99%A8/%E7%B4%AF%E5%8A%A0%E5%99%A8%E9%94%99%E8%AF%AF%E7%9A%84%E5%9B%BE%E8%A7%A3.png" alt="累加器错误的图解"></p>
<p>正确的图解<br><img src="/2018/10/02/Spark%E8%AE%A1%E6%95%B0%E5%99%A8/%E7%B4%AF%E5%8A%A0%E5%99%A8%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%9B%BE%E8%A7%A3.png" alt="累加器正确的图解"></p>
<p>计数器种类很多，但是经常用的就是两种，longAccumulator和collectionAccumulator</p>
<p><strong>需要注意的是计数器是lazy的，只有触发action才会进行计数，在不持久化的情况下重复触发action，计数器会重复累加</strong></p>
<h5 id="LongAccumulator"><a href="#LongAccumulator" class="headerlink" title="LongAccumulator"></a>LongAccumulator</h5><p>Accumulators 是只能通过associative和commutative操作“added”的变量，因此可以有效地并行支持。它们可用于实现计数器(如MapReduce)和Spark本身支持数字类型的累加器，程序员还可以添加对新类型的支持。</p>
<p>longAccumulator通过累加的方式计数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="keyword">var</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">// 计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// action操作 </span></span><br><span class="line">        forRDD.count()</span><br><span class="line">       </span><br><span class="line">        println(acc.value)	<span class="comment">// 9</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用longAccumulator做计数的时候要小心重复执行action导致的acc.value的变化</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulatorV2</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="comment">//生成计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">//计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)	<span class="comment">//8</span></span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)	<span class="comment">//16</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由于重复执行了count()，累加器的数量成倍增长，解决这种错误累加也很简单，就是在count之前调用forRDD的cache方法(或persist)，这样在count后数据集就会被缓存下来，reduce操作就会读取缓存的数据集，而无需从头开始计算。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="comment">//生成计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">//计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        forRDD.cache().count()</span><br><span class="line">        println(acc.value)	<span class="comment">//8</span></span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)	<span class="comment">//8</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="CollectionAccumulator"><a href="#CollectionAccumulator" class="headerlink" title="CollectionAccumulator"></a>CollectionAccumulator</h5><p>collectionAccumulator，集合计数器，计数器中保存的是集合元素，通过泛型指定。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：id后三位相同的加入计数器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyCollectionAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc  = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成集合计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.collectionAccumulator[<span class="type">People</span>](<span class="string">"集合计数器"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成RDD</span></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">People</span>] = sc.parallelize(<span class="type">Array</span>(<span class="type">People</span>(<span class="string">"tunan"</span>, <span class="number">100000</span>), <span class="type">People</span>(<span class="string">"xiaoqi"</span>, <span class="number">100001</span>), <span class="type">People</span>(<span class="string">"张三"</span>, <span class="number">100222</span>), <span class="type">People</span>(<span class="string">"李四"</span>, <span class="number">100003</span>)))</span><br><span class="line"></span><br><span class="line"> 		<span class="comment">//map操作</span></span><br><span class="line">        rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> id2 = x.id.toString.reverse</span><br><span class="line">            <span class="comment">//满足条件就加入计数器，</span></span><br><span class="line">            <span class="keyword">if</span> (id2(<span class="number">0</span>) == id2(<span class="number">1</span>) &amp;&amp; id2(<span class="number">0</span>) ==id2(<span class="number">2</span>))&#123;</span><br><span class="line">                acc.add(x)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).count()	<span class="comment">//触发action</span></span><br><span class="line"></span><br><span class="line">        println(acc.value)	<span class="comment">//[People(张三,100222), People(tunan,100000)]</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>,id:<span class="type">Long</span></span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意事项：</p>
<ol>
<li><p>计数器在Driver端定义赋初始值，计数器只能在Driver端读取最后的值，在Excutor端更新。</p>
</li>
<li><p>计数器不是一个调优的操作，因为如果不这样做，结果是错的</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Tableau 移除</title>
    <url>/2021/03/24/Tableau%E7%A7%BB%E9%99%A4/</url>
    <content><![CDATA[<p>对Linux Tableau Server 进行移除操作。</p>
<a id="more"></a>
<hr>
<p><a href="https://help.tableau.com/current/server-linux/zh-cn/remove_tableau.htm" target="_blank" rel="noopener">Tableau 移除 官方文档</a></p>
<h4 id="Tableau-许可移除"><a href="#Tableau-许可移除" class="headerlink" title="Tableau 许可移除"></a>Tableau 许可移除</h4><p>停用任何活动产品密钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm licenses deactivate -k &lt;product_key&gt;</span><br></pre></td></tr></table></figure>

<h4 id="Tableau-Server移除"><a href="#Tableau-Server移除" class="headerlink" title="Tableau Server移除"></a>Tableau Server移除</h4><p>运行 <code>tableau-server-obliterate</code> 脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo /opt/tableau/tableau_server/packages/scripts.&lt;version_code&gt;/ tableau-server-obliterate -a -y -y -y -l</span><br></pre></td></tr></table></figure>

<p>如果有 Tableau Server 的多节点（分布式）安装，请在群集中的每个节点上运行 <code>tableau-server-obliterate</code> 脚本。您无需在任何其他节点上停用许可证。</p>
]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title>UBT 用户行为分析平台 背景描述</title>
    <url>/2021/06/11/UBT%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%E8%83%8C%E6%99%AF%E6%8F%8F%E8%BF%B0/</url>
    <content><![CDATA[<p>UBT (User Behaviour Track) 用户行为分析平台 </p>
<p>UBT 的项目成立的最原始目的是替换当前使用的收费平台（易观）</p>
<a id="more"></a>

<hr>
<p>以下是这个项目的解决方案：</p>
<p>UBT 项目完全参考易观的项目架构来进行开发</p>
<h4 id="项目分为两个阶段来完成"><a href="#项目分为两个阶段来完成" class="headerlink" title="项目分为两个阶段来完成"></a>项目分为两个阶段来完成</h4><h5 id="一阶段："><a href="#一阶段：" class="headerlink" title="一阶段："></a>一阶段：</h5><p>​    使用易观SDK上报的数据，完成数据的ETL扁平化、分发、用户前端交互界面</p>
<h5 id="二阶段："><a href="#二阶段：" class="headerlink" title="二阶段："></a>二阶段：</h5><p>​    深入了解易观SDK埋点，替换易观SDK采集，优化用户前端交互界面</p>
<h4 id="一阶段架构"><a href="#一阶段架构" class="headerlink" title="一阶段架构:"></a>一阶段架构:</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">易观Kafka Post Topic -&gt; CDH Kafka Topic -&gt; Flink ETL -&gt; Kafka Doris Routine Load Topic </span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Doris Routine Load Task -&gt; Doris Event/Profile Table</span></span><br></pre></td></tr></table></figure>

<h5 id="1-数据采集"><a href="#1-数据采集" class="headerlink" title="1.数据采集"></a>1.数据采集</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">易观Kafka数据使用 Flume 采集到 CDH 集群 Kafka</span><br><span class="line">不直接从易观 Kafka 消费是因为，易观的 Kafka 版本为 0.8.*，Flink 不能与这么低版本的 Kafka 对接，所以使用 Flume 进行了数据的转发。</span><br><span class="line">需要注意的是，Flume 的版本为1.6.0，这是 Flume 能够对接 Kafka 0.8.* 的最高版本。</span><br></pre></td></tr></table></figure>

<h5 id="2-数据清洗"><a href="#2-数据清洗" class="headerlink" title="2.数据清洗"></a>2.数据清洗</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">数据清洗的部分主要有两个动作：</span><br><span class="line">1. 将易观数据按照用户和事件两个主题进行拆分</span><br><span class="line">2. 拆分后的Json数据扁平化，便于下游数据的导入</span><br></pre></td></tr></table></figure>

<h5 id="3-数据导入"><a href="#3-数据导入" class="headerlink" title="3.数据导入"></a>3.数据导入</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使用 Doris Routine Load Task 进行数据的导入，目标表有两个：</span><br><span class="line">1. Event Table，Doris 明细模型表，直接进行导入即可</span><br><span class="line">2. Profile Table，Doris Unique 模型表，需要注意消息的乱序问题，建表时需要设置Sequence，Routine Load 需要配置排序字段</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>UBT 用户行为分析平台</category>
      </categories>
      <tags>
        <tag>UBT 用户行为分析平台</tag>
      </tags>
  </entry>
  <entry>
    <title>Tableau 部署</title>
    <url>/2021/03/22/Tableau%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>在一台机器上进行Tableau的部署，以下是部署Tableau的具体步骤。</p>
<a id="more"></a>
<hr>
<p><a href="https://www.tableau.com/zh-cn/support/releases/server" target="_blank" rel="noopener">Tableau 下载地址</a></p>
<h4 id="Tableau-部署"><a href="#Tableau-部署" class="headerlink" title="Tableau 部署"></a>Tableau 部署</h4><h5 id="Tableau-安装"><a href="#Tableau-安装" class="headerlink" title="Tableau 安装"></a>Tableau 安装</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum update</span><br><span class="line">sudo yum install tableau-server-2020-3-3.x86_64.rpm</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/22/Tableau%E9%83%A8%E7%BD%B2/image-20210322171708121.png" alt="Tableau 安装完成"></p>
<h5 id="Tableau-初始化tsm"><a href="#Tableau-初始化tsm" class="headerlink" title="Tableau 初始化tsm"></a>Tableau 初始化tsm</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /opt/tableau/tableau_server/packages/scripts.20203.20.1110.1623/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加新的用户</span></span><br><span class="line">useradd hmdadmin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 给新增用户创建密码</span></span><br><span class="line">passwd hmdadmin</span><br><span class="line"><span class="meta">dmha#</span><span class="bash">dmin</span></span><br><span class="line"></span><br><span class="line">./initialize-tsm --accepteula -a hmdadmin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更改用户</span></span><br><span class="line">source /etc/profile.d/tableau_server.sh</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/22/Tableau%E9%83%A8%E7%BD%B2/image-20210322171620100.png" alt="增加新的用户"></p>
<h5 id="修改防火墙配置"><a href="#修改防火墙配置" class="headerlink" title="修改防火墙配置"></a>修改防火墙配置</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start firewalld</span><br><span class="line">firewall-cmd --get-default-zone</span><br><span class="line">firewall-cmd --set-default-zone&#x3D;public</span><br><span class="line">firewall-cmd --permanent --add-port&#x3D;80&#x2F;tcp</span><br><span class="line">firewall-cmd --permanent --add-port&#x3D;8850&#x2F;tcp</span><br><span class="line">firewall-cmd --reload</span><br><span class="line">firewall-cmd --list-all</span><br><span class="line">grep tsmadmin &#x2F;etc&#x2F;group</span><br><span class="line">usermod -G tsmadmin -a hmdadmin</span><br></pre></td></tr></table></figure>



<h5 id="tsm-Web管理员配置"><a href="#tsm-Web管理员配置" class="headerlink" title="tsm Web管理员配置"></a>tsm Web管理员配置</h5><p><a href="https://host:8850/" target="_blank" rel="noopener">https://host:8850/</a></p>
<p><img src="/2021/03/22/Tableau%E9%83%A8%E7%BD%B2/image-20210322171442857.png" alt="Tableau Web 配置"></p>
<p><img src="/2021/03/22/Tableau%E9%83%A8%E7%BD%B2/image-20210322171929820.png" alt="Tableau Web 配置完成"></p>
<h5 id="添加管理员账号"><a href="#添加管理员账号" class="headerlink" title="添加管理员账号"></a>添加管理员账号</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tabcmd initialuser --server http:&#x2F;&#x2F;localhost --username &#39;hmdadmin&#39;</span><br></pre></td></tr></table></figure>



<h5 id="缓存级别设置"><a href="#缓存级别设置" class="headerlink" title="缓存级别设置"></a>缓存级别设置</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tsm data-access caching set -r 30</span><br></pre></td></tr></table></figure>



<h4 id="数据源安装"><a href="#数据源安装" class="headerlink" title="数据源安装"></a>数据源安装</h4><h5 id="安装MySQL数据源"><a href="#安装MySQL数据源" class="headerlink" title="安装MySQL数据源"></a>安装MySQL数据源</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install mysql-connector-odbc-5.3.13-1.el7.x86_64.rpm</span><br><span class="line">sudo yum install mysql-connector-odbc-8.0.20-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>



<h5 id="安装postgresql数据源"><a href="#安装postgresql数据源" class="headerlink" title="安装postgresql数据源"></a>安装postgresql数据源</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install tableau-postgresql-odbc-09.06.0500-1.x86_64.rpm</span><br></pre></td></tr></table></figure>



<h5 id="安装Hive数据源"><a href="#安装Hive数据源" class="headerlink" title="安装Hive数据源"></a>安装Hive数据源</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -ivh ClouderaHiveODBC-2.6.9.1009-1.x86_64.rpm</span><br></pre></td></tr></table></figure>



<h5 id="数据源安装完成后，确认odbcinst-ini-信息"><a href="#数据源安装完成后，确认odbcinst-ini-信息" class="headerlink" title="数据源安装完成后，确认odbcinst.ini 信息"></a>数据源安装完成后，确认odbcinst.ini 信息</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/odbcinst.ini</span><br><span class="line"></span><br><span class="line">[ODBC Drivers]</span><br><span class="line">PostgreSQL Unicode=Installed</span><br><span class="line"></span><br><span class="line">[PostgreSQL]</span><br><span class="line">Description=ODBC for PostgreSQL</span><br><span class="line">Driver=/usr/lib/psqlodbcw.so</span><br><span class="line">Setup=/usr/lib/libodbcpsqlS.so</span><br><span class="line">Driver64=/usr/lib64/psqlodbcw.so</span><br><span class="line">Setup64=/usr/lib64/libodbcpsqlS.so</span><br><span class="line">FileUsage=1</span><br><span class="line"></span><br><span class="line">[MySQL]</span><br><span class="line">Description=ODBC for MySQL</span><br><span class="line">Driver=/usr/lib/libmyodbc5.so</span><br><span class="line">Setup=/usr/lib/libodbcmyS.so</span><br><span class="line">Driver64=/usr/lib64/libmyodbc5.so</span><br><span class="line">Setup64=/usr/lib64/libodbcmyS.so</span><br><span class="line">FileUsage=1</span><br><span class="line"></span><br><span class="line">[MySQL ODBC 8.0 Unicode Driver]</span><br><span class="line">Driver=/usr/lib64/libmyodbc8w.so</span><br><span class="line">UsageCount=1</span><br><span class="line"></span><br><span class="line">[MySQL ODBC 8.0 ANSI Driver]</span><br><span class="line">Driver=/usr/lib64/libmyodbc8a.so</span><br><span class="line">UsageCount=1</span><br><span class="line"></span><br><span class="line">[Cloudera ODBC Driver for Apache Hive 64-bit]</span><br><span class="line">Description=Cloudera ODBC Driver for Apache Hive (64-bit)</span><br><span class="line">Driver=/opt/cloudera/hiveodbc/lib/64/libclouderahiveodbc64.so</span><br><span class="line"></span><br><span class="line">[PostgreSQL Unicode]</span><br><span class="line">Description=ODBC for PostgreSQL</span><br><span class="line">Driver=/opt/tableau/tableau_driver/postgresql-odbc/psqlodbcw.so</span><br><span class="line">FileUsage=1</span><br></pre></td></tr></table></figure>



<h4 id="中文乱码问题修复"><a href="#中文乱码问题修复" class="headerlink" title="中文乱码问题修复"></a>中文乱码问题修复</h4><h5 id="确认安装的字体，如果没有宋体黑体表示需要安装"><a href="#确认安装的字体，如果没有宋体黑体表示需要安装" class="headerlink" title="确认安装的字体，如果没有宋体黑体表示需要安装"></a>确认安装的字体，如果没有宋体黑体表示需要安装</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fc-list</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/22/Tableau%E9%83%A8%E7%BD%B2/image-20210322181348023.png" alt="当前已有字体确认"></p>
<h5 id="安装字体管理工具"><a href="#安装字体管理工具" class="headerlink" title="安装字体管理工具"></a>安装字体管理工具</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install fontconfig mkfontscale -y</span><br></pre></td></tr></table></figure>



<h5 id="建立中文字体目录"><a href="#建立中文字体目录" class="headerlink" title="建立中文字体目录"></a>建立中文字体目录</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir -p /usr/share/fonts/chinese</span><br></pre></td></tr></table></figure>



<h5 id="将windows系统中字体上传到上一步建立的目录，windows字体目录在c-windows-fonts"><a href="#将windows系统中字体上传到上一步建立的目录，windows字体目录在c-windows-fonts" class="headerlink" title="将windows系统中字体上传到上一步建立的目录，windows字体目录在c:\windows\fonts"></a>将windows系统中字体上传到上一步建立的目录，windows字体目录在c:\windows\fonts</h5><p><img src="/2021/03/22/Tableau%E9%83%A8%E7%BD%B2/image2020-12-29_10-20-19.png" alt="windows系统字体"></p>
<h5 id="建立缓存字体，生成字库索引信息"><a href="#建立缓存字体，生成字库索引信息" class="headerlink" title="建立缓存字体，生成字库索引信息"></a>建立缓存字体，生成字库索引信息</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/share/fonts/chinese/</span><br><span class="line">sudo mkfontscale</span><br><span class="line">sudo mkfontdir</span><br><span class="line">sudo fc-cache</span><br></pre></td></tr></table></figure>



<h5 id="再次查看系统中已安装的中文字体，重启Tableau"><a href="#再次查看系统中已安装的中文字体，重启Tableau" class="headerlink" title="再次查看系统中已安装的中文字体，重启Tableau"></a>再次查看系统中已安装的中文字体，重启Tableau</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fc-list</span><br><span class="line">tsm restart</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/22/Tableau%E9%83%A8%E7%BD%B2/image-20210322182122121.png" alt="确认字体、重启Tableau"></p>
]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title>单节点部署三台Kafka</title>
    <url>/2019/04/12/%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2%E4%B8%89%E5%8F%B0Kafka/</url>
    <content><![CDATA[<p>单节点部署三台Kafka、启动及测试</p>
<a id="more"></a>
<hr>
<h4 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h4><p><a href="http://archive.cloudera.com/kafka/kafka/4/kafka-2.2.1-kafka4.1.0.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/kafka/kafka/4/kafka-2.2.1-kafka4.1.0.tar.gz</a></p>
<h4 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka-2.2.1-kafka4.1.0.tar.gz -C ../app</span><br></pre></td></tr></table></figure>
<h4 id="部署Kafka之前，检测Zookeeper是ok的"><a href="#部署Kafka之前，检测Zookeeper是ok的" class="headerlink" title="部署Kafka之前，检测Zookeeper是ok的"></a>部署Kafka之前，检测Zookeeper是ok的</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[zktest, tunan, zookeeper, kafka]</span><br></pre></td></tr></table></figure>
<h4 id="编辑config-server-properties文件"><a href="#编辑config-server-properties文件" class="headerlink" title="编辑config/server.properties文件"></a>编辑config/server.properties文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">broker.id=0</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.dirs=/tmp/kafka-logs</span></span><br><span class="line"></span><br><span class="line">broker.id=0</span><br><span class="line">host.name=hadoop</span><br><span class="line">port=9090</span><br><span class="line">log.dirs=/home/hadoop/tmp/kafka-logs00</span><br><span class="line">zookeeper.connect=hadoop:2181/kafka</span><br></pre></td></tr></table></figure>
<h4 id="复制Kafka文件夹为三份"><a href="#复制Kafka文件夹为三份" class="headerlink" title="复制Kafka文件夹为三份"></a>复制Kafka文件夹为三份</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp -R kafka_2.11-2.2.1-kafka-4.1.0/ kafka01</span><br><span class="line">cp -R kafka_2.11-2.2.1-kafka-4.1.0/ kafka02</span><br><span class="line">mv  kafka_2.11-2.2.1-kafka-4.1.0/ kafka03</span><br></pre></td></tr></table></figure>
<h4 id="修改kafka02的config-server-properties文件"><a href="#修改kafka02的config-server-properties文件" class="headerlink" title="修改kafka02的config/server.properties文件"></a>修改kafka02的config/server.properties文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">broker.id=2</span><br><span class="line">host.name=hadoop</span><br><span class="line">port=9092</span><br><span class="line">log.dirs=/home/hadoop/tmp/kafka-logs02</span><br><span class="line">zookeeper.connect=hadoop:2181/kafka</span><br></pre></td></tr></table></figure>
<h4 id="启动-三台都需要输入命令"><a href="#启动-三台都需要输入命令" class="headerlink" title="启动(三台都需要输入命令)"></a>启动(三台都需要输入命令)</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure>
<h4 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh \</span><br><span class="line">--create \</span><br><span class="line">--zookeeper hadoop:2181/kafka \</span><br><span class="line">--partitions 3 \</span><br><span class="line">--replication-factor 2 \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure>
<h4 id="查看指定topic的状况"><a href="#查看指定topic的状况" class="headerlink" title="查看指定topic的状况"></a>查看指定topic的状况</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh \</span><br><span class="line">--describe \</span><br><span class="line">--zookeeper hadoop:2181/kafka \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>启动生产者</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-console-producer.sh \</span><br><span class="line">--broker-list hadoop:9090,hadoop:9091,hadoop:9092 \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure>
<p>启动消费者</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server hadoop:9090,hadoop:9091,hadoop:9092 \</span><br><span class="line">--from-beginning \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure>
<p>发送数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">a</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">a</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>接收数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a</span><br><span class="line">a</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h4 id="关闭kafka集群-每台都要执行"><a href="#关闭kafka集群-每台都要执行" class="headerlink" title="关闭kafka集群(每台都要执行)"></a>关闭kafka集群(每台都要执行)</h4><p>修改kafka-server-stop.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')</span><br><span class="line">修改为</span><br><span class="line">PIDS=$(jps -lm | grep -i 'kafka'| awk '&#123;print $1&#125;')</span><br></pre></td></tr></table></figure>
<p>命令详解：<br>使用jps -lm命令列出所有的java进程，然后通过管道，利用grep -i ‘kafka.Kafka’命令将kafka进程筛出来，最后再接一管道命令，利用awk将进程号取出来。</p>
<p>分别执行kafka-server-stop.sh</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Yarn 调度流程</title>
    <url>/2018/06/18/yarn%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<h4 id="Yarn-调度流程"><a href="#Yarn-调度流程" class="headerlink" title="Yarn 调度流程"></a>Yarn 调度流程</h4><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p>
<a id="more"></a>
<hr>
<h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><p>官网原文：</p>
<p>The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system.</p>
<p>The ResourceManager has two main components: Scheduler and ApplicationsManager.</p>
<p>RM组成：</p>
<ul>
<li>Application Manager 应用程序管理器</li>
<li>Resource Scheduler memory+cpu资源调度器</li>
</ul>
<h4 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h4><p>官网原文：</p>
<p>The NodeManager is responsible for launching and managing containers on a node. Containers execute tasks as specified by the AppMaster.</p>
<p>NM组成：</p>
<ul>
<li>container - 虚拟概念，执行MR、Spark计算任务的最小单元</li>
</ul>
<h4 id="MR-on-YARN-调度流程"><a href="#MR-on-YARN-调度流程" class="headerlink" title="MR on YARN 调度流程"></a>MR on YARN 调度流程</h4><p><img src="/2018/06/18/yarn%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B/mr-on-yarn.png" alt="MR on YARN 调度流程"></p>
<h5 id="MR-on-YARN-调度过程描述："><a href="#MR-on-YARN-调度过程描述：" class="headerlink" title="MR on YARN 调度过程描述："></a>MR on YARN 调度过程描述：</h5><ol>
<li>用户向YARN提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令等。</li>
<li>RM为该job分配第一个container，运行job的ApplicationMaster。</li>
<li>App Master向Application Manager注册，这样就可以在RM WEB界面查询这个job的运行状态。</li>
<li>App Master采用轮询的方式通过RPC协议向RM申请和领取资源。</li>
<li>一旦App Master拿到资源，就对应的与NM通信，要求启动任务。</li>
<li>NM为任务设置好运行环境（jar包等），将任务启动命令写在一个脚本里，并通过该脚本启动任务task。</li>
<li>各个task通过RPC协议向App Master汇报自己的状态和进度，以此让App Master随时掌握各个task的运行状态，从而在task运行失败后重启任务。</li>
<li>App Master向Application Manager注销且关闭自己。</li>
</ol>
<p>总的来说就是两步：</p>
<ul>
<li>启动App Master，申请资源；</li>
<li>运行任务，直到任务运行完成。</li>
</ul>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>基本概念</tag>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title>数据中台_04-元数据中心的关键目标和技术实现方案</title>
    <url>/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/</url>
    <content><![CDATA[<p>在上一篇中，我们从宏观的角度，系统性地了解了数据中台建设的方法论、支撑技术和组织架构，从这篇开始，我们正式进入实现篇，我会从微观的角度出发，带你具体分析数据中台的支撑技术，以电商场景为例，分别讲解元数据中心、指标管理、模型设计、数据质量等技术如何在企业落地。</p>
<p>现在，咱们来聊聊元数据。</p>
<p>为什么要先讲元数据呢？我来举个例子。在原理篇中，我提到数据中台的构建，需要确保全局指标的业务口径一致，要把原先口径不一致的、重复的指标进行梳理，整合成一个统一的指标字典。而这项工作的前提，是要搞清楚这些指标的业务口径、数据来源和计算逻辑。而这些数据呢都是元数据。</p>
<p>你可以认为，如果没有这些元数据，就没法去梳理指标，更谈不上构建一个统一的指标体系。当你看到一个数 700W，如果你不知道这个数对应的指标是每日日活，就没办法理解这个数据的业务含义，也就无法去整合这些数据。<strong>所以你必须要掌握元数据的管理，才能构建一个数据中台。</strong></p>
<p>那么问题来了：元数据中心应该包括哪些元数据呢？ 什么样的数据是元数据？</p>
<a id="more"></a>
<hr>
<h2 id="元数据包括哪些？"><a href="#元数据包括哪些？" class="headerlink" title="元数据包括哪些？"></a>元数据包括哪些？</h2><p>结合我的实践经验，我把元数据划为三类：数据字典、数据血缘和数据特征。我们还是通过一个例子来理解这三类元数据。</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/DEMO1.png" alt="DEMO1"></p>
<p>在这个图中，dwd_trd_order_df 是一张订单交易明细数据，任务 flow_dws_trd_sku_1d 读取这张表，按照 sku 粒度，计算每日 sku 的交易金额和订单数量，输出轻度汇总表 dws_trd_sku_1d。</p>
<p>数据字典描述的是数据的结构信息，我们以 dws_trd_sku_1d 为例，数据字典包括：</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E6%95%B0%E6%8D%AE%E5%AD%97%E5%85%B8%E5%86%85%E5%AE%B9.png" alt="数据字典内容"></p>
<p>数据血缘是指一个表是直接通过哪些表加工而来，在上面的例子中，dws_trd_sku_1d 是通过 dwd_trd_order_df 的数据计算而来，所以，dwd_trd_order_df 是 dws_trd_sku_1d 的上游表。</p>
<p>数据血缘一般会帮我们做影响分析和故障溯源。比如说有一天，你的老板看到某个指标的数据违反常识，让你去排查这个指标计算是否正确，你首先需要找到这个指标所在的表，然后顺着这个表的上游表逐个去排查校验数据，才能找到异常数据的根源。</p>
<p>而数据特征主要是指数据的属性信息，我们以 dws_trd_sku_1d 为例：</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81.png" alt="数据特征"></p>
<p>通过这个例子，你了解了元数据了吗？ 不过元数据的种类非常多，为了管理这些元数据，你必须要构建一个元数据中心。那么接下来，我们就来看看如何搭建一个元数据中心，打通企业的元数据。</p>
<h2 id="业界元数据中心产品"><a href="#业界元数据中心产品" class="headerlink" title="业界元数据中心产品"></a>业界元数据中心产品</h2><p>我做系统设计这么些年，一直有一个习惯，是先看看业界的产品都是怎么设计的，避免关门造车。业界的比较有影响力的产品：</p>
<p>开源的有 Netflix 的 Metacat、Apache Atlas；</p>
<p>商业化的产品有 Cloudera Navigator。</p>
<p>我今天重点想带你了解 Metacat 和 Atlas 这两款产品，一个擅长于管理数据字典，一个擅长于管理数据血缘，通过了解这两款产品，你更能深入的理解元数据中心应该如何设计。</p>
<h3 id="Metacat-多数据源集成型架构设计"><a href="#Metacat-多数据源集成型架构设计" class="headerlink" title="Metacat 多数据源集成型架构设计"></a>Metacat 多数据源集成型架构设计</h3><p>关于Metacat，你可以在 GitHub 上找到相关介绍，所以关于这个项目的背景和功能特性，我就不再多讲，我只想强调一个点，就是它多数据源的可扩展架构设计，因为这个点对于数据字典的管理，真的太重要！</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/Metacat%E6%9E%B6%E6%9E%84.png" alt="Metacat架构"></p>
<p>在一般的公司中，数据源类型非常多是很常见的现象，包括 Hive、MySQL、Oracle、Greenplum 等等。支持不同数据源，建立一个可扩展的、统一的元数据层是非常重要的，否则你的元数据是缺失的。</p>
<p>从上面 Metacat 的架构图中，你可以看到，Metacat 的设计非常巧妙，它并没有单独再保存一份元数据，而是采取直连数据源拉的方式，一方面它不存在保存两份元数据一致性的问题，另一方面，这种架构设计很轻量化，每个数据源只要实现一个连接实现类即可，扩展成本很低，我把这种设计叫做集成型设计。我认为这种设计方式对于希望构建元数据中心的企业，是非常有借鉴意义的。</p>
<h3 id="Apache-Atlas-实时数据血缘采集"><a href="#Apache-Atlas-实时数据血缘采集" class="headerlink" title="Apache Atlas 实时数据血缘采集"></a>Apache Atlas 实时数据血缘采集</h3><p>同样，关于Apache Atlas的背景和功能，我也不多说，只是想强调 Atlas 实时数据血缘采集的架构设计，因为它为解决血缘采集的准确性和时效性难题提供了很多的解决思路。</p>
<p>血缘采集，一般可以通过三种方式：</p>
<p>通过静态解析 SQL，获得输入表和输出表；</p>
<p>通过实时抓取正在执行的 SQL，解析执行计划，获取输入表和输出表；</p>
<p>通过任务日志解析的方式，获取执行后的 SQL 输入表和输出表。</p>
<p>第一种方式，面临准确性的问题，因为任务没有执行，这个 SQL 对不对都是一个问题。第三种方式，血缘虽然是执行后产生的，可以确保是准确的，但是时效性比较差，通常要分析大量的任务日志数据。所以第二种方式，我认为是比较理想的实现方式，而 Atlas 就是这种实现。</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/Atlas%E6%9E%B6%E6%9E%84.png" alt="Atlas架构"></p>
<p>对于 Hive 计算引擎，Atlas 通过 Hook 方式，实时地捕捉任务执行计划，获取输入表和输出表，推送给 Kafka，由一个 Ingest 模块负责将血缘写入 JanusGraph 图数据库中。然后通过 API 的方式，基于图查询引擎，获取血缘关系。对于 Spark，Atlas 提供了 Listener 的实现方式，此外 Sqoop、Flink 也有对应的实现方式。</p>
<p>这两款产品在设计网易元数据中心时，给了很多灵感，下面我就带你了解一下网易元数据中心的设计，以便你掌握一个元数据中心在设计时应该考虑哪些点。</p>
<h2 id="网易元数据中心设计"><a href="#网易元数据中心设计" class="headerlink" title="网易元数据中心设计"></a>网易元数据中心设计</h2><p>在设计网易元数据中心之初，我设定了元数据中心必须实现的 5 个关键目标：</p>
<p><strong>其一，多业务线、多租户支持。</strong></p>
<p>在网易，电商、音乐都是不同的业务线，同一个业务线内，也分为算法、数仓、风控等多个租户，所以元数据中心必须支持多业务线、多租户。</p>
<p><strong>其二，多数据源的支持。</strong></p>
<p>元数据中心必须要能够支持不同类型的数据源（比如 MySQL、Hive、Kudu 等），同时还要支持相同数据源的多个集群。为了规范化管理，还需要考虑将半结构化的 KV 也纳入元数据中心的管理（比如 Kafka、Redis、HBase 等）。这些系统本身并没有表结构元数据，所以需要能够在元数据中心里定义 Kafka 每个 Topic 的每条记录 JSON 中的格式，每个字段代表什么含义。</p>
<p><strong>其三，数据血缘。</strong></p>
<p>元数据中心需要支持数据血缘的实时采集和高性能的查询。同时，还必须支持字段级别的血缘。</p>
<p>什么是字段级别的血缘，我们来举个例子。</p>
<p>insert overwrite table t2 select classid, count(userid) from t1 group</p>
<p>by classid;</p>
<p>t2 表是由 t1 表的数据计算来的，所以 t2 和 t1 是表血缘上下游关系，t2 的 classid 字段是由 t1 的 classid 字段产生的，count 字段是由 userid 经过按照 classid 字段聚合计算得到的，所以 t2 表的 classid 与 t1 的 classid 存在字段血缘，t2 表的 count 分别与 t1 表的 classid 和 userid 存在血缘关系。</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E5%AD%97%E6%AE%B5%E8%A1%80%E7%BC%98.png" alt="字段血缘"></p>
<p>字段血缘在做溯源的时候非常有用，因为大数据加工链路的下游是集市层，为了方便使用者使用，一般都是一些很宽的表（列很多的表，避免 Join 带来的性能损耗），这个表的上游可能是有几十个表产生的，如果不通过字段血缘限定溯源范围，就会导致搜索范围变得很大，无法快速地精准定位到有问题的表。</p>
<p>另外，数据血缘还必须要支持生命周期管理，已经下线的任务应该立即清理血缘，血缘要保留一段时间，如果没有继续被调度，过期的血缘关系应该予以清理。</p>
<p><strong>其四，与大数据平台集成。</strong></p>
<p>元数据中心需要与 Ranger 集成，实现基于 tag 的权限管理方式。在元数据中心中可以为表定义一组标签，Ranger 可以基于这个标签，对拥有某一个标签的一组表按照相同的权限授权。这种方式大幅提高了权限管理的效率。比如，对于会员、交易、毛利、成本，可以设定表的敏感等级，然后根据敏感等级，设定不同的人有权限查看。</p>
<p>另外，元数据中心作为基础元数据服务，包括自助取数分析系统，数据传输系统，数据服务，都应该基于元数据中心提供的统一接口获取元数据。</p>
<p><strong>其五，数据标签。</strong></p>
<p>元数据中心必须要支持对表和表中的字段打标签，通过丰富的不同类型的标签，可以完善数据中台数据的特征，比如指标可以作为一种类型的标签打在表上，主题域、分层信息都可以作为不同类型的标签关联到表。</p>
<p>基于这 5 个因素的考虑，我们设计了网易元数据中心。</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E7%BD%91%E6%98%93%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%9B%BE.png" alt="网易元数据中心系统架构设计图"></p>
<p>这个图按照功能模块分为数据血缘、数据字典和数据特征。</p>
<p>数据血缘由采集端、消息中间件、消费端以及血缘清理模块组成，基于 Hive Hook，Spark Listener，Flink Hook ，可以获取任务执行时输入表和输出表，推送给统一的消息中间件（Kafka），然后消费端负责将血缘关系沉淀到图数据库中。</p>
<p>图数据库选择 Neo4j，主要考虑是性能快、部署轻量化、依赖模块少，当然，开源的 Neo4j 没有高可用方案，并且不支持水平扩展，但是因为单个业务活跃的表规模基本也就在几万的规模，所以单机也够用，高可用可以通过双写的方式实现。</p>
<p>血缘还有一个清理的模块，主要负责定时清理过期的血缘，一般我们把血缘的生命周期设置为 7 天。</p>
<p>数据字典部分，我们参考了 Metacat 实现，我们由一个统一的 Connector Mananger 负责管理到各个数据源的连接。对于 Hive、MySQL，元数据中心并不会保存系统元数据，而是直接连数据源实时获取。对于 Kafka、HBase、Redis 等 KV，我们在元数据中心里内置了一个元数据管理模块，可以在这个模块中定义 Value 的 schema 信息。</p>
<p>数据特征主要是标签的管理以及数据的访问热度信息。元数据中心内置了不同类型的标签，同时允许用户自定义扩展标签类型。指标、分层信息、主题域信息都是以标签的形式存储在元数据中心的系统库里，同时元数据中心允许用户基于标签类型和标签搜索表和字段。</p>
<p>元数据中心统一对外提供了 API 访问接口，数据传输、数据地图、数据服务等其他的子系统都可以通过 API 接口获取元数据。另外 Ranger 可以基于元数据中心提供的 API 接口，获取标签对应的表，然后根据标签更新表对应的权限，实现基于标签的权限控制。</p>
<p>元数据中心构建好以后，你肯定会问，这个元数据中心没有界面吗？它长什么样子？用户咋使用这个元数据中心？ 别急，我们接着往下看。</p>
<h2 id="数据地图：元数据中心的界面"><a href="#数据地图：元数据中心的界面" class="headerlink" title="数据地图：元数据中心的界面"></a>数据地图：元数据中心的界面</h2><p>数据地图是基于元数据中心构建的一站式企业数据资产目录，可以看作是元数据中心的界面。数据开发、分析师、数据运营、算法工程师可以在数据地图上完成数据的检索，解决了“不知道有哪些数据？”“到哪里找数据？”“如何准确的理解数据”的难题。</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83-%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2.png" alt="元数据中心-数据查询"></p>
<p>数据地图提供了多维度的检索功能，使用者可以按照表名、列名、注释、主题域、分层、指标进行检索，结果按照匹配相关度进行排序。考虑到数据中台中有一些表是数仓维护的表，有一些表数仓已经不再维护，在结果排序的时候，增加了数仓维护的表优先展示的规则。同时数据地图还提供了按照主题域、业务过程导览，可以帮助使用者快速了解当前有哪些表可以使用。</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83-%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86.png" alt="元数据中心-数据管理"></p>
<p>当使用者定位到某一个表打开时，会进入详情页，详情页中会展示表的基础信息，字段信息、分区信息、产出信息以及数据血缘。数据血缘可以帮助使用者了解这个表的来源和去向，这个表可能影响的下游应用和报表，这个表的数据来源。</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83-%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86-%E6%98%8E%E7%BB%86.png" alt="元数据中心-数据管理-明细"></p>
<p>数据地图同时还提供了数据预览的功能，考虑到安全性因素，只允许预览 10 条数据，用于判断数据是否符合使用者的预期。数据地图提供的收藏功能， 方便使用者快速找到自己经常使用的表。当数据开发、分析师、数据运营找到自己需要的表时，在数据地图上可以直接发起申请对该表的权限申请。</p>
<p>数据地图对于提高数据发现的效率，实现非技术人员自助取数有重要作用。经过我的实践，数据地图是数据中台中使用频率最高的一个工具产品，在网易，每天都有 500 以上人在使用数据地图查找数据。</p>
<h2 id="课程总结"><a href="#课程总结" class="headerlink" title="课程总结"></a>课程总结</h2><p>本节课，我以元数据作为起点，带你了解了元数据应该包括数据字典、数据血缘和数据特征，然后通过分析两个业界比较有影响力的元数据中心产品，结合我在网易数据中台实践，给出了元数据中心设计的 5 个关键特性和技术实现架构，最后介绍了基于元数据中心之上的数据地图产品。我想在最后强调几个关键点：</p>
<p>元数据中心设计上必须注意扩展性，能够支持多个数据源，所以宜采用集成型的设计方式。</p>
<p>数据血缘需要支持字段级别的血缘，否则会影响溯源的范围和准确性。</p>
<p>数据地图提供了一站式的数据发现服务，解决了检索数据，理解数据的“找数据的需求”。</p>
<p>最后，你要知道，元数据中心是数据中台的基石，它提供了我们做数据治理的必须的数据支撑，在后续的章节中，我们将逐一介绍指标、模型、质量、成本、安全等的治理，这些都离不开元数据中心的支撑。</p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E6%80%BB%E7%BB%93.png" alt="元数据中心总结"></p>
<p><img src="/2022/04/18/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_04-%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E5%85%B3%E9%94%AE%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83.png" alt="元数据中心"></p>
<h2 id="课程思考"><a href="#课程思考" class="headerlink" title="课程思考"></a>课程思考</h2><p>在课程中，我介绍了血缘采集的三种方式，并且推荐了通过实时采集的方式，但是其实静态解析血缘也有它的优势应用场景，你能想到有哪些么？</p>
<h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><ol>
<li><p>数据特征的管理是怎么处理的呢？手动维护吗？（比如标签、关联指标之类描述性的）</p>
<p>数据特征，标签的维护，其实是靠基于元数据中心之上的各个数据中台支撑产品下沉到元数据中心上的。比如指标系统创建了一个指标，在模型设计中，我们会为某个表的某个字段关联一个指标，然后指标和表就产生了关联关系，就会下沉到元数据中心中，以标签的形式存在。 </p>
<p>标签的来源，来自各个基于元数据中心的数据中台工具产品。</p>
</li>
<li><p>文中元数据中心依赖了 Atlas，Ranger，neo4j，ES，Kafka 等，是否依赖的太多，太重？</p>
<p>元数据中心并没有依赖 Atlas，是参考 Atlas的数据血缘 runtime 血缘采集方式，实现了数据血缘部分，他与 Ranger 是集成关系，可以基于 tag 实现授权。neo4j是元数据中心底层的图数据库，ES 提供了元数据的检索，Kafka 主要是接受采集来的实时血缘，这三个系统是元数据中心必须依赖的。</p>
</li>
<li><p>Ranger 通过 tag 实现权限管理，是否数据权限管理都使用 Ranger，不会另外单独一个数据权限模块么？</p>
<p>数据权限，统一使用 Ranger 来管理。产品的数据权限管理模块，底层是基于 Ranger 实现包装的。</p>
</li>
<li><p>静态血缘解析可以对一个正在开发的SQL提供参考信息，看系统中表有哪些SQL处理，避免SQL冗余和冲突。</p>
<p>当我们要提交任务上线，建立任务依赖时，如果我们依赖的表，还没有被调度产生数据，此时就会导致我们根据这张表找不到表的产出任务，系统就无法自动推荐依赖任务。</p>
<p>所以此时就需要静态血缘的介入啦。 对于还未执行，但是保存，SQL语法检查通过的任务，我们可以通过解析SQL获取静态血缘，然后当其他任务读取这张表，要建立到这张表产出任务的依赖时，我们可以根据静态血缘，找到这张表的产出任务。</p>
</li>
<li><p>如果每次查询都是从数据源拉取，是否存在性能问题。且无法做到跨源查询，对数据源集群的性能要求也较高。为何不用etl先统一抽取，再进行统一的sql查询（非实时数据），对用户也相对友好。</p>
<p>元数据的查询并不是每次都查数据源的，而是通过存到ES里面，提供给上层查询的。</p>
</li>
<li><p>一个组织架构的问题，业务都不愿做数据治理，因为没有KPI，而中台方又对数据没那么了解，导致主题域/指标梳理困难。</p>
<p>数据中台，既要独立于业务，又不能脱离业务，数据中台，数据产品（互联网公司希望叫数据pd）角色很关键，他要深入业务，了解业务目标，要通过数据，帮助业务实现目标，孵化数据产品，收管指标。</p>
</li>
<li><p>所有的处理和计算是如何都保证是由SQL完成的？如果数据的计算逻辑是用户的程序完成的，最后只是写到了table里面，这时候如何保证数据血缘？</p>
<p>MR 和 Spark，非 SQL 的代码，在运行时，hadoop client 和 Spark client 也可以通过 Plugin 的方式获取到输入表和输出表的关系，并不一定非要 SQL。</p>
</li>
<li><p>“数据字典部分，我们参考了 Metacat 实现，我们由一个统一的 Connector Mananger 负责管理到各个数据源的连接。对于 Hive、MySQL，元数据中心并不会保存系统元数据，而是直接连数据源实时获取。对于 Kafka、HBase、Redis 等 KV，我们在元数据中心里内置了一个元数据管理模块，可以在这个模块中定义 Value 的 schema 信息。” </p>
<p>这部分有个细节，在元数据平台会保存表基础信息，例如保存表名作为一个关联的依据，然后查详情的时候 connector 去获取表的字典信息，因为平台本身要去加一些标签的话也需要有个载体，如果是这样的话，是定期同步表的列表到元数据平台吗？</p>
<p>查询的话，走的是ES， 我们会把对应数据源的表结构信息同步到ES一份，方便做快速的查询。标签也在ES中，标签分为表级别的和字段级别的，分别打在字段和表名上。</p>
</li>
<li><p>一个数据中台一般得由哪几部分人员组成，又大概需要多少人呢？</p>
<p>数据中台团队，主要包括数据开发、数据产品、平台开发和数据应用开发。 </p>
<p>整体人员规模和你的业务规模要保持同步的。网易不同业务数据中台团队人员规模也相差比较大，多的100多人也是有的，少的十来个人的，其实刚开始，可以先从数据开发团队开始构建，上层通过BI报表的方式呈现，这样数据应用团队也可以先不需要。起步可以10个人以内的团队就可以开始。</p>
</li>
<li><p>在传统企业里，高层领导都是业务出身，而像元数据中心这种产品，如何能说服业务领导同意建设，同时数据地图在设计时如何能让纯业务人员感受到其价值？</p>
<p>元数据中心本身是一个偏实现层的产品，领导其实根本就不关心是否存在这样的一个数据中台的底层。 </p>
<p>但是数据地图，是元数据中心的界面，通过数据地图，领导可以看到数据中台的统一元数据视图，另外，结合数据地图的使用频率、使用范围，可以凸显数据地图的价值。 </p>
<p>数据地图在设计时，一方面他的使用对象是数据开发，另外一方面，他的使用对象又是业务人员。让业务人员感受到数据地图的价值，主要是能够让业务人员搜索指标、数据报表，帮助他们快速找到自己想要的数据。无论是数据表，还是数据报表，还是指标，都能够通过数据地图进行搜索和导览。</p>
</li>
<li><p>数据中台和数仓分层</p>
<p>数据中台团队，维护了公共数据层以及数据中台产出的集市层和应用层模型。其他分析师团队，会基于数据中台产出的模型，构建数据报表。数据中台管理的模型，是通过分层的方式来管理的。非中台管理的模型，原则上如果是临时表，应该是要过期删除的，固化下来的应该属于应用层。</p>
</li>
<li><p>血缘怎么处理，一般是通过 SQL 来做分析，建立血缘，如果是模型的话，一般是代码，比较难自动分析血缘，那是通过手动的方式添加吗？<br>不是通过手工维护的，根本维护不过来。我们是通过 Spark / Hive 提供的 Listener / Hook 机制，获取执行计划中的 input / output table 拿到执行时血缘实现的。这个好处在于，第一，Spark 代码，MR 代码也可以拿到。另外，执行引擎本身会对 SQL 做一些优化，有一些虽然 from 里面有这个表，但是实际没用到这个表，执行引擎的优化器会自动优化掉，如果你靠解析 SQL 的方式去获取血缘，那这部分是不准确的脏血缘。</p>
</li>
<li><p>数据中台建设的周期和资源投入风险怎么平衡？</p>
<p>很可能就是你在担心数据中台建设，前期需要投入很多的资源。而我的建议是，数据中台的建设可以采取滚雪球的方式，逐步以场景化的方式落地。这样既可以控制前期的资源投入风险，又可以保证数据中台有一些阶段性成果的输出。</p>
</li>
<li><p>元数据中心数据的一致性</p>
<p>元数据中心管理了所有数据中台的元数据，所有系统都与元数据中心打通，把元数据的管理入口都收归到元数据中心，可以确保元数据的一致。</p>
</li>
<li><p>元数据中心建设，是否可以理解主要以元数据管理工具进行落地，只是需要配置，就可以实现呢？还是需要有相关的代码开发的工作，才能落地元数据中心的建设?</p>
<p>元数据中心的建设，对于数据字典中直连数据源获取元数据的数据源，以及数据血缘部分，工具落地就可以统一收集到元数据。</p>
<p>但是对于数据特征，尤其是指标、维度标签，这部分是需要数据开发实施介入的，需要进行规范化梳理，一个表，哪些字段是指标，哪些字段是维度，这些不是工具落地就可以自动获取的。</p>
</li>
<li><p>元数据中心本身不管理元数据，不存储元数据信息，但又提到给元数据打标签的场景，那打上这些标签是在哪个环节进行呢？标签与元数据的关联关系又存储在哪里呢？</p>
<p>元数据中心并不维护元数据，但是为了对外提供查询的服务，会同步元数据到ES中，然后标签也会存在ES中，与表建立关联。</p>
</li>
<li><p>如果源数据数据结构变更（增加/修改/删除字段），ods、dwd 等的历史数据怎么处理？有自动处理的方案，还是手动的全流程修改所有数据？</p>
<p>源数据结构变更，ods 和 dwd 需要处理是否要表结构变更，同时历史数据层面，需要数据开发来判断是否需要调整数据计算逻辑，一般来说，源系统数据结构变更，并不影响历史数据，只会影响新的数据计算过程。</p>
</li>
<li><p>Metacat 官方几乎没什么说明文档，想问一下网易那边是怎么利用 Metacat 的？用的公司多吗？</p>
<p>没有直接使用metacat，元数据中心是自己实现的，但是参考了metacat的设计实现。metacat 外面也有直接用的公司。</p>
</li>
<li><p>Metacat 官网上说有数据发现功能会把数据发送到 ES 上，他这个发现是通过定时去拿那些数据源中元数据的信息吗？我在自己电脑上启动 Metacat 服务，数据源，以及 ES.，但是并没有发现元数据同步到 ES 中，请问是我配置出问题了还是什么原因？</p>
<p>看一下 isElasticSearchEnabled 参数</p>
</li>
<li><p>对于静态数据结构、动态血缘分析这种可以通过工具采集，但是数据库中没有中文，是否还是需要人工梳理录入登记？这个工作量也不小了。</p>
<p>你是指的数据字典中字段级别的业务元数据信息？ 有的数据库中，并没有相关的commet描述，或者comment 不适合查看，此时可以通过标签的形式，作为一种特定类型的标签，关联到表的字段中。</p>
<p>本身这个梳理的工作是跑不掉的，但是也可以采取用到的时候再补充，数据源是数据集成阶段登记到数据中台的元数据中心中，此时再梳理补充，并不需要一口气全部补充完整。</p>
</li>
<li><p>元数据中心在项目运维中是怎么和调度系统结合的？ 比如表的使用热度等信息是基于什么指标进行判断的？</p>
<p>表的使用热度，是根据平台上调度运行的job和adhoc执行的query计算来的。</p>
<p>通过数据血缘，我们可以获取到表和任务、query的关联关系，然后可以计算这部分的引用热度。数据血缘是通过hive/spark插件的方式获取的。</p>
</li>
<li><p>表字段信息是实时采集的，像表负责人这些信息怎么关联上的？</p>
<p>metastore是有owner属性的，它可以作为表的负责人角色。 </p>
<p>另外，对于非hive表，负责人可以作为一种类型的标签，和表建立关联。</p>
</li>
<li><p>主题域、表分层等是数仓的概念，这些应该由元数据中心维护吗？ 引用：第5讲有写“指标系统是基于元数据中心构建的一个指标管理工具，它从元数据中心自动同步数仓的主题域和业务过程，按照规范化定义创建指标。</p>
<p>主题域、表的分层信息，都是在元数据中心中，通过表的标签的方式维护的。</p>
</li>
<li><p>表、列等 schema 由 connector 实时从数据源获取，业务元数据维护在元数据本地数据库，那这两类数据是如何关联到一起？在 Metacat 的源码中，没看到这部分的具体实现。</p>
<p>Metacat 没有实现标签的功能，我们会维护一个表和标签的映射关系，存储在元数据中心的数据库中。</p>
</li>
<li><p>数据标准与元数据的区别与联系？数字标签与指标如何区分？</p>
<p>数据标准是指一条数据规范，包括数据的命名、数据的组织等等，概念比较大。元数据，就是指数据的字典、血缘、数据特征。</p>
<p>你说的标签是指业务标签还是元数据标签。我在04中提交，指标可以作为一种特定类型的标签，是指后者，元数据的标签。</p>
<p>现实中还存在一类业务标签，描述的是一个确定的数据集。这个标签在标签工厂中管理。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据中台</category>
      </categories>
      <tags>
        <tag>数据中台</tag>
      </tags>
  </entry>
  <entry>
    <title>数据中台_02-什么样的企业应该建数据中台</title>
    <url>/2022/04/13/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_02-%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E4%BC%81%E4%B8%9A%E5%BA%94%E8%AF%A5%E5%BB%BA%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0/</url>
    <content><![CDATA[<p>在上一篇中，我们一起回顾了大数据的发展历史，从历史脉络中，我们看到了数据中台凸显的价值，并得出数据中台是大数据下一站的结论。</p>
<p>既然数据中台受到了前所未有的关注，价值如此之大，是不是所有的企业都适合建设数据中台呢？到底什么样的企业应该建数据中台？带着这样的疑问，我们正式进入今天的课程。</p>
<p>我先跟你分享一下，2018 年我们在建数据中台前面临的窘境，通过了解我们建数据中台的背景，你也可以对照着看一下自己所在的企业是否存在这样的问题，从而针对“是否需要构建一个数据中台”这个问题形成自己的看法。</p>
<a id="more"></a>
<hr>
<h2 id="建设中台前，我们面临的挑战"><a href="#建设中台前，我们面临的挑战" class="headerlink" title="建设中台前，我们面临的挑战"></a>建设中台前，我们面临的挑战</h2><p>对于绝大多数互联网企业来说，2018 年绝对是煎熬的一年，因为面临线上流量枯竭，业绩增长乏力，企业成本高筑， 利润飞速下滑的风险。 原先粗放的企业管理模式和经营模式（比如我们在采购商品的时候，凭借经验去做出采购哪个商品的决策）已经没办法继续支撑企业的高速增长，越来越多的企业开始提数字化转型，强调数据是企业增长的新动力，它应该深入企业经营的各个环节。</p>
<p>数据需求的爆发式增长，促进了数据产品的蓬勃发展，在每个业务过程中，都有大量的数据产品辅助运营完成日常工作。例如，在电商的场景中，用户运营、商品运营、市场运营……每个场景下，都有很多的数据产品，每天有大量的运营基于这些产品完成经营决策。</p>
<p>比如在供应链决策协同系统中，我们有一个智能补货的功能，会根据商品的库存、历史销售数据以及商品的舆情，智能计算商品的最佳采购计划，推送给运营审核，然后完成采购下单。</p>
<p><img src="/2022/04/13/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_02-%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E4%BC%81%E4%B8%9A%E5%BA%94%E8%AF%A5%E5%BB%BA%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0/%E6%95%B0%E6%8D%AE%E4%BA%A7%E5%93%81%E4%BD%93%E7%B3%BB.png" alt="数据产品体系"></p>
<p><strong>大量数据产品的出现，在不断提高企业运营效率的同时，也暴露出很多尖锐的问题，在我看来，主要有五点。</strong></p>
<ol>
<li><p>指标口径不一致。 两个数据产品一个包含税，一个不包含税，它们相同的一个指标名称都是销售额，结果却不一样。运营面对这些指标的时候，不知道指标的业务口径，很难去使用这些数据。</p>
</li>
<li><p>数据重复建设，需求响应时间长。随着需求的增长，运营和分析师不断抱怨需求的交付时间拉长，面对快速变化的业务，需求响应时间已经无法满足业务对数据的敏捷研发要求。</p>
</li>
<li><p>取数效率低。 面对数十万张表，我们的运营和分析师找数据、准确地理解数据非常困难，想找到一个想要的数据，确认这个数据和自己的需求匹配，他们往往需要花费三天以上的时间，对新人来说，这个时间会更长。</p>
<p>除了查找数据效率低，从系统中取数分析对于非技术出身的分析师和运营来说也是一个难题，这就导致大部分的取数工作还是依赖数据开发来完成。数据开发大部分的时间都被临时取数的需求占据，根本无法专注在数仓模型的构建和集市层数据的建设，最终形成了一个恶性循环，一方面是数据不完善，另一方面是数据开发忙于各种临时取数需求。</p>
</li>
<li><p>数据质量差。数据经常因为 BUG 导致计算结果错误，最终导致错误的商业决策。<strong>分享一个我们踩过的坑，</strong>在大促期间，某类商品搜索转化率增长，于是我们给这个商品分配了更大的流量，可转化率增长的原因是数据计算错误，所以这部分流量也就浪费了，如果分配给其他的商品的话，可以多赚 200W 的营收。</p>
</li>
<li><p>数据成本线性增长。数据成本随着需求的增长而线性增长，2017 年的时候，我们一个业务的大数据资源在 4000Core，但是 2018 就已经到达 9000Core 水平，如果折算成钱的话，已经多了 500 多万的机器成本。</p>
</li>
</ol>
<p>相信你能在我“惨痛”的经历中，找到自己的影子，这些事儿的确很头疼，好在后来，我们用数据中台解决了这些问题。</p>
<h2 id="为什么数据中台可以解决这些问题？"><a href="#为什么数据中台可以解决这些问题？" class="headerlink" title="为什么数据中台可以解决这些问题？"></a>为什么数据中台可以解决这些问题？</h2><p>要想回答这个问题，你需要了解上述问题背后的原因。</p>
<p>指标口径不一致，可能原因包括三种：<strong>业务口径不一致、计算逻辑不一致、数据来源不一致。</strong></p>
<p>如果是业务口径不一致，那就要明确区分两个指标不能使用相同的标识，像上面的例子，含税和不含税的两个指标， 不能同时叫销售额。</p>
<p>业务口径的描述往往是一段话，但是对于很多指标，涉及的计算逻辑非常复杂，仅仅一段话是描述不清楚的，此时，两个相同业务口径的指标，恰巧又分别是由两个数据开发去实现的，这样就有可能造成计算逻辑不一致。比如，有一个指标叫做排关单（排关单：把关单的排除；关单：关闭订单）的当天交易额这个指标，A 认为关单的定义是未发货前关闭的订单，B 认为关单是当天关闭的订单，大家对业务口径理解不一致，这样实现的计算结果也就会不一致。</p>
<p>最后，还可能是两个指标的数据来源不一样，比如一个来自实时数据，一个是来自离线的数据，即使加工逻辑一样，最终结果也可能不相同。</p>
<p><strong>综合看来，要实现一致，就务必确保对同一个指标，只有一个业务口径，只加工一次，数据来源必须相同。</strong></p>
<p>而数据需求响应慢在于烟囱式的开发模式，导致了大量重复逻辑代码的研发，比如同一份原始数据，两个任务都对原始数据进行清洗。如果只有一个任务清洗，产出一张明细表，另外一个任务直接引用这张表，就可以节省一个研发的清洗逻辑的开发。</p>
<p><strong>所以，要解决数据需求响应慢，就必须解决数据复用的问题，要确保相同数据只加工一次，实现数据的共享。</strong></p>
<p>取数效率低，一方面原因是找不到数据，另一方面原因可能是取不到数据。要解决找不到数据的问题，就必须要构建一个全局的企业数据资产目录，实现数据地图的功能，快速找到数据。而非技术人员并不适合用写 SQL 的方式来取数据，所以要解决取不到数据的问题，就要为他们提供可视化的查询平台，通过勾选一些参数，才更容易使用。</p>
<p>数据质量差的背后其实是数据问题很难被发现。我们经常是等到使用数据的人反馈投诉，才知道数据有问题。而数据的加工链路一般非常长，在我们的业务中，一个指标上游的所有链路加起来有 100 多个节点，这是很正常的事情。等到运营投诉再知道数据有问题就太迟了，因为要逐个去排查到底哪个任务有问题，然后再重跑这个任务以及所有下游链路上的每个任务，这样往往需要花费半天到一天的时间，最终导致故障恢复的效率很低，时间很长。</p>
<p><strong>所以，要解决数据质量差，就要及时发现然后快速恢复数据问题。</strong></p>
<p>最后一个是大数据的成本问题，它其实与需求响应慢背后的数据重复建设有关，因为重复开发任务的话，这些任务上线肯定会花费双倍的资源。如果我们可以节省一个任务的资源消耗，满足两个数据需求，就可以控制不必要的资源消耗。所以，成本问题背后也是数据重复建设的问题。</p>
<p>正当我们为这些问题苦恼的时候，数据中台的理念给了我们全新的启迪，那么数据中台到底是怎么一回事儿呢？<strong>在我看来，数据中台是企业构建的标准的、安全的、统一的、共享的数据组织，通过数据服务化的方式支撑前端数据应用。</strong></p>
<p>数据中台消除了冗余数据，构建了企业级数据资产，提高了数据的共享能力，这与我们需要的能力不谋而合，所以很快，我们开启了数据中台的建设。</p>
<h2 id="数据中台是如何解决这些问题的？"><a href="#数据中台是如何解决这些问题的？" class="headerlink" title="数据中台是如何解决这些问题的？"></a>数据中台是如何解决这些问题的？</h2><p>指标是数据加工的结果，要确保数据需求高质量的交付，首先是要管好指标。</p>
<p>原先指标的管理非常分散，没有全局统一的管理，在数据中台中，必须要有一个团队统一负责指标口径的管控。</p>
<p>其次，要实现指标体系化的管理，提高指标管理的效率。在指标系统中，我们会明确每个指标的业务口径，数据来源和计算逻辑，同时会按照类似数仓主题域的方式进行管理。</p>
<p>最后，要确保所有的数据产品、报表都引用指标系统的口径定义。当运营把鼠标 Hover 到某个指标上时，就可以浮现出该指标的口径定义。</p>
<p>通过对全局指标的梳理，我们实现了 100% 的数据产品的指标口径统一，消除了数据产品中，指标口径二义性的问题，同时还提供了方便分析师、运营查询的指标管理系统。</p>
<p><strong>那么数据中台是怎么实现所有数据只加工一次的呢？</strong>简单来说，就是对于数仓数据，我们要求相同粒度的度量或者指标只加工一次，构建全局一致的公共维表。要实现上述目标，需要两个工具产品：</p>
<p>一个是数仓设计中心，在模型设计阶段，强制相同聚合粒度的模型，度量不能重复。</p>
<p>另外一个是数据地图，方便数据开发能够快速地理解一张表的准确含义。</p>
<p>这样就解决了数据重复加工导致研发效率瓶颈的问题，现在我们把需求的平均交付时间从一周减少到 2～3 天，大幅提高了数据产能，得到了分析师和运营的认可。</p>
<p><strong>数据中台通过服务化的方式，提高了数据应用接入和管理的效率。</strong>原先数仓提供给应用的访问方式是直接提供表，应用开发自己把数据导出到一个查询引擎上，然后去访问查询引擎。在数据中台中，数仓的数据是通过 API 接口的方式提供给数据应用，数据应用不用关心底层不同的查询引擎访问方式的差异。</p>
<p><strong>对于非技术人员，数据中台提供了可视化的取数平台，</strong>你只需要选取指标、通过获取指标系统中每个指标的可分析维度，然后勾选，添加筛选过滤条件，点击查询，就可以获取数据。</p>
<p>同时，数据中台构建了企业数据地图，你可以很方便地检索有哪些数据，它们在哪些表中，又关联了哪些指标和维度。通过自助取数平台和数据地图，公司的非技术人员开始自助完成取数，相比通过提需求给技术人员的方式，取数效率提高了 300%。</p>
<p>![EasyFetch 网易自助取数界面](数据中台_02-什么样的企业应该建数据中台/EasyFetch 网易自助取数界面.png)</p>
<p><strong>数据中台由于数据只能加工一次，强调数据的复用性，这就对数据的质量提出了更高的要求。</strong>而我们实现了全链路的数据质量稽核监控，对一个指标的产出上游链路中涉及的每个表，都实现了数据一致性、完整性、正确性和及时性的监控，确保在第一时间发现、恢复、通知数据问题。</p>
<p>原先，当技术人员问我们“今天数据有没有问题？” 的时候，我们很难回答，现在我们可以很自信地回答，数据对不对，哪些数据不对，什么时候能恢复了。我个人认为这个能力对我们最终达成 99.8% 数据 SLA 至关重要。</p>
<p><strong>最后一个是成本问题。</strong>我们在构建数据中台的时候，研发了一个数据成本治理系统，从应用维度、表维度、任务的维度、文件的维度进行全面的治理。 从应用的维度，如果一个报表 30 天内没有访问，这个报表的产出价值就是低的，然后结合这个报表产出的所有上游表以及上游表的产出任务，我们可以计算加工这张表的成本，有了价值和成本，我们就能计算 ROI，根据 ROI 就可以实现将低价值的报表下线的功能。通过综合治理，最终我们在一个业务中节省了超过 20% 的成本，约 900W。</p>
<p>通过数据中台，最终我们成功解决了面临的问题，大幅提高了数据研发的效率、质量，降低了数据的成本。那么现在让我们回到课程开始时的问题，到底什么样的企业适合建数据中台？ 是不是所有企业都要构建一个数据中台？</p>
<h2 id="什么样的企业适合建数据中台？"><a href="#什么样的企业适合建数据中台？" class="headerlink" title="什么样的企业适合建数据中台？"></a>什么样的企业适合建数据中台？</h2><p>不可否认，数据中台的构建需要非常大的投入：一方面数据中台的建设离不开系统支撑，研发系统需要投入大量的人力，而这些系统是否能够匹配中台建设的需求，还需要持续打磨。另外一方面，面对大量的数据需求，要花费额外的人力去做数据模型的重构，也需要下定决心。</p>
<p>所以数据中台的建设，需要结合企业的现状，根据需要进行选择。我认为企业在选择数据中台的时候，应该考虑这样几个因素。</p>
<p>企业是否有大量的数据应用场景： 数据中台本身并不能直接产生业务价值，数据中台的本质是支撑快速地孵化数据应用。所以当你的企业有较多数据应用的场景时（一般有 3 个以上就可以考虑），就像我在课程开始时提到电商中有各种各样的数据应用场景，此时你要考虑构建一个数据中台。</p>
<p>经过了快速的信息化建设，企业存在较多的业务数据的孤岛，需要整合各个业务系统的数据，进行关联的分析，此时，你需要构建一个数据中台。比如在我们做电商的初期，仓储、供应链、市场运营都是独立的数据仓库，当时数据分析的时候，往往跨了很多数据系统，为了消除这些数据孤岛，就必须要构建一个数据中台。</p>
<p>当你的团队正在面临效率、质量和成本的苦恼时，面对大量的开发，却不知道如何提高效能，数据经常出问题而束手无策，老板还要求你控制数据的成本，这个时候，数据中台可以帮助你。</p>
<p>当你所在的企业面临经营困难，需要通过数据实现精益运营，提高企业的运营效率的时候，你需要构建一个数据中台，同时结合可视化的 BI 数据产品，实现数据从应用到中台的完整构建，在我的接触中，这种类型往往出现在传统企业中。</p>
<p>企业规模也是必须要考虑的一个因素，数据中台因为投入大，收益偏长线，所以更适合业务相对稳定的大公司，并不适合初创型的小公司。</p>
<p>如果你的公司有这样几个特征，不要怀疑，把数据中台提上日程吧。</p>
<h2 id="课堂总结"><a href="#课堂总结" class="headerlink" title="课堂总结"></a>课堂总结</h2><p>本节课，我结合自己的经历，带你了解了企业数据在日常使用过程中面临的一些难题，通过分析，我们发现，数据中台恰好可以对症下药，解决这些问题。在这个过程中，我想强调这样几个重点：</p>
<p>效率、质量和成本是决定数据能否支撑好业务的关键，构建数据中台的目标就是要实现高效率、高质量、低成本。</p>
<p>数据只加工一次是建设数据中台的核心，本质上是要实现公共计算逻辑的下沉和复用。</p>
<p>如果你的企业拥有 3 个以上的数据应用场景，数据产品还在不断研发和更新，你必须要认真考虑建设数据中台。</p>
<p>在最后，我想再次强调一下，建设数据中台不能盲目跟风，因为它不一定适合你，我在生活中见到了很多不符合上述特征，却想要建设数据中台的公司，比如一些初创型的小公司，初期投入了大量人力成本建设数据中台，因为业务变化快，缺少深入数据应用场景，结果却是虎头蛇尾，价值无法落地。所以，你最正确的做法是仔细想想我提出的上述 5 点要素。</p>
<p>因为这节课信息比较密集，我用一个脑图帮你梳理一下知识体系，便于你理解：</p>
<p><img src="/2022/04/13/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_02-%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E4%BC%81%E4%B8%9A%E5%BA%94%E8%AF%A5%E5%BB%BA%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0/%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0.png" alt="是否需要数据中台"></p>
<h2 id="思考时间"><a href="#思考时间" class="headerlink" title="思考时间"></a>思考时间</h2><p>我同样给留给你一道思考题，一个企业是不是只能建设一个数据中台？</p>
]]></content>
      <categories>
        <category>数据中台</category>
      </categories>
      <tags>
        <tag>数据中台</tag>
      </tags>
  </entry>
  <entry>
    <title>数据中台_01-为什么数据中台是大数据的下一站</title>
    <url>/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/</url>
    <content><![CDATA[<p>为什么数据中台被认为是大数据的下一站呢？它与你之前遇到的数据仓库、数据湖、大数据平台又有什么区别？</p>
<p>这篇文章带着这个问题，<strong>与你深入大数据的发展历史，先从数据仓库的出现讲起，途径数据湖，再到大数据平台，</strong>因为这样，才能理解大数据发展的每个阶段遇到的问题，从而深入理解数据中台在大数据发展中的历史定位。</p>
<a id="more"></a>
<hr>
<h2 id="启蒙时代：数据仓库的出现"><a href="#启蒙时代：数据仓库的出现" class="headerlink" title="启蒙时代：数据仓库的出现"></a>启蒙时代：数据仓库的出现</h2><p>商业智能（Business Intelligence）诞生在上个世纪 90 年代，它是将企业已有的数据转化为知识，帮助企业做出经营分析决策。比如在零售行业的门店管理中，如何使得单个门店的利润最大化，我们就需要分析每个商品的销售数据和库存信息，为每个商品制定合理的销售采购计划，有的商品存在滞销，应该降价促销，有的商品比较畅销，需要根据对未来销售数据的预测，进行提前采购，这些都离不开大量的数据分析。</p>
<p>而数据分析需要聚合多个业务系统的数据，比如需要集成交易系统的数据，需要集成仓储系统的数据等等，同时需要保存历史数据，进行大数据量的范围查询。传统数据库面向单一业务系统，主要实现的是面向事务的增删改查，已经不能满足数据分析的场景，<strong>这促使数据仓库概念的出现。</strong></p>
<p>在 1991 年出版的《Building the Data Warehouse》中，数据仓库之父比尔·恩门（Bill Inmon）首次给出了数据仓库的完整定义，他认为：</p>
<p>数据仓库是在企业管理和决策中面向主题的、集成的、与时间相关的，不可修改的数据集合。</p>
<p>为了帮你理解数据仓库的四要素，我举个电商的例子。</p>
<p>在电商场景中，有一个数据库专门存放订单的数据，另外一个数据库存放会员相关的数据。构建数据仓库，首先要把不同业务系统的数据同步到一个统一的数据仓库中，然后按照主题域方式组织数据。</p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93.png" alt="数据仓库"></p>
<p>主题域是业务过程的一个高层次的抽象，像商品、交易、用户、流量都能作为一个主题域，<strong>你可以把它理解为数据仓库的一个目录。</strong>数据仓库中的数据一般是按照时间进行分区存放，一般会保留 5 年以上，每个时间分区内的数据都是追加写的方式，对于某条记录是不可更新的。</p>
<p>除了这个概念之外，我还要提一下他和金博尔（Kimball） 共同开创的数仓建模的设计方法，这个方法对于后来基于数据湖的现代数据仓库的设计有重要的意义，所以你有必要了解。</p>
<p>恩门提出的建模方法自顶向下（这里的顶是指数据的来源，在传统数据仓库中，就是各个业务数据库），基于业务中各个实体以及实体之间的关系，构建数据仓库。</p>
<p>比如，在一个最简单的买家购买商品的场景中，按照恩门建模的思维模式，首先你要理清这个业务过程中涉及哪些实体。买家、商品是一个实体，买家购买商品是一个关系。所以，模型设计应该有买家表，商品表，和买家商品交易表三个模型。</p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E4%B9%B0%E5%AE%B6%E8%A1%A8.png" alt="买家表"></p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E5%95%86%E5%93%81%E8%A1%A8.png" alt="商品表"></p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E4%B9%B0%E5%AE%B6%E5%95%86%E5%93%81%E4%BA%A4%E6%98%93%E8%A1%A8.png" alt="买家商品交易表"></p>
<p>金博尔建模与恩门正好相反，是一种自底向上的模型设计方法，从数据分析的需求出发，拆分维度和事实。那么用户、商品就是维度，库存、用户账户余额是事实。</p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E7%94%A8%E6%88%B7%E7%BB%B4%E5%BA%A6%E8%A1%A8.png" alt="用户维度表"></p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E5%95%86%E5%93%81%E7%BB%B4%E5%BA%A6%E8%A1%A8.png" alt="商品维度表"></p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E8%B4%A6%E6%88%B7%E4%BD%99%E9%A2%9D%E4%BA%8B%E5%AE%9E%E8%A1%A8.png" alt="账户余额事实表"></p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E5%95%86%E5%93%81%E5%BA%93%E5%AD%98%E4%BA%8B%E5%AE%9E%E8%A1%A8.png" alt="商品库存事实表"></p>
<p>这两种方法各有优劣，恩门建模因为是从数据源开始构建，构建成本比较高，适用于应用场景比较固定的业务，比如金融领域，冗余数据少是它的优势。金博尔建模由于是从分析场景出发，适用于变化速度比较快的业务，比如互联网业务。<strong>由于现在的业务变化都比较快，所以我更推荐金博尔的建模设计方法。</strong></p>
<p>传统数据仓库，第一次明确了数据分析的应用场景应该用单独的解决方案去实现，不再依赖于业务的数据库。在模型设计上，提出了数据仓库模型设计的方法论，为后来数据分析的大规模应用奠定了基础。但是进入互联网时代后，传统数据仓库逐渐没落，一场由互联网巨头发起的技术革命催生了大数据时代的到来。</p>
<h2 id="技术革命：从-Hadoop-到数据湖"><a href="#技术革命：从-Hadoop-到数据湖" class="headerlink" title="技术革命：从 Hadoop 到数据湖"></a>技术革命：从 Hadoop 到数据湖</h2><p>进入互联网时代，有两个最重要的变化。</p>
<p>一个是数据规模前所未有，一个成功的互联网产品日活可以过亿，就像你熟知的头条、抖音、快手、网易云音乐，每天产生几千亿的用户行为。传统数据仓库难于扩展，根本无法承载如此规模的海量数据。</p>
<p>另一个是数据类型变得异构化，互联网时代的数据除了来自业务数据库的结构化数据，还有来自 App、Web 的前端埋点数据，或者业务服务器的后端埋点日志，这些数据一般都是半结构化，甚至无结构的。传统数据仓库对数据模型有严格的要求，在数据导入到数据仓库前，数据模型就必须事先定义好，数据必须按照模型设计存储。</p>
<p>所以，数据规模和数据类型的限制，导致传统数据仓库无法支撑互联网时代的商业智能。</p>
<p>而以谷歌和亚马逊为代表的互联网巨头率先开始了相关探索。从 2003 年开始，互联网巨头谷歌先后发表了 3 篇论文：《The Google File System》《MapReduce：Simplified Data Processing on Large Clusters》《Bigtable：A Distributed Storage System for Structed Data》，这三篇论文奠定了现代大数据的技术基础。它们提出了一种新的，面向数据分析的海量异构数据的统一计算、存储的方法。关于这三篇论文，在这里我们不做深入的解读，如果对实现技术感兴趣的话，也可以查看我在文末提供的链接。</p>
<p>但 2005 年 Hadoop 出现的时候，大数据技术才开始普及。你可以把 Hadoop 认为是前面三篇论文的一个开源实现，我认为 Hadoop 相比传统数据仓库主要有两个优势：</p>
<p>完全分布式，易于扩展，可以使用价格低廉的机器堆出一个计算、存储能力很强的集群，满足海量数据的处理要求；</p>
<p>弱化数据格式，数据被集成到 Hadoop 之后，可以不保留任何数据格式，数据模型与数据存储分离，数据在被使用的时候，可以按照不同的模型读取，满足异构数据灵活分析的需求。</p>
<p>随着 Hadoop 技术日趋成熟，2010 年，Pentaho 创始人兼 CTO James Dixon 在纽约 Hadoop World 大会上提出了数据湖的概念，他提到：</p>
<p>数据湖（Data Lake）是一个以原始格式存储数据的存储库或系统。</p>
<p>数据湖概念的提出，我认为是 Hadoop 从开源技术走向商业化成熟的标志。企业可以基于 Hadoop 构建数据湖，将数据作为一种企业核心资产。</p>
<p>数据湖拉开了 Hadoop 商用化的大幕，但是一个商用的 Hadoop 包含 20 多种计算引擎， 数据研发涉及流程非常多，技术门槛限制了 Hadoop 的商用化进程。那么如何让数据的加工像工厂一样，直接在设备流水线上完成呢？</p>
<h2 id="数据工厂时代：大数据平台兴起"><a href="#数据工厂时代：大数据平台兴起" class="headerlink" title="数据工厂时代：大数据平台兴起"></a>数据工厂时代：大数据平台兴起</h2><p>对于一个数据开发，在完成一项需求时，常见的一个流程是首先要把数据导入到大数据平台中，然后按照需求进行数据开发。开发完成以后要进行数据验证比对，确认是否符合预期。接下来是把数据发布上线，提交调度。最后是日常的任务运维，确保任务每日能够正常产出数据。</p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E9%9C%80%E6%B1%82%E6%B5%81%E7%A8%8B.png" alt="需求流程"></p>
<p>如此繁杂的一个工作流程，如果没有一个高效的平台作为支撑，就跟写代码没有一个好用的 IDE， 用文本编辑器写代码一样，别人完成十个需求，你可能连一个需求都完成不了，效率异常低下，根本无法大规模的应用。</p>
<p>提出大数据平台的概念，就是为了提高数据研发的效率，降低数据研发的门槛，让数据能够在一个设备流水线上快速地完成加工。</p>
<p>大数据平台是面向数据研发场景的，覆盖数据研发的完整链路的数据工作台</p>
<p><img src="/2022/04/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_01-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8B%E4%B8%80%E7%AB%99/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0.png" alt="大数据平台"></p>
<p>大数据平台按照使用场景，分为数据集成、数据开发、数据测试……任务运维，大数据平台的使用对象是数据开发。大数据平台的底层是以 Hadoop 为代表的基础设施，分为计算、资源调度和存储。</p>
<p>Hive、Spark、Flink、Impala 提供了大数据计算引擎：</p>
<p>Hive、Spark 主要解决离线数据清洗、加工的场景，目前，Spark 用得越来越多，性能要比 Hive 高不少；</p>
<p>Flink 主要是解决实时计算的场景；</p>
<p>Impala 主要是解决交互式查询的场景。</p>
<p>这些计算引擎统一运行在一个称为 Yarn 的资源调度管理框架内，由 Yarn 来分配计算资源。目前最新的研究方向中也有基于 Kubernetes 实现资源调度的，例如在最新的 Spark 版本（2.4.4）中，Spark 已经能够运行在 Kubernetes 管理的集群上，这样的好处是可以实现在线和离线的资源混合部署，节省机器成本。</p>
<p>数据存储在 HDFS、Kudu 和 HBase 系统内。HDFS 不可更新，主要存全量数据，HBase 提供了一个可更新的 KV，主要存一些维度表，Kudu 提供了实时更新的能力，一般用在实时数仓的构建场景中。</p>
<p>大数据平台像一条设备流水线，经过大数据平台的加工，原始数据变成了指标，出现在各个报表或者数据产品中。随着数据需求的快速增长，报表、指标、数据模型越来越多，找不到数据，数据不好用，数据需求响应速度慢等问题日益尖锐，成为阻塞数据产生价值的绊脚石。</p>
<h2 id="数据价值时代：数据中台崛起"><a href="#数据价值时代：数据中台崛起" class="headerlink" title="数据价值时代：数据中台崛起"></a>数据价值时代：数据中台崛起</h2><p>时间到了 2016 年前后，互联网高速发展，背后对数据的需求越来越多，数据的应用场景也越来越多，有大量的数据产品进入到了我们运营的日常工作，成为运营工作中不可或缺的一部分。在电商业务中，有供应链系统，供应链系统会根据各个商品的毛利、库存、销售数据以及商品的舆情，产生商品的补货决策，然后推送给采购系统。</p>
<p>大规模数据的应用，也逐渐暴露出现一些问题。</p>
<p>业务发展前期，为了快速实现业务的需求，烟囱式的开发导致企业不同业务线，甚至相同业务线的不同应用之间，数据都是割裂的。两个数据应用的相同指标，展示的结果不一致，导致运营对数据的信任度下降。如果你是运营，当你想看一下商品的销售额，发现两个报表上，都叫销售额的指标出现了两个值，你的感受如何?  你第一反应肯定是数据算错了，你不敢继续使用这个数据了。</p>
<p>数据割裂的另外一个问题，就是大量的重复计算、开发，导致的研发效率的浪费，计算、存储资源的浪费，大数据的应用成本越来越高。</p>
<p>如果你是运营，当你想要一个数据的时候，开发告诉你至少需要一周，你肯定想是不是太慢了，能不能再快一点儿？</p>
<p>如果你是数据开发，当面对大量的需求的时候，你肯定是在抱怨，需求太多，人太少，活干不完。</p>
<p>如果你是一个企业的老板，当你看到每个月的账单成指数级增长的时候，你肯定觉得这也太贵了，能不能再省一点，要不吃不消了。</p>
<p>这些问题的根源在于，数据无法共享。2016 年，阿里巴巴率先提出了“数据中台”的口号。<strong>数据中台的核心，是避免数据的重复计算，通过数据服务化，提高数据的共享能力，赋能数据应用。</strong>之前，数据是要啥没啥，中间数据难于共享，无法积累。现在建设数据中台之后，要啥有啥，数据应用的研发速度不再受限于数据开发的速度，一夜之间，我们就可以根据场景，孵化出很多数据应用，这些应用让数据产生价值。</p>
<h2 id="课堂总结"><a href="#课堂总结" class="headerlink" title="课堂总结"></a>课堂总结</h2><p>现在，回到我们本节课的题目：为什么说数据中台是大数据的下一站？ 在我看来，有这样几个原因：</p>
<p>数据中台构建于数据湖之上，具备数据湖异构数据统一计算、存储的能力，同时让数据湖中杂乱的数据通过规范化的方式管理起来。</p>
<p>数据中台需要依赖大数据平台，大数据平台完成了数据研发的全流程覆盖，数据中台增加了数据治理和数据服务化的内容。</p>
<p>数据中台借鉴了传统数据仓库面向主题域的数据组织模式，基于维度建模的理论，构建统一的数据公共层。</p>
<p>总的来说，数据中台吸收了传统数据仓库、数据湖、大数据平台的优势，同时又解决了数据共享的难题，通过数据应用，实现数据价值的落地。</p>
<p>在文章的最后，为了帮你把数据中台诞生的大事件串联起来，我做了一张时间图，在这个时间线里，你可以很清晰地看到数据中台诞生的前期、中期，和后期的大事件，这样可以帮你更清晰的掌握数据中台背景。</p>
<p><img src="https://static001.geekbang.org/resource/image/e5/78/e5ec6e5bc4e7c64718c0d961b9627b78.jpg" alt="数据诞生史"></p>
<h2 id="思考时间"><a href="#思考时间" class="headerlink" title="思考时间"></a>思考时间</h2><p>在这节课快要结束时，我给你留一个发散性的思考题：如果说数据中台是大数据的下一站，那数据中台的下一站是什么？这个话题很有趣，欢迎你大开“脑洞”，在留言区与我分享。</p>
<p>最后，感谢你的阅读，如果这篇文章让你有所收获，也欢迎你将它分享给更多的朋友。</p>
<p><strong>论文链接：</strong></p>
<p><a href="https://dl.acm.org/doi/abs/10.1145/945445.945450" target="_blank" rel="noopener">《The Google File System》</a></p>
<p><a href="https://dl.acm.org/doi/abs/10.1145/1327452.1327492" target="_blank" rel="noopener">《MapReduce：Simplified Data Processing on Large Clusters》</a></p>
<p><a href="https://dl.acm.org/doi/abs/10.1145/1365815.1365816" target="_blank" rel="noopener">《Bigtable：A Distributed Storage System for Structed Data》</a></p>
]]></content>
      <categories>
        <category>数据中台</category>
      </categories>
      <tags>
        <tag>数据中台</tag>
      </tags>
  </entry>
  <entry>
    <title>生产相关配置</title>
    <url>/2020/03/20/%E7%94%9F%E4%BA%A7%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">请输入密码...</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="133ced714506e802a6d1d2f8c459c127c8c8d3b25b19c6d8ed4a81f2761b8518">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>生产配置</category>
      </categories>
      <tags>
        <tag>生产配置</tag>
      </tags>
  </entry>
  <entry>
    <title>数据中台_03-建设三板斧：方法论、组织和技术</title>
    <url>/2022/04/14/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_03-%E5%BB%BA%E8%AE%BE%E4%B8%89%E6%9D%BF%E6%96%A7%EF%BC%9A%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E7%BB%84%E7%BB%87%E5%92%8C%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<p>在上一篇中，我们了解了什么样的企业适合建数据中台，可能有的同学会说：你可真的戳中我了，我们现在就面临这个问题，可是知道要转型，要建设数据中台，却不知道要咋做，怎么办呢？</p>
<p>现在有很多讲“如何建设数据中台”的文章，大家的观点各不相同：</p>
<p>有的观点说，数据中台是一种数据建设的方法论，按照数据中台设计方法和规范实施就可以建成数据中台了；</p>
<p>也有观点认为，数据中台的背后是数据部门组织架构的变更，把原先分散的组织架构形成一个统一的中台部门，就建成了数据中台；</p>
<p>除此之外，你可能还听到过一些大数据公司说，他们可以卖支撑数据中台建设的产品技术。</p>
<a id="more"></a>
<hr>
<p>那数据中台到底如何建设呢？咱们先不着急回答这个问题，而是看一个例子。</p>
<p>你肯定见过盖房子，盖房子之前，是不是先得有设计图纸，知道如何去盖这个房子？然后还必须要有一个好用的工具（比如水泥搅拌机、钢筋切割机）帮你盖好这个房子。当然了，盖房子离不开一个靠谱的施工队伍，这里面涉及很多角色（泥瓦工、木工、水电工等等），这些人必须高效协作，最终才能盖出一个好的房子。</p>
<p>如果我们把建数据中台比作是盖房子，那么设计图纸就是数据中台建设的方法论；工具是数据中台的支撑技术；施工队伍就是数据中台的组织架构。这三者缺一不可。</p>
<p>这一讲我就以全局的视角，让你从宏观上了解如何建设一个企业级的数据中台。</p>
<h2 id="数据中台建设方法论"><a href="#数据中台建设方法论" class="headerlink" title="数据中台建设方法论"></a>数据中台建设方法论</h2><p>早在 2016 年，阿里巴巴就提出了数据中台建设的核心方法论：OneData 和 OneService。经过这么多年，很多公司都进行了实践，但你很难找出一个准确的定义去描述这些方法论，而我结合自己在网易数据中台建设的经验，对方法论进行了系统化的定义，你可以借鉴参考一下。</p>
<h3 id="OneData"><a href="#OneData" class="headerlink" title="OneData"></a>OneData</h3><p>用一句话定义 OneData 的话，就是所有数据只加工一次。 这个概念具体是啥意思呢？我们来看一个例子。</p>
<p><img src="/2022/04/14/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_03-%E5%BB%BA%E8%AE%BE%E4%B8%89%E6%9D%BF%E6%96%A7%EF%BC%9A%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E7%BB%84%E7%BB%87%E5%92%8C%E6%8A%80%E6%9C%AF/%E4%BB%93%E5%82%A8%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.png" alt="仓储管理系统"></p>
<p>电商业务建设数据中台之前，每个部门内部都会有一些小的数仓去完成本部门的数据分析需求。</p>
<p>有一天，供应链团队接到一个数据需求，那就是计算“商品库存”指标，供应链的运营需要根据每个商品的库存制订商品采购计划，部门的数据开发从业务系统同步数据，进行数据清洗、聚合、深度加工，最终，产出这个指标花了 1 周的时间。</p>
<p>而恰逢全年最重要的大促节日，市场部门也需要根据每个商品的库存，制订商品的促销计划。该数据开发接到这个紧急的需求（与供应链团队类似）从需求开发到上线，也足足花费了 1 周的时间。同部门的运营会抱怨说，为什么数据需求开发这么慢，根本无法满足大促期间高频的市场运营决策。而对公司而言，等待 1 周意味着遭受巨大损失，该促销的商品没有促销，不该促销的却低价卖了。</p>
<p>如果你是这个公司的老板， 肯定会问，既然供应链团队已经计算出来了商品库存的数据，为什么市场部门不直接用，还要从头再计算一遍呢？这个看似很傻的行为，却处处出现在我们日常的数据建设中。</p>
<p>而数据中台就是要在整个电商业务形成一个公共数据层，消灭这些跨部门的小数仓，实现数据的复用，所以强调数据只加工一次，不会因为不同的应用场景，不同的部门数据重复加工。</p>
<p>那具体来说，如何去做才能实现数据只加工一次呢？有这样五点：</p>
<p><img src="/2022/04/14/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_03-%E5%BB%BA%E8%AE%BE%E4%B8%89%E6%9D%BF%E6%96%A7%EF%BC%9A%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E7%BB%84%E7%BB%87%E5%92%8C%E6%8A%80%E6%9C%AF/OneData.png" alt="OneData"></p>
<p>试想一下，现在你构建了数据中台，但存在几万张表，同时又有几十个数据开发维护这些表，如何来确保这些表的管理效率？ <strong>我建议你选择划主题域。</strong></p>
<p>我们可以将这几万张表划到不同的主题域中，比如在电商业务中，商品、交易、流量、用户、售后、配送、供应链都可以作为主题域。好的主题域划分，是相对稳定，尽可能地覆盖绝大多数的表。</p>
<p><strong>除此之外，还要对表的命名进行规范化统一，</strong> 表的名称中最好能够携带表的主题域、业务过程、分层以及分区信息。比如，对于仓储域下的一张入库明细表，规则命名可以这样：</p>
<p><img src="/2022/04/14/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_03-%E5%BB%BA%E8%AE%BE%E4%B8%89%E6%9D%BF%E6%96%A7%EF%BC%9A%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E7%BB%84%E7%BB%87%E5%92%8C%E6%8A%80%E6%9C%AF/%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83.png" alt="命名规范"></p>
<p>接着你还必须构建全局的指标字典，确保所有表中相同指标的口径必须一致（这部分内容我会在 06 讲细说）。</p>
<p><img src="/2022/04/14/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_03-%E5%BB%BA%E8%AE%BE%E4%B8%89%E6%9D%BF%E6%96%A7%EF%BC%9A%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E7%BB%84%E7%BB%87%E5%92%8C%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E5%88%86%E5%B1%82.png" alt="数据分层"></p>
<p>另外，为了实现模型的复用，数据中台适合采用分层的设计方式，常见的分层包括：ODS 原始数据层，DWD 明细数据层，DWS 轻度汇总数据层，ADS/DM 应用数据层 / 数据集市层。<strong>最后，数据中台的数据必须尽可能的覆盖所有的业务过程，</strong>数据中台中每一层的数据也要尽可能完善，让数据使用者尽可能的使用汇总后的数据。</p>
<p>OneData 体系的目标是构建统一的数据规范标准，让数据成为一种资产，而不是成本。资产和成本的差别在于资产是可以沉淀的，是可以被复用的。成本是消耗性质的、是临时的、无法被复用的。</p>
<h3 id="OneService"><a href="#OneService" class="headerlink" title="OneService"></a>OneService</h3><p>OneService，数据即服务，强调数据中台中的数据应该是通过 API 接口的方式被访问。</p>
<p>这里我想提个问题：为什么数据一定要通过 API 接口的方式被访问，不通过 API 接口，直接提供数据表给用户又存在哪些问题呢？</p>
<p>如果你是数据应用开发，当你要开发一个数据产品时，首先要把数据导出到不同的查询引擎上：</p>
<p>数据量小的使用 MySQL；</p>
<p>大的可能用到 HBase；</p>
<p>需要多维分析的可能需要 Greenplum；</p>
<p>实时性要求高的需要用到 Redis；</p>
<p>总的来说，不同的查询引擎，应用开发需要定制不同的访问接口。</p>
<p>如果你是一个数据开发，当某个任务无法按时产出，发生异常时，想要了解这个表可能会影响到下游的哪些应用或者报表，但是却发现单纯依赖表与表的血缘无法触及应用，根本无法知道最后的这些表被哪些应用访问。与此同时，当你想下线一张表时，因为不知道谁访问了这张表，无法实施，最终造成了“上线容易，下线难”的窘境。</p>
<p>而 API 接口一方面对应用开发屏蔽了底层数据存储，使用统一标准的 API 接口查询数据，提高了数据接入的速度。另一方面，对于数据开发，提高了数据应用的管理效率，建立了表到应用的链路关系。</p>
<p>那如何来实现数据服务化呢？</p>
<p><img src="/2022/04/14/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_03-%E5%BB%BA%E8%AE%BE%E4%B8%89%E6%9D%BF%E6%96%A7%EF%BC%9A%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E7%BB%84%E7%BB%87%E5%92%8C%E6%8A%80%E6%9C%AF/OneService.png" alt="OneService"></p>
<p><strong>屏蔽异构数据源：</strong>数据服务必须要能够支撑类型丰富的查询引擎，满足不同场景下数据的查询需求，常见的有 MySQL、HBase、Greenplum、Redis、Elasticsearch 等。</p>
<p><strong>数据网关：</strong>要实现包括权限、监控、流控、日志在内的一系列管控能力，哪个应用的哪个页面访问了哪个模型，要做到实时跟踪，如果有一些模型长时间没有被访问，应该予以下线。使用数据的每个应用都应该通过 accesskey 和 secretkey 实现身份认证和接口权限的管理。另外，访问日志可以方便在访问出现问题时，加快排查速度。</p>
<p><strong>逻辑模型：</strong>从用户的视角出发，屏蔽底层的模型设计的实现，面向用户提供逻辑模型。什么是逻辑模型呢？熟悉数据库的同学应该知道，数据库中有一个视图的概念，视图本身并没有真实的数据，一个视图可以关联一张或者多张表，每次在查询的时候，动态地将不同表的查询结果聚合成视图的查询结果。逻辑模型可以类比视图，它可以帮助应用开发者屏蔽底层的数据物理实现，实现相同粒度的数据构造一个逻辑模型，简化了数据接入的复杂度。</p>
<p><strong>性能和稳定性：</strong>由于数据服务侵入到用户的访问链路，所以对服务的可用性和性能都有很高的要求，数据服务必须是无状态的，可以做到横向扩展。</p>
<p>OneService 体系的目标是提高数据的共享能力，让数据可以被用得好，用得爽。</p>
<h2 id="数据中台支撑技术"><a href="#数据中台支撑技术" class="headerlink" title="数据中台支撑技术"></a>数据中台支撑技术</h2><p>讲完方法论，我们接着要来讲数据中台的支撑技术，因为一个好用的工具，可以让你的数据中台建设事半功倍。</p>
<p><img src="/2022/04/14/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_03-%E5%BB%BA%E8%AE%BE%E4%B8%89%E6%9D%BF%E6%96%A7%EF%BC%9A%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E7%BB%84%E7%BB%87%E5%92%8C%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%94%AF%E6%92%91%E6%8A%80%E6%9C%AF%E4%BD%93%E7%B3%BB.png" alt="数据中台支撑技术体系"></p>
<p>这个图完整地描述了数据中台支撑技术体系，它的底层是以 Hadoop 为代表的大数据计算、存储基础设施，提供了大数据运行所必须的计算、存储资源。</p>
<p>以 HDFS 为代表的分布式文件系统，以 Yarn/Kubernates 为代表的资源调度系统，以 Hive、Spark、Fink 为代表的分布式计算引擎，都属于基础设施范畴。如果把数据中台比作是一个数据工厂，那可以把它们比作是这个工厂的水、电。</p>
<p>在 Hadoop 之上，浅绿色的部分是原有大数据平台范畴内的工具产品，覆盖了从数据集成、数据开发、数据测试到任务运维的整套工具链产品。同时还包括基础的监控运维系统、权限访问控制系统和项目用户的管理系统。由于涉及多人协作，所以还有一个流程协作与通知中心。</p>
<p>灰色的部分，是数据中台的核心组成部分：数据治理模块。它对应的方法论就是 OneData 体系。以元数据中心为基础，在统一了企业所有数据源的元数据基础上，提供了包括数据地图、数仓设计、数据质量、成本优化以及指标管理在内的 5 个产品，分别对应的就是数据发现、模型、质量、成本和指标的治理。</p>
<p>深绿色的部分是数据服务，它是数据中台的门户，对外提供了统一的数据服务，对应的方法论就是 OneService。数据服务向下提供了应用和表的访问关系，使数据血缘可以延申到数据应用，向上支撑了各种数据应用和服务，所有的系统通过统一的 API 接口获取数据。</p>
<p>在数据服务之上，是面向不同场景的数据产品和应用，包括面向非技术人员的自助取数系统；面向数据开发、分析师的自助分析系统；面向敏捷数据分析场景的 BI 产品；活动直播场景下的大屏系统；以及用户画像相关的标签工厂。</p>
<p>这套产品技术支撑体系，覆盖了数据中台建设的整个过程，配合规范化实施，你就可以搭建出一个数据中台，关于具体的细节我会在实现篇中逐一分析讲解，这里你只需要知道这个框架就可以了。</p>
<h2 id="组织架构"><a href="#组织架构" class="headerlink" title="组织架构"></a>组织架构</h2><p>我在开篇提到，在网易电商数据中台建设之前，各个部门都会存在一些小的数仓，<strong>那么你有没有想过，为什么会存在这些分散的小数仓？</strong> 归根结底是因为建设这些数仓的人分散在各个业务部门。所以，如果你要建设数据中台，单纯有方法论和支撑技术还不够，还必须要有一个独立于业务部门的中台团队。</p>
<p>数据中台提供的是一个跨业务部门共享的公共数据能力，所以，承担数据中台建设职责的部门一定是一个独立于业务线的部门。这个部门的负责人应该直接向公司的 CTO 汇报工作，当然这个也要取决于数据中台建设的层次，例如在网易内，有云音乐、严选等多个产品线，数据中台的建设层次是在产品级别的，也就是说，云音乐有一个数据中台，严选有一个数据中台，所以严选的数据中台应该向严选的 CTO 汇报。</p>
<p>而独立部门的最大风险是与业务脱节，所以我们对数据中台的组织定位是：<strong>懂业务，能够深入业务，扎根业务。</strong>数据中台要管理所有的指标，而每个业务线之间的指标既有差异，也有交叉，要理解指标的口径定义，就必须要了解业务的过程。同时，当我们要制定一些新的指标时，必须要了解各个业务线新的业务目标，指标的本质还是为业务目标服务的。</p>
<p>综合来讲，什么样的组织架构是适合数据中台建设的呢？</p>
<p><img src="/2022/04/14/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0_03-%E5%BB%BA%E8%AE%BE%E4%B8%89%E6%9D%BF%E6%96%A7%EF%BC%9A%E6%96%B9%E6%B3%95%E8%AE%BA%E3%80%81%E7%BB%84%E7%BB%87%E5%92%8C%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E7%BB%84%E7%BB%87%E6%9E%B6%E6%9E%84.png" alt="数据中台组织架构"></p>
<p>数据产品部门：负责数据中台、数据产品的体系规划、产品设计、规范制定、应用效果跟进，指标口径的定义和维护（有的部门是由分析师管理）。</p>
<p>数据平台部门：负责研发支撑数据中台构建的产品，例如指标系统、元数据中心、数据地图等。</p>
<p>数据开发团队：负责维护数据中台的公共数据层，满足数据产品制定的数据需求。</p>
<p>应用开发团队：负责开发数据应用产品，比如报表系统、电商中的供应链系统、高层看板、经营分析。</p>
<p>而且，中台组织的绩效目标一定是要与业务落地价值绑定的，比如在电商中，我们提供了供应链决策系统，有智能补货的功能，会根据商品的库存，各个地区的历史销售情况，生产加工周期，自动生成补货决策，由人工审核以后，直接推送给采购系统。那我们评估价值时，我们会拿由系统自动生成的采购计划占整体采购计划的比例来衡量数据的应用价值。</p>
<p>最后，数据中台的组织架构改革涉及原有各个部门的利益，所以这个是数据中台构建最难又不得不做的地方，必须要取得高层领导的支持和重视。</p>
<h2 id="课堂小结"><a href="#课堂小结" class="headerlink" title="课堂小结"></a>课堂小结</h2><p>这节课，我以自己建设数据中台的经历，带领你了解了数据中台建设的三板斧：方法论、支撑技术和组织架构。在课程的最后，我想再强调几个点。</p>
<p>适合数据中台的组织架构是建设数据中台的第一步，数据中台组织一定是独立的部门，同时要避免与业务脱节，深入业务，要与业务目标绑定。</p>
<p>数据中台支撑技术大规模落地，需要有成熟的系统工具作为支撑，同时要注意这些系统工具之间的联动和打通。</p>
<p>数据中台的方法论可以借鉴，但是不能完全照搬，每个公司的数据应用水平和当前遇到的问题都不相同，可以针对这些问题，分阶段制定数据中台的建设计划，选择性的应用一些技术，例如当前最主要的问题是数据质量问题，那就应该优先落地数据质量中心，提升质量。</p>
<p>最后，让我们回到开篇的那个问题，如何建设数据中台？在我看来，方法论、支撑技术和组织架构，对于建设数据中台，三者缺一不可。而且我想再强调一下，数据中台的建设绝对不是为了建中台而建中台，数据中台的建设一定要结合落地场景，可以先从从一些小的场景开始，但是规划一定是要有顶层设计的。关于具体的操作细节，我会在第二部分，用 8 讲的篇幅来讲给你听。</p>
]]></content>
      <categories>
        <category>数据中台</category>
      </categories>
      <tags>
        <tag>数据中台</tag>
      </tags>
  </entry>
  <entry>
    <title>微众平台部署</title>
    <url>/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>以下是微众平台的部署流程。</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/84615498-c3030200-aefb-11ea-9b16-7e4058bf6026.png" alt></p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/84615483-bb435d80-aefb-11ea-81b5-67f62b156628.png" alt></p>
<a id="more"></a>
<hr>
<h4 id="基础环境"><a href="#基础环境" class="headerlink" title="基础环境"></a>基础环境</h4><h5 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h5><table>
<thead>
<tr>
<th align="left"><strong>环境变量 ~/.bashrc</strong></th>
<th align="left"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">变量名</td>
<td align="left">变量路径</td>
</tr>
<tr>
<td align="left">JAVA_HOME</td>
<td align="left">/usr/java/jdk1.8.0_281</td>
</tr>
<tr>
<td align="left">HADOOP_CONF_DIR</td>
<td align="left">/opt/cloudera/parcels/CDH/lib/hadoop/etc/hadoop</td>
</tr>
<tr>
<td align="left">HIVE_CONF_DIR</td>
<td align="left">/opt/cloudera/parcels/CDH/lib/hive/conf</td>
</tr>
<tr>
<td align="left">SPARK_HOME</td>
<td align="left">/opt/cloudera/parcels/CDH/lib/spark</td>
</tr>
<tr>
<td align="left">SPARK_CONF_DIR</td>
<td align="left">/opt/cloudera/parcels/CDH/lib/spark/conf</td>
</tr>
</tbody></table>
<h5 id="环境组件版本"><a href="#环境组件版本" class="headerlink" title="环境组件版本"></a>环境组件版本</h5><table>
<thead>
<tr>
<th align="left"><strong>基础组件版本</strong></th>
<th align="left"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">组件</td>
<td align="left">版本</td>
</tr>
<tr>
<td align="left">Python</td>
<td align="left">2.7.5</td>
</tr>
<tr>
<td align="left">JDK</td>
<td align="left">1.8.0_281</td>
</tr>
<tr>
<td align="left">MySQL</td>
<td align="left">5.7.22</td>
</tr>
<tr>
<td align="left">Nginx</td>
<td align="left">1.16.1</td>
</tr>
<tr>
<td align="left">Hadoop</td>
<td align="left">3.0.0-cdh6.3.2</td>
</tr>
<tr>
<td align="left">Hive</td>
<td align="left">2.1.1-cdh6.3.2</td>
</tr>
<tr>
<td align="left">Spark</td>
<td align="left">2.4.0-cdh6.3.2</td>
</tr>
<tr>
<td align="left">Scala</td>
<td align="left">2.11.12</td>
</tr>
</tbody></table>
<h5 id="以下组件需要确认本地是否能够正常通讯"><a href="#以下组件需要确认本地是否能够正常通讯" class="headerlink" title="以下组件需要确认本地是否能够正常通讯"></a>以下组件需要确认本地是否能够正常通讯</h5><table>
<thead>
<tr>
<th align="left"><strong>环境组件连通性确认</strong></th>
<th align="left"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">组件</td>
<td align="left">确认命令</td>
</tr>
<tr>
<td align="left">MySQL</td>
<td align="left">mysql -h10.0.<em>.</em> -P3306 -uusername -ppassword</td>
</tr>
<tr>
<td align="left">Hadoop</td>
<td align="left">hadoop fs -ls /</td>
</tr>
<tr>
<td align="left">Hive</td>
<td align="left">hive</td>
</tr>
<tr>
<td align="left">Spark</td>
<td align="left">spark-shell</td>
</tr>
</tbody></table>
<h5 id="微众组件使用版本"><a href="#微众组件使用版本" class="headerlink" title="微众组件使用版本"></a>微众组件使用版本</h5><table>
<thead>
<tr>
<th align="left"><strong>微众组件使用版本</strong></th>
<th align="left"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">组件</td>
<td align="left">版本</td>
</tr>
<tr>
<td align="left">wedatasphere-dss</td>
<td align="left">0.9.0</td>
</tr>
<tr>
<td align="left">wedatasphere-dss-web</td>
<td align="left">0.9.0</td>
</tr>
<tr>
<td align="left">wedatasphere-linkis</td>
<td align="left">0.11.0</td>
</tr>
<tr>
<td align="left">wedatasphere-qualitis</td>
<td align="left">0.8.0</td>
</tr>
<tr>
<td align="left">linkis-jobtype</td>
<td align="left">0.9.0</td>
</tr>
<tr>
<td align="left">azkaban-exec-server</td>
<td align="left">3.51.0</td>
</tr>
<tr>
<td align="left">azkaban-web-server</td>
<td align="left">3.51.0</td>
</tr>
<tr>
<td align="left">Visualis</td>
<td align="left">0.3.0-beta.8</td>
</tr>
</tbody></table>
<h4 id="组件编译"><a href="#组件编译" class="headerlink" title="组件编译"></a>组件编译</h4><h5 id="Linkis编译"><a href="#Linkis编译" class="headerlink" title="Linkis编译"></a>Linkis编译</h5><h6 id="pom文件修改"><a href="#pom文件修改" class="headerlink" title="pom文件修改"></a>pom文件修改</h6><p>properties修改</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 hadoop、hive、hbase、spark 版本为CDH集群版本 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>3.0.0-cdh6.3.2<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hive.version</span>&gt;</span>2.1.1-cdh6.3.2<span class="tag">&lt;/<span class="name">hive.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hbase.version</span>&gt;</span>2.1.0-cdh6.3.2<span class="tag">&lt;/<span class="name">hbase.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.0-cdh6.3.2<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">presto.version</span>&gt;</span>0.234<span class="tag">&lt;/<span class="name">presto.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spring.eureka.version</span>&gt;</span>1.4.4.RELEASE<span class="tag">&lt;/<span class="name">spring.eureka.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spring.boot.version</span>&gt;</span>2.0.3.RELEASE<span class="tag">&lt;/<span class="name">spring.boot.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">gson.version</span>&gt;</span>2.8.5<span class="tag">&lt;/<span class="name">gson.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">fasterxml.jackson.version</span>&gt;</span>2.10.0<span class="tag">&lt;/<span class="name">fasterxml.jackson.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">jdk.compile.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">jdk.compile.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin.scala.version</span>&gt;</span>2.15.2<span class="tag">&lt;/<span class="name">plugin.scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">jersey.version</span>&gt;</span>2.16<span class="tag">&lt;/<span class="name">jersey.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">jersey.servlet.version</span>&gt;</span>2.23.1<span class="tag">&lt;/<span class="name">jersey.servlet.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">jetty.version</span>&gt;</span>9.4.11.v20180605<span class="tag">&lt;/<span class="name">jetty.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">httpclient.version</span>&gt;</span>4.5.4<span class="tag">&lt;/<span class="name">httpclient.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">httpmime.version</span>&gt;</span>4.5.4<span class="tag">&lt;/<span class="name">httpmime.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">slf4j.version</span>&gt;</span>1.7.12<span class="tag">&lt;/<span class="name">slf4j.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.version</span>&gt;</span>3.3.3<span class="tag">&lt;/<span class="name">maven.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">xstream.core.version</span>&gt;</span>1.4.11.1<span class="tag">&lt;/<span class="name">xstream.core.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dispatch.version</span>&gt;</span>0.11.2<span class="tag">&lt;/<span class="name">dispatch.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">jsoup.version</span>&gt;</span>1.8.2<span class="tag">&lt;/<span class="name">jsoup.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">harbor.reposity</span>&gt;</span>wedatasphere/linkis<span class="tag">&lt;/<span class="name">harbor.reposity</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>profiles修改</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">profiles</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>product<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">activation</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">activeByDefault</span>&gt;</span>true<span class="tag">&lt;/<span class="name">activeByDefault</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">activation</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">project.release.version</span>&gt;</span>0.11.0<span class="tag">&lt;/<span class="name">project.release.version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">netty.version</span>&gt;</span>4.1.17.Final<span class="tag">&lt;/<span class="name">netty.version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">json4s.version</span>&gt;</span>3.5.3<span class="tag">&lt;/<span class="name">json4s.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 spark版本为spark2.4.0.cloudera1 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>spark2.4.0.cloudera1<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">project.release.version</span>&gt;</span>0.11.0<span class="tag">&lt;/<span class="name">project.release.version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">netty.version</span>&gt;</span>4.1.17.Final<span class="tag">&lt;/<span class="name">netty.version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">json4s.version</span>&gt;</span>3.2.11<span class="tag">&lt;/<span class="name">json4s.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>spark2.2-2.0<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">project.release.version</span>&gt;</span>0.11.0<span class="tag">&lt;/<span class="name">project.release.version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">netty.version</span>&gt;</span>4.0.47.Final<span class="tag">&lt;/<span class="name">netty.version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">json4s.version</span>&gt;</span>3.2.11<span class="tag">&lt;/<span class="name">json4s.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>release<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-source-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">attach</span>&gt;</span>true<span class="tag">&lt;/<span class="name">attach</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">id</span>&gt;</span>create-source-jar<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>jar-no-fork<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>test-jar-no-fork<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">                <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-deploy-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0-M1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-javadoc-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">id</span>&gt;</span>attach-javadocs<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="编译时Maven-setting-xml-配置信息"><a href="#编译时Maven-setting-xml-配置信息" class="headerlink" title="编译时Maven setting.xml 配置信息"></a>编译时Maven setting.xml 配置信息</h6><p>需要注意cloudera相关的镜像配置，避免出现找不到cloudera源的情况</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment">or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment">distributed with this work for additional information</span></span><br><span class="line"><span class="comment">regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment">to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment">"License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment">with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment">Unless required by applicable law or agreed to in writing,</span></span><br><span class="line"><span class="comment">software distributed under the License is distributed on an</span></span><br><span class="line"><span class="comment">"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span></span><br><span class="line"><span class="comment">KIND, either express or implied.  See the License for the</span></span><br><span class="line"><span class="comment">specific language governing permissions and limitations</span></span><br><span class="line"><span class="comment">under the License.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment"> | This is the configuration file for Maven. It can be specified at two levels:</span></span><br><span class="line"><span class="comment"> |</span></span><br><span class="line"><span class="comment"> |  1. User Level. This settings.xml file provides configuration for a single user,</span></span><br><span class="line"><span class="comment"> |                 and is normally provided in $&#123;user.home&#125;/.m2/settings.xml.</span></span><br><span class="line"><span class="comment"> |</span></span><br><span class="line"><span class="comment"> |                 <span class="doctag">NOTE:</span> This location can be overridden with the CLI option:</span></span><br><span class="line"><span class="comment"> |</span></span><br><span class="line"><span class="comment"> |                 -s /path/to/user/settings.xml</span></span><br><span class="line"><span class="comment"> |</span></span><br><span class="line"><span class="comment"> |  2. Global Level. This settings.xml file provides configuration for all Maven</span></span><br><span class="line"><span class="comment"> |                 users on a machine (assuming they're all using the same Maven</span></span><br><span class="line"><span class="comment"> |                 installation). It's normally provided in</span></span><br><span class="line"><span class="comment"> |                 $&#123;maven.conf&#125;/settings.xml.</span></span><br><span class="line"><span class="comment"> |</span></span><br><span class="line"><span class="comment"> |                 <span class="doctag">NOTE:</span> This location can be overridden with the CLI option:</span></span><br><span class="line"><span class="comment"> |</span></span><br><span class="line"><span class="comment"> |                 -gs /path/to/global/settings.xml</span></span><br><span class="line"><span class="comment"> |</span></span><br><span class="line"><span class="comment"> | The sections in this sample file are intended to give you a running start at</span></span><br><span class="line"><span class="comment"> | getting the most out of your Maven installation. Where appropriate, the default</span></span><br><span class="line"><span class="comment"> | values (values used when the setting is not specified) are provided.</span></span><br><span class="line"><span class="comment"> |</span></span><br><span class="line"><span class="comment"> |--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">settings</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/SETTINGS/1.0.0"</span></span></span><br><span class="line"><span class="tag">          <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">          <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- localRepository</span></span><br><span class="line"><span class="comment">   | The path to the local repository maven will use to store artifacts.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   | Default: $&#123;user.home&#125;/.m2/repository</span></span><br><span class="line"><span class="comment">  &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;</span></span><br><span class="line"><span class="comment">  --&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!-- interactiveMode</span></span><br><span class="line"><span class="comment">   | This will determine whether maven prompts you when it needs input. If set to false,</span></span><br><span class="line"><span class="comment">   | maven will use a sensible default value, perhaps based on some other setting, for</span></span><br><span class="line"><span class="comment">   | the parameter in question.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   | Default: true</span></span><br><span class="line"><span class="comment">  &lt;interactiveMode&gt;true&lt;/interactiveMode&gt;</span></span><br><span class="line"><span class="comment">  --&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!-- offline</span></span><br><span class="line"><span class="comment">   | Determines whether maven should attempt to connect to the network when executing a build.</span></span><br><span class="line"><span class="comment">   | This will have an effect on artifact downloads, artifact deployment, and others.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   | Default: false</span></span><br><span class="line"><span class="comment">  &lt;offline&gt;false&lt;/offline&gt;</span></span><br><span class="line"><span class="comment">  --&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!-- pluginGroups</span></span><br><span class="line"><span class="comment">   | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.</span></span><br><span class="line"><span class="comment">   | when invoking a command line like "mvn prefix:goal". Maven will automatically add the group identifiers</span></span><br><span class="line"><span class="comment">   | "org.apache.maven.plugins" and "org.codehaus.mojo" if these are not already contained in the list.</span></span><br><span class="line"><span class="comment">   |--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pluginGroups</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- pluginGroup</span></span><br><span class="line"><span class="comment">     | Specifies a further group identifier to use for plugin lookup.</span></span><br><span class="line"><span class="comment">    &lt;pluginGroup&gt;com.your.plugins&lt;/pluginGroup&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pluginGroups</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!-- proxies</span></span><br><span class="line"><span class="comment">   | This is a list of proxies which can be used on this machine to connect to the network.</span></span><br><span class="line"><span class="comment">   | Unless otherwise specified (by system property or command-line switch), the first proxy</span></span><br><span class="line"><span class="comment">   | specification in this list marked as active will be used.</span></span><br><span class="line"><span class="comment">   |--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">proxies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- proxy</span></span><br><span class="line"><span class="comment">     | Specification for one proxy, to be used in connecting to the network.</span></span><br><span class="line"><span class="comment">     |</span></span><br><span class="line"><span class="comment">    &lt;proxy&gt;</span></span><br><span class="line"><span class="comment">      &lt;id&gt;optional&lt;/id&gt;</span></span><br><span class="line"><span class="comment">      &lt;active&gt;true&lt;/active&gt;</span></span><br><span class="line"><span class="comment">      &lt;protocol&gt;http&lt;/protocol&gt;</span></span><br><span class="line"><span class="comment">      &lt;username&gt;proxyuser&lt;/username&gt;</span></span><br><span class="line"><span class="comment">      &lt;password&gt;proxypass&lt;/password&gt;</span></span><br><span class="line"><span class="comment">      &lt;host&gt;proxy.host.net&lt;/host&gt;</span></span><br><span class="line"><span class="comment">      &lt;port&gt;80&lt;/port&gt;</span></span><br><span class="line"><span class="comment">      &lt;nonProxyHosts&gt;local.net|some.host.com&lt;/nonProxyHosts&gt;</span></span><br><span class="line"><span class="comment">    &lt;/proxy&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">proxies</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!-- servers</span></span><br><span class="line"><span class="comment">   | This is a list of authentication profiles, keyed by the server-id used within the system.</span></span><br><span class="line"><span class="comment">   | Authentication profiles can be used whenever maven must make a connection to a remote server.</span></span><br><span class="line"><span class="comment">   |--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">servers</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- server</span></span><br><span class="line"><span class="comment">     | Specifies the authentication information to use when connecting to a particular server, identified by</span></span><br><span class="line"><span class="comment">     | a unique name within the system (referred to by the 'id' attribute below).</span></span><br><span class="line"><span class="comment">     |</span></span><br><span class="line"><span class="comment">     | <span class="doctag">NOTE:</span> You should either specify username/password OR privateKey/passphrase, since these pairings are</span></span><br><span class="line"><span class="comment">     |       used together.</span></span><br><span class="line"><span class="comment">     |</span></span><br><span class="line"><span class="comment">    &lt;server&gt;</span></span><br><span class="line"><span class="comment">      &lt;id&gt;deploymentRepo&lt;/id&gt;</span></span><br><span class="line"><span class="comment">      &lt;username&gt;repouser&lt;/username&gt;</span></span><br><span class="line"><span class="comment">      &lt;password&gt;repopwd&lt;/password&gt;</span></span><br><span class="line"><span class="comment">    &lt;/server&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- Another sample, using keys to authenticate.</span></span><br><span class="line"><span class="comment">    &lt;server&gt;</span></span><br><span class="line"><span class="comment">      &lt;id&gt;siteServer&lt;/id&gt;</span></span><br><span class="line"><span class="comment">      &lt;privateKey&gt;/path/to/private/key&lt;/privateKey&gt;</span></span><br><span class="line"><span class="comment">      &lt;passphrase&gt;optional; leave empty if not used.&lt;/passphrase&gt;</span></span><br><span class="line"><span class="comment">    &lt;/server&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">servers</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!-- mirrors</span></span><br><span class="line"><span class="comment">   | This is a list of mirrors to be used in downloading artifacts from remote repositories.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   | It works like this: a POM may declare a repository to use in resolving certain artifacts.</span></span><br><span class="line"><span class="comment">   | However, this repository may have problems with heavy traffic at times, so people have mirrored</span></span><br><span class="line"><span class="comment">   | it to several places.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   | That repository definition will have a unique id, so we can create a mirror reference for that</span></span><br><span class="line"><span class="comment">   | repository, to be used as an alternate download site. The mirror site will be the preferred</span></span><br><span class="line"><span class="comment">   | server for that repository.</span></span><br><span class="line"><span class="comment">   |--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">mirrors</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- mirror</span></span><br><span class="line"><span class="comment">     | Specifies a repository mirror site to use instead of a given repository. The repository that</span></span><br><span class="line"><span class="comment">     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used</span></span><br><span class="line"><span class="comment">     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.</span></span><br><span class="line"><span class="comment">     |</span></span><br><span class="line"><span class="comment">    &lt;mirror&gt;</span></span><br><span class="line"><span class="comment">      &lt;id&gt;mirrorId&lt;/id&gt;</span></span><br><span class="line"><span class="comment">      &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt;</span></span><br><span class="line"><span class="comment">      &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt;</span></span><br><span class="line"><span class="comment">      &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt;</span></span><br><span class="line"><span class="comment">    &lt;/mirror&gt;</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/repositories/central/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">id</span>&gt;</span>jFog<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>bintray<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://jcenter.bintray.com/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">id</span>&gt;</span>repository.jboss.org-public<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>JBoss repository<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.jboss.org/nexus/content/groups/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">mirrors</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!-- profiles</span></span><br><span class="line"><span class="comment">   | This is a list of profiles which can be activated in a variety of ways, and which can modify</span></span><br><span class="line"><span class="comment">   | the build process. Profiles provided in the settings.xml are intended to provide local machine-</span></span><br><span class="line"><span class="comment">   | specific paths and repository locations which allow the build to work in the local environment.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   | For example, if you have an integration testing plugin - like cactus - that needs to know where</span></span><br><span class="line"><span class="comment">   | your Tomcat instance is installed, you can provide a variable here such that the variable is</span></span><br><span class="line"><span class="comment">   | dereferenced during the build process to configure the cactus plugin.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles</span></span><br><span class="line"><span class="comment">   | section of this document (settings.xml) - will be discussed later. Another way essentially</span></span><br><span class="line"><span class="comment">   | relies on the detection of a system property, either matching a particular value for the property,</span></span><br><span class="line"><span class="comment">   | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a</span></span><br><span class="line"><span class="comment">   | value of '1.4' might activate a profile when the build is executed on a JDK version of '1.4.2_07'.</span></span><br><span class="line"><span class="comment">   | Finally, the list of active profiles can be specified directly from the command line.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   | <span class="doctag">NOTE:</span> For profiles defined in the settings.xml, you are restricted to specifying only artifact</span></span><br><span class="line"><span class="comment">   |       repositories, plugin repositories, and free-form properties to be used as configuration</span></span><br><span class="line"><span class="comment">   |       variables for plugins in the POM.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">   |--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">profiles</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>boundlessgeo<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">id</span>&gt;</span>boundlessgeo<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repo.boundlessgeo.com/main/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">updatePolicy</span>&gt;</span>always<span class="tag">&lt;/<span class="name">updatePolicy</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">id</span>&gt;</span>aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">updatePolicy</span>&gt;</span>always<span class="tag">&lt;/<span class="name">updatePolicy</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>maven-central<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">id</span>&gt;</span>maven-central<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://central.maven.org/maven2/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">updatePolicy</span>&gt;</span>always<span class="tag">&lt;/<span class="name">updatePolicy</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">updatePolicy</span>&gt;</span>always<span class="tag">&lt;/<span class="name">updatePolicy</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!-- activeProfiles</span></span><br><span class="line"><span class="comment">   | List of profiles that are active for all builds.</span></span><br><span class="line"><span class="comment">   |</span></span><br><span class="line"><span class="comment">  &lt;activeProfiles&gt;</span></span><br><span class="line"><span class="comment">    &lt;activeProfile&gt;alwaysActiveProfile&lt;/activeProfile&gt;</span></span><br><span class="line"><span class="comment">    &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;/activeProfile&gt;</span></span><br><span class="line"><span class="comment">  &lt;/activeProfiles&gt;</span></span><br><span class="line"><span class="comment">  --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">activeProfiles</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">activeProfile</span>&gt;</span>boundlessgeo<span class="tag">&lt;/<span class="name">activeProfile</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">activeProfile</span>&gt;</span>aliyun<span class="tag">&lt;/<span class="name">activeProfile</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">activeProfile</span>&gt;</span>maven-central<span class="tag">&lt;/<span class="name">activeProfile</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">activeProfile</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">activeProfile</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">activeProfiles</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">settings</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="编译命令"><a href="#编译命令" class="headerlink" title="编译命令"></a>编译命令</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在Linkis pom.xml同级路径执行一下命令</span></span><br><span class="line">mvn -N install</span><br><span class="line">mvn clean install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取安装包，在工程的assembly -&gt; target目录下：</span></span><br><span class="line">wedatasphere-linkis-0.11.0-dist.tar.gz</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-12_11-12-29.png" alt></p>
<h5 id="Qualitis编译"><a href="#Qualitis编译" class="headerlink" title="Qualitis编译"></a>Qualitis编译</h5><h6 id="编译命令-1"><a href="#编译命令-1" class="headerlink" title="编译命令"></a>编译命令</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在Qualitis build.gradle同级路径执行一下命令</span></span><br><span class="line">gradle clean distZip</span><br></pre></td></tr></table></figure>

<h5 id="DSS编译"><a href="#DSS编译" class="headerlink" title="DSS编译"></a>DSS编译</h5><h6 id="pom文件修改-1"><a href="#pom文件修改-1" class="headerlink" title="pom文件修改"></a>pom文件修改</h6><p>properties修改</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dss.version</span>&gt;</span>0.9.0<span class="tag">&lt;/<span class="name">dss.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">linkis.version</span>&gt;</span>0.11.0<span class="tag">&lt;/<span class="name">linkis.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">jdk.compile.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">jdk.compile.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.version</span>&gt;</span>3.3.3<span class="tag">&lt;/<span class="name">maven.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="编译命令-2"><a href="#编译命令-2" class="headerlink" title="编译命令"></a>编译命令</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在DSS pom.xml同级路径执行一下命令</span></span><br><span class="line">mvn -N install</span><br><span class="line">mvn clean install</span><br></pre></td></tr></table></figure>

<h4 id="组件部署"><a href="#组件部署" class="headerlink" title="组件部署"></a>组件部署</h4><h5 id="Nginx部署"><a href="#Nginx部署" class="headerlink" title="Nginx部署"></a>Nginx部署</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> First update the system software packages to the latest version.</span></span><br><span class="line">yum -y update</span><br><span class="line"><span class="meta">#</span><span class="bash"> Next, install Nginx</span></span><br><span class="line">yum install epel-release</span><br><span class="line">yum install nginx</span><br><span class="line"><span class="meta">#</span><span class="bash"> Once Nginx web server installed, you can start it first time and <span class="built_in">enable</span> it to start automatically at system boot.</span></span><br><span class="line">systemctl start nginx</span><br><span class="line">systemctl enable nginx</span><br><span class="line">systemctl status nginx</span><br><span class="line"><span class="meta">#</span><span class="bash"> By default, CentOS 7 built-in firewall is <span class="built_in">set</span> to block Nginx traffic.</span></span><br><span class="line">firewall-cmd --zone=public --permanent --add-service=http</span><br><span class="line">firewall-cmd --zone=public --permanent --add-service=https</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-12_13-34-53.png" alt></p>
<h5 id="DSS-Linkis部署"><a href="#DSS-Linkis部署" class="headerlink" title="DSS_Linkis部署"></a>DSS_Linkis部署</h5><p>由于DataSphere Studio依赖于Linkis，官方文档提供了两种部署方式：</p>
<ol>
<li><p>DSS &amp; Linkis 一键部署<br>该模式适合于DSS和Linkis都没有安装的情况。</p>
<p>进入DSS &amp; Linkis安装环境准备</p>
</li>
<li><p>DSS 一键部署<br>该模式适合于Linkis已经安装，需要安装DSS的情况。</p>
<p><strong>由于官方的一键部署不完美，Azkaban和Qualitis自动安装集成会有很多问题，很深的坑。</strong></p>
<p><strong>本次选择的是第一种方式安装大部分组件，Azkaban和Qualitis手动部署的方式。</strong></p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── dss_linkis # 一键部署主目录</span><br><span class="line"> ├── backup    # 用于兼容Linkis老版本的安装启动脚本</span><br><span class="line"> ├── bin       # 用于一键安装启动DSS+Linkis</span><br><span class="line"> ├── conf      # 一键部署的配置文件</span><br><span class="line"> ├── azkaban-solo-server-x.x.x.tar.gz       # Azkaban安装包 Ps：本次安装使用的是分布式的Azkaban，单独与Linkis进行了对接。</span><br><span class="line"> ├── linkis-jobtype-x.x.x.zip               # Linkis jobtype安装包</span><br><span class="line"> ├── wedatasphere-dss-x.x.x-dist.tar.gz     # DSS后台安装包</span><br><span class="line"> ├── wedatasphere-dss-web-x.x.x-dist.zip    # DSS前端安装包</span><br><span class="line"> ├── wedatasphere-linkis-x.x.x-dist.tar.gz  # Linkis安装包</span><br><span class="line"> ├── wedatasphere-qualitis-x.x.x.zip        # Qualitis安装包 Ps：本次安装单独对Qualitis进行了部署，单独与Linkis进行了对接。</span><br></pre></td></tr></table></figure>

<h5 id="一键部署配置文件修改"><a href="#一键部署配置文件修改" class="headerlink" title="一键部署配置文件修改"></a>一键部署配置文件修改</h5><h6 id="修改config-sh"><a href="#修改config-sh" class="headerlink" title="修改config.sh"></a>修改config.sh</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将conf目录下的config.sh.lite.template，修改为config.sh</span></span><br><span class="line">cp conf/config.sh.lite.template conf/config.sh</span><br><span class="line">vi conf/config.sh </span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> ssh默认端口</span></span><br><span class="line">SSH_PORT=22</span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取当前用户为部署用户</span></span><br><span class="line">deployUser="`whoami`"</span><br><span class="line"><span class="meta">#</span><span class="bash"> 工作空间路径，默认为本地路径，尽量提前创建并授于写权限</span></span><br><span class="line">WORKSPACE_USER_ROOT_PATH=file:///tmp/linkis/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 结果集路径，默认为本地路径，尽量提前创建并授于写权限</span></span><br><span class="line">RESULT_SET_ROOT_PATH=file:///tmp/linkis</span><br><span class="line">DSS_NGINX_IP=127.0.0.1 #DSS Nginx访问IP</span><br><span class="line">DSS_WEB_PORT=8088 #DSS Web页面访问端口</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> azkaban address <span class="keyword">for</span> check</span></span><br><span class="line">AZKABAN_ADRESS_IP=10.0.*.*</span><br><span class="line">AZKABAN_ADRESS_PORT=8082</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> qualitis address <span class="keyword">for</span> check</span></span><br><span class="line">QUALITIS_ADRESS_IP=10.0.*.*</span><br><span class="line">QUALITIS_ADRESS_PORT=8099</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> hive metastore</span></span><br><span class="line">HIVE_META_URL=jdbc:mysql://10.0.*.*/metastore?useUnicode=true</span><br><span class="line">HIVE_META_USER=database_username</span><br><span class="line">HIVE_META_PASSWORD=database_password</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> HADOOP CONF DIR</span></span><br><span class="line">HADOOP_CONF_DIR=/opt/cloudera/parcels/CDH/lib/hadoop/etc/hadoop</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> HIVE CONF DIR</span></span><br><span class="line">HIVE_CONF_DIR=/opt/cloudera/parcels/CDH/lib/hive/conf</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> SPARK CONF DIR</span></span><br><span class="line">SPARK_CONF_DIR=/opt/cloudera/parcels/CDH/lib/spark/conf</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#########################################################################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">###The following parameters can be modified by the user as required, but not necessary###</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#########################################################################################</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> java application default jvm memory</span></span><br><span class="line">export SERVER_HEAP_SIZE="512M"</span><br><span class="line"> </span><br><span class="line">LINKIS_VERSION=0.11.0</span><br><span class="line"> </span><br><span class="line">DSS_VERSION=0.9.0</span><br></pre></td></tr></table></figure>

<h6 id="修改db-sh"><a href="#修改db-sh" class="headerlink" title="修改db.sh"></a>修改db.sh</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置数据库的连接信息</span></span><br><span class="line">MYSQL_HOST=10.0.*.*</span><br><span class="line">MYSQL_PORT=3306</span><br><span class="line">MYSQL_DB=webank</span><br><span class="line">MYSQL_USER=database_username</span><br><span class="line">MYSQL_PASSWORD=database_password</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 说明：此为必须配置参数，并确保可以从本机进行访问，验证方式：</span></span><br><span class="line">mysql -h$MYSQL_HOST -P$MYSQL_PORT -u$MYSQL_USER -p$MYSQL_PASSWORD</span><br></pre></td></tr></table></figure>

<h6 id="增加环境变量"><a href="#增加环境变量" class="headerlink" title="增加环境变量"></a>增加环境变量</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改安装用户的.bash_rc，命令如下：</span></span><br><span class="line">vi ~/.bash_rc</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加环境变量如下：</span></span><br><span class="line"><span class="meta">#</span><span class="bash">JDK</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_281</span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP</span></span><br><span class="line">export HADOOP_CONF_DIR=/opt/cloudera/parcels/CDH/lib/hadoop/etc/hadoop</span><br><span class="line"><span class="meta">#</span><span class="bash">Hive</span></span><br><span class="line">export HIVE_CONF_DIR=/opt/cloudera/parcels/CDH/lib/hive/conf</span><br><span class="line"><span class="meta">#</span><span class="bash">Spark</span></span><br><span class="line">export SPARK_HOME=/opt/cloudera/parcels/CDH/lib/spark</span><br><span class="line">export SPARK_CONF_DIR=/opt/cloudera/parcels/CDH/lib/spark/conf</span><br></pre></td></tr></table></figure>

<h5 id="安装前必要指令确认"><a href="#安装前必要指令确认" class="headerlink" title="安装前必要指令确认"></a>安装前必要指令确认</h5><h6 id="指令确认"><a href="#指令确认" class="headerlink" title="指令确认"></a>指令确认</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">该安装脚本会检查各项集成环境命令，如果没有请按照提示进行安装，以下命令为必须项： yum java mysql unzip expect telnet tar sed dos2unix nginx</span><br></pre></td></tr></table></figure>

<h6 id="alias-去掉cp、mv、rm的别名"><a href="#alias-去掉cp、mv、rm的别名" class="headerlink" title="alias 去掉cp、mv、rm的别名"></a>alias 去掉cp、mv、rm的别名</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">安装过程如果有很多cp 命令提示您是否覆盖安装，说明您的系统配置有别名，输入alias，如果有cp、mv、rm的别名，如果有可以去掉，就不会再有大量提示</span><br><span class="line">vi ~/.bashrc</span><br><span class="line">注释掉以下三行</span><br><span class="line"> alias rm='rm -i'</span><br><span class="line"> alias cp='cp -i'</span><br><span class="line"> alias mv='mv -i'</span><br></pre></td></tr></table></figure>

<h6 id="开始一键部署"><a href="#开始一键部署" class="headerlink" title="开始一键部署"></a>开始一键部署</h6><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 执行安装脚本：</span><br><span class="line">sh bin/install.sh</span><br><span class="line">注意：安装脚本有两处是相对路径，为了正确安装，请按照以上命令执行。</span><br><span class="line"> </span><br><span class="line">2. 安装步骤</span><br><span class="line">install.sh脚本会询问您安装模式。 安装模式分为精简版、标准版，请根据您准备的环境情况，选择合适的安装模式。  本次选择的是标准版</span><br><span class="line">install.sh脚本会询问您是否需要初始化数据库并导入元数据，linkis和dss 均会询问。   本次选择的是初始化数据库并导入元数据</span><br><span class="line">第一次安装必须选是。</span><br><span class="line"> </span><br><span class="line">ps.为了数据库、元数据不出现问题，多次重复安装时，都选择初始化数据库并导入元数据。</span><br></pre></td></tr></table></figure>

<h6 id="登录Eureka页面确认服务启动情况"><a href="#登录Eureka页面确认服务启动情况" class="headerlink" title="登录Eureka页面确认服务启动情况"></a>登录Eureka页面确认服务启动情况</h6><p>以下是正常运行时的Eureka服务</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-12_14-58-53.png" alt></p>
<h6 id="登录DSS-Web页面确认"><a href="#登录DSS-Web页面确认" class="headerlink" title="登录DSS Web页面确认"></a>登录DSS Web页面确认</h6><p><a href="http://dss_nginx_ipdss_web_port/" target="_blank" rel="noopener">http://DSS_NGINX_IP:DSS_WEB_PORT</a> 启动日志会打印此访问地址。</p>
<p>登陆时管理员的用户名和密码均为部署用户名，</p>
<p>如部署用户为root，则管理员的用户名/密码为：root/root。</p>
<p><strong>至次部署完成</strong></p>
<h4 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a>可能遇到的问题</h4><h5 id="DSS前端界面报错-Unexpected-token-lt-in-JSON-at-position-0"><a href="#DSS前端界面报错-Unexpected-token-lt-in-JSON-at-position-0" class="headerlink" title="DSS前端界面报错 [ Unexpected token &lt; in JSON at position 0 ]"></a>DSS前端界面报错 [ Unexpected token &lt; in JSON at position 0 ]</h5><p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_10-2-37.png" alt></p>
<p><strong>解决方法：</strong></p>
<ol>
<li><p>检查前端配置文件中linkis gateway的url配置</p>
<p>确认到网关端口为9002</p>
</li>
</ol>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_10-7-54.png" alt></p>
<ol start="2">
<li><p>确认Nginx配置</p>
<p>修改Nginx配置，后端Linkis的地址端口与网关9002一致</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_10-20-54.png" alt></p>
</li>
<li><p>Nginx reload 配置</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nginx -s reload</span><br></pre></td></tr></table></figure>



<h5 id="DSS工作流创建失败-api-rest-j-v1-bml-upload-request-failed"><a href="#DSS工作流创建失败-api-rest-j-v1-bml-upload-request-failed" class="headerlink" title="DSS工作流创建失败 [ /api/rest_j/v1/bml/upload request failed! ]"></a>DSS工作流创建失败 [ /api/rest_j/v1/bml/upload request failed! ]</h5><p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/20210110214940116.png" alt></p>
<p>修改如下配置，重启后问题解决</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_11-20-49.png" alt></p>
<h5 id="DSS创建工作流时报错-No-FileSystem-for-scheme-hdfs问题"><a href="#DSS创建工作流时报错-No-FileSystem-for-scheme-hdfs问题" class="headerlink" title="DSS创建工作流时报错 [ No FileSystem for scheme: hdfs问题 ]"></a>DSS创建工作流时报错 [ No FileSystem for scheme: hdfs问题 ]</h5><p>该问题是linkis/linkis-bml报错，缺少FileSystem类。</p>
<p>在以下路径找到hadoop-common-*.jar</p>
<p>/opt/dss_linkis/linkis/linkis-bml/lib</p>
<p>修改jar包中的pom.xml文件，增加以下依赖。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.hdfs.impl<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.DistributedFileSystem<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The FileSystem for hdfs: uris.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>找到与环境匹配的依赖类的jar包 hadoop-hdfs-client-*.jar</p>
<p>本次使用的是hadoop-hdfs-client-3.0.0-cdh6.3.2.jar</p>
<p>ps：jar包在这个位置有 /opt/cloudera/parcels/CDH/jars</p>
<p>将修改后的jar包和依赖的jar包（包名如下）放到 /opt/dss_linkis/linkis/linkis-bml/lib</p>
<p>hadoop-common-3.0.0-cdh6.3.2.jar</p>
<p>hadoop-hdfs-client-3.0.0-cdh6.3.2.jar</p>
<p><strong>重启后问题解决</strong></p>
<h5 id="DSS工作流保存失败问题-api-rest-j-v1-bml-upload-request-failed"><a href="#DSS工作流保存失败问题-api-rest-j-v1-bml-upload-request-failed" class="headerlink" title="DSS工作流保存失败问题 [ /api/rest_j/v1/bml/upload request failed! ]"></a>DSS工作流保存失败问题 [ /api/rest_j/v1/bml/upload request failed! ]</h5><p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_10-33-36.png" alt></p>
<p><strong>解决方法：</strong></p>
<p>因为将修改网关端口为9002，而public service源码中网关端口为9001，所以需要在配置文件中覆盖端口配置。</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_10-51-38.png" alt></p>
<p>修改如下配置，重启后问题解决</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_10-57-3.png" alt></p>
<h5 id="Scriptis数据库读取不到信息的问题"><a href="#Scriptis数据库读取不到信息的问题" class="headerlink" title="Scriptis数据库读取不到信息的问题"></a>Scriptis数据库读取不到信息的问题</h5><p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_13-15-38.png" alt></p>
<p>解决方法：</p>
<p>修改linkis-metadata配置文件</p>
<p>配置元数据库配置，重启linkis-metadata后该问题解决。</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_13-18-54.png" alt></p>
<h5 id="Scriptis查询完成后可视化报错"><a href="#Scriptis查询完成后可视化报错" class="headerlink" title="Scriptis查询完成后可视化报错"></a>Scriptis查询完成后可视化报错</h5><p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_11-7-16.png" alt></p>
<p>与webank官方确认，是Linkis0.11.0的Bug，在1.0版本修复。</p>
<h5 id="Qualitis执行失败-NoSuchDatabaseException-Database-‘-ind’-not-found"><a href="#Qualitis执行失败-NoSuchDatabaseException-Database-‘-ind’-not-found" class="headerlink" title="Qualitis执行失败 [ NoSuchDatabaseException: Database ‘*****_ind’ not found; ]"></a>Qualitis执行失败 [ NoSuchDatabaseException: Database ‘*****_ind’ not found; ]</h5><p>解决方法：</p>
<p>需要在hive里面建立一个username_ind的数据库，然后授权给登录用户：username</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_13-43-36.png" alt></p>
<h5 id="Qualitis执行失败-java-lang-ClassNotFoundException-com-mysql-jdbc-Driver"><a href="#Qualitis执行失败-java-lang-ClassNotFoundException-com-mysql-jdbc-Driver" class="headerlink" title="Qualitis执行失败 [ java.lang.ClassNotFoundException: com.mysql.jdbc.Driver ]"></a>Qualitis执行失败 [ java.lang.ClassNotFoundException: com.mysql.jdbc.Driver ]</h5><p>解决方法：</p>
<p>在Spark 各个节点jars路径下增加mysql-connector-java-8.0.17.jar</p>
<p>Spark jars路径：/opt/cloudera/parcels/CDH/lib/spark/jars</p>
<h5 id="工作流发布失败、Azkaban登录失败"><a href="#工作流发布失败、Azkaban登录失败" class="headerlink" title="工作流发布失败、Azkaban登录失败"></a>工作流发布失败、Azkaban登录失败</h5><p>Azkaban需要创建与DSS对应的用户，并赋权限。</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_14-8-14.png" alt></p>
<p>创建完成后重启Azkaban后问题解决。</p>
<h5 id="工作流Sendemail功能增加"><a href="#工作流Sendemail功能增加" class="headerlink" title="工作流Sendemail功能增加"></a>工作流Sendemail功能增加</h5><p>微众平台工作流有邮件发送功能模块，但是开源代码中没有整合进来，需要自行修改源码增加这个功能。</p>
<p>源码地址：<a href="https://github.com/WeBankFinTech/DataSphereStudio" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio</a></p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_14-12-10.png" alt></p>
<p>编译后，需要将jar包放置到指定的位置。 jar包需要放置到dss-server和linkis-appjoint-entrance两个微服务中，</p>
<p>以linkis-appjoint-entrance 为例(dss-server与linkis-appjoint-entrance一致)， 在linkis-appjont-entrance下面的lib的同级目录有一个appjoints目录，目录层次如图所示</p>
<p><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-17_10-31-50.png" alt></p>
<p>依赖包和已经实现的jar包统一放置到lib目录中。</p>
<p>lib依赖包清单如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 62983 Mar 10 16:43 activation-1.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 3482 Mar 10 16:43 animal-sniffer-annotations-1.14.jar</span><br><span class="line">-rw-r--r-- 1 root root 74557 Mar 10 16:43 annotations-2.0.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 445288 Mar 10 16:43 antlr-2.7.7.jar</span><br><span class="line">-rw-r--r-- 1 root root 164368 Mar 10 16:43 antlr-runtime-3.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 4467 Mar 10 16:43 aopalliance-1.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 14765 Mar 10 16:43 aopalliance-repackaged-2.4.0-b31.jar</span><br><span class="line">-rw-r--r-- 1 root root 44925 Mar 10 16:43 apacheds-i18n-2.0.0-M15.jar</span><br><span class="line">-rw-r--r-- 1 root root 691479 Mar 10 16:43 apacheds-kerberos-codec-2.0.0-M15.jar</span><br><span class="line">-rw-r--r-- 1 root root 243137 Mar 10 16:43 apache-el-8.5.24.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 16560 Mar 10 16:43 api-asn1-api-1.0.0-M20.jar</span><br><span class="line">-rw-r--r-- 1 root root 79912 Mar 10 16:43 api-util-1.0.0-M20.jar</span><br><span class="line">-rw-r--r-- 1 root root 139611 Mar 10 16:43 archaius-core-0.7.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 43581 Mar 10 16:43 asm-3.3.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 398782 Mar 10 16:43 asm-all-repackaged-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 32041 Mar 10 16:43 asm-analysis-6.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 78919 Mar 10 16:43 asm-commons-6.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 50370 Mar 10 16:43 asm-tree-6.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 1930381 Mar 10 16:43 aspectjweaver-1.8.13.jar</span><br><span class="line">-rw-r--r-- 1 root root 685403 Mar 10 16:43 bcpkix-jdk15on-1.56.jar</span><br><span class="line">-rw-r--r-- 1 root root 3448507 Mar 10 16:43 bcprov-jdk15on-1.56.jar</span><br><span class="line">-rw-r--r-- 1 root root 813288 Mar 10 16:43 bean-validator-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 287192 Mar 10 16:43 cglib-2.2.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 343222 Mar 10 16:43 checker-qual-2.0.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 88035 Mar 10 16:43 class-model-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 188671 Mar 10 16:43 commons-beanutils-1.7.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 206035 Mar 10 16:43 commons-beanutils-core-1.8.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 41123 Mar 10 16:43 commons-cli-1.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 284184 Mar 10 16:43 commons-codec-1.10.jar</span><br><span class="line">-rw-r--r-- 1 root root 588337 Mar 10 16:43 commons-collections-3.2.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 751238 Mar 10 16:43 commons-collections4-4.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 241367 Mar 10 16:43 commons-compress-1.4.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 298829 Mar 10 16:43 commons-configuration-1.6.jar</span><br><span class="line">-rw-r--r-- 1 root root 24239 Mar 10 16:43 commons-daemon-1.0.13.jar</span><br><span class="line">-rw-r--r-- 1 root root 160519 Mar 10 16:43 commons-dbcp-1.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 143602 Mar 10 16:43 commons-digester-1.8.jar</span><br><span class="line">-rw-r--r-- 1 root root 305001 Mar 10 16:43 commons-httpclient-3.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 185140 Mar 10 16:43 commons-io-2.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 299994 Mar 10 16:43 commons-jxpath-1.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 284220 Mar 10 16:43 commons-lang-2.6.jar</span><br><span class="line">-rw-r--r-- 1 root root 61829 Mar 10 16:43 commons-logging-1.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 988514 Mar 10 16:43 commons-math-2.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 1599627 Mar 10 16:43 commons-math3-3.1.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 273370 Mar 10 16:43 commons-net-3.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 96221 Mar 10 16:43 commons-pool-1.5.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 8460 Mar 10 16:43 config-types-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 69500 Mar 10 16:43 curator-client-2.7.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 186273 Mar 10 16:43 curator-framework-2.7.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 270342 Mar 10 16:43 curator-recipes-2.7.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 98365 Mar 10 16:43 curvesapi-1.04.jar</span><br><span class="line">-rw-r--r-- 1 root root 50858 Mar 10 16:43 dss-appjoint-core-0.9.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 39775 Mar 10 16:43 dss-common-0.9.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 112806 Mar 10 16:43 dss-sendmail-appjoint-core-0.9.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 13704 Mar 10 16:43 error_prone_annotations-2.1.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 409687 Mar 10 16:42 eureka-client-1.7.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 252899 Mar 10 16:42 eureka-core-1.7.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 241622 Mar 10 16:42 gson-2.8.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 2734339 Mar 10 16:42 guava-25.1-jre.jar</span><br><span class="line">-rw-r--r-- 1 root root 674028 Mar 10 16:42 guice-4.1.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 17385 Mar 10 16:42 hadoop-annotations-2.7.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 70685 Mar 10 16:42 hadoop-auth-2.7.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 3443040 Mar 10 16:42 hadoop-common-2.7.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 8268375 Mar 10 16:42 hadoop-hdfs-2.7.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 117910 Mar 10 16:42 HdrHistogram-2.1.10.jar</span><br><span class="line">-rw-r--r-- 1 root root 3227 Mar 10 16:42 hk2-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 173037 Mar 10 16:42 hk2-api-2.4.0-b31.jar</span><br><span class="line">-rw-r--r-- 1 root root 205644 Mar 10 16:42 hk2-config-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 117502 Mar 10 16:42 hk2-core-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 179705 Mar 10 16:42 hk2-locator-2.4.0-b31.jar</span><br><span class="line">-rw-r--r-- 1 root root 54379 Mar 10 16:42 hk2-runlevel-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 118973 Mar 10 16:42 hk2-utils-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 1475955 Mar 10 16:42 htrace-core-3.1.0-incubating.jar</span><br><span class="line">-rw-r--r-- 1 root root 781831 Mar 10 16:42 httpclient-4.5.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 325123 Mar 10 16:42 httpcore-4.4.7.jar</span><br><span class="line">-rw-r--r-- 1 root root 41752 Mar 10 16:42 httpmime-4.5.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 286596 Mar 10 16:42 hystrix-core-1.4.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 5925194 Mar 10 16:42 icu4j-4.6.jar</span><br><span class="line">-rw-r--r-- 1 root root 8782 Mar 10 16:42 j2objc-annotations-1.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 67897 Mar 10 16:42 jackson-annotations-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 348635 Mar 10 16:42 jackson-core-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 228286 Mar 10 16:42 jackson-core-asl-1.9.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 1400944 Mar 10 16:42 jackson-databind-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 34395 Mar 10 16:42 jackson-datatype-jdk8-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 105749 Mar 10 16:42 jackson-datatype-jsr310-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 18323 Mar 10 16:42 jackson-jaxrs-1.9.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 27584 Mar 10 16:42 jackson-jaxrs-base-2.3.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 16511 Mar 10 16:42 jackson-jaxrs-json-provider-2.3.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 765648 Mar 10 16:42 jackson-mapper-asl-1.9.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 31636 Mar 10 16:42 jackson-module-jaxb-annotations-2.3.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 9327 Mar 10 16:42 jackson-module-parameter-names-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 43740 Mar 10 16:42 jackson-module-paranamer-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 475328 Mar 10 16:42 jackson-module-scala_2.11-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 27075 Mar 10 16:42 jackson-xc-1.9.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 13417 Mar 10 16:42 javacsv-2.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 84156 Mar 10 16:42 java-cup-10k.jar</span><br><span class="line">-rw-r--r-- 1 root root 749499 Mar 10 16:42 javassist-3.19.0-GA.jar</span><br><span class="line">-rw-r--r-- 1 root root 26586 Mar 10 16:42 javax.annotation-api-1.3.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 2497 Mar 10 16:42 javax.inject-1.jar</span><br><span class="line">-rw-r--r-- 1 root root 5950 Mar 10 16:42 javax.inject-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 95806 Mar 10 16:42 javax.servlet-api-3.1.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 36611 Mar 10 16:42 javax.websocket-api-1.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 161351 Mar 10 16:42 javax-websocket-client-impl-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 36872 Mar 10 16:42 javax-websocket-server-impl-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 115534 Mar 10 16:42 javax.ws.rs-api-2.0.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 105134 Mar 10 16:42 jaxb-api-2.2.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 890168 Mar 10 16:42 jaxb-impl-2.2.3-1.jar</span><br><span class="line">-rw-r--r-- 1 root root 1975408 Mar 10 16:42 jaxrs-ri-2.21.jar</span><br><span class="line">-rw-r--r-- 1 root root 18241792 Mar 10 16:42 jdk.tools-1.8.jar</span><br><span class="line">-rw-r--r-- 1 root root 16610 Mar 10 16:42 jersey-apache-client4-1.19.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 134077 Mar 10 16:42 jersey-client-1.19.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 161168 Mar 10 16:42 jersey-client-2.21.jar</span><br><span class="line">-rw-r--r-- 1 root root 694436 Mar 10 16:42 jersey-common-2.21.jar</span><br><span class="line">-rw-r--r-- 1 root root 18102 Mar 10 16:42 jersey-container-servlet-2.23.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 66276 Mar 10 16:42 jersey-container-servlet-core-2.23.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 436795 Mar 10 16:42 jersey-core-1.19.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 68523 Mar 10 16:42 jersey-entity-filtering-2.16.jar</span><br><span class="line">-rw-r--r-- 1 root root 960810 Mar 10 16:42 jersey-guava-2.21.jar</span><br><span class="line">-rw-r--r-- 1 root root 165345 Mar 10 16:42 jersey-json-1.19.jar</span><br><span class="line">-rw-r--r-- 1 root root 72763 Mar 10 16:42 jersey-media-jaxb-2.21.jar</span><br><span class="line">-rw-r--r-- 1 root root 21769 Mar 10 16:42 jersey-media-json-jackson-2.16.jar</span><br><span class="line">-rw-r--r-- 1 root root 64075 Mar 10 16:42 jersey-media-multipart-2.16.jar</span><br><span class="line">-rw-r--r-- 1 root root 705416 Mar 10 16:42 jersey-server-1.19.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 944928 Mar 10 16:42 jersey-server-2.21.jar</span><br><span class="line">-rw-r--r-- 1 root root 129319 Mar 10 16:42 jersey-servlet-1.19.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 21755 Mar 10 16:42 jersey-spring3-2.23.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 67758 Mar 10 16:42 jettison-1.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 539912 Mar 10 16:42 jetty-6.1.26.jar</span><br><span class="line">-rw-r--r-- 1 root root 78505 Mar 10 16:42 jetty-annotations-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 281919 Mar 10 16:42 jetty-client-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 16681 Mar 10 16:42 jetty-continuation-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 195948 Mar 10 16:42 jetty-http-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 135153 Mar 10 16:42 jetty-io-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 54852 Mar 10 16:42 jetty-plus-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 93072 Mar 10 16:42 jetty-security-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 591927 Mar 10 16:42 jetty-server-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 110825 Mar 10 16:42 jetty-servlet-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 93192 Mar 10 16:42 jetty-servlets-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 177131 Mar 10 16:42 jetty-util-6.1.26.jar</span><br><span class="line">-rw-r--r-- 1 root root 499219 Mar 10 16:42 jetty-util-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 128603 Mar 10 16:42 jetty-webapp-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 52044 Mar 10 16:42 jetty-xml-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 581571 Mar 10 16:42 joda-time-2.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 90660 Mar 10 16:42 json4s-ast_2.11-3.5.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 689977 Mar 10 16:42 json4s-core_2.11-3.5.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 48193 Mar 10 16:42 json4s-jackson_2.11-3.5.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 614765 Mar 10 16:42 json4s-scalap_2.11-3.5.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 100636 Mar 10 16:42 jsp-api-2.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 19936 Mar 10 16:42 jsr305-3.0.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 4596 Mar 10 16:42 jul-to-slf4j-1.7.25.jar</span><br><span class="line">-rw-r--r-- 1 root root 29779 Mar 10 16:42 LatencyUtils-2.0.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 1045744 Mar 10 16:42 leveldbjni-all-1.8.jar</span><br><span class="line">-rw-r--r-- 1 root root 280950 Mar 10 16:42 linkis-common-0.9.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 10698 Mar 10 16:42 linkis-hadoop-common-0.9.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 120891 Mar 10 16:42 linkis-httpclient-0.9.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 199384 Mar 10 16:42 linkis-module-0.9.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 248096 Mar 10 16:42 linkis-protocol-0.9.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 415357 Mar 10 16:42 linkis-storage-0.9.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 489884 Mar 10 16:42 log4j-1.2.17.jar</span><br><span class="line">-rw-r--r-- 1 root root 255485 Mar 10 16:42 log4j-api-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 1597622 Mar 10 16:42 log4j-core-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 23996 Mar 10 16:42 log4j-jul-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 24173 Mar 10 16:42 log4j-slf4j-impl-2.10.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 388864 Mar 10 16:42 mail-1.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 352776 Mar 10 16:42 micrometer-core-1.0.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 62135 Mar 10 16:42 mimepull-1.9.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 1006904 Mar 10 16:42 mysql-connector-java-5.1.49.jar</span><br><span class="line">-rw-r--r-- 1 root root 6690 Mar 10 16:42 netflix-commons-util-0.1.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 69073 Mar 10 16:42 netflix-eventbus-0.3.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 101813 Mar 10 16:42 netflix-infix-0.3.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 13772 Mar 10 16:42 netflix-statistics-0.1.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 1199572 Mar 10 16:42 netty-3.6.2.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 3780056 Mar 10 16:42 netty-all-4.1.17.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 167898 Mar 10 16:42 netty-buffer-4.0.27.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 143916 Mar 10 16:42 netty-codec-4.0.27.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 426622 Mar 10 16:42 netty-codec-http-4.0.27.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 420854 Mar 10 16:42 netty-common-4.0.27.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 238957 Mar 10 16:42 netty-handler-4.0.27.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 316806 Mar 10 16:42 netty-transport-4.0.27.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 96651 Mar 10 16:42 netty-transport-native-epoll-4.0.27.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 15555629 Mar 10 16:42 ooxml-schemas-1.3.jar</span><br><span class="line">-rw-r--r-- 1 root root 657585 Mar 10 16:42 org.eclipse.wst.xml.xpath2.processor-2.1.100.jar</span><br><span class="line">-rw-r--r-- 1 root root 20235 Mar 10 16:42 osgi-resource-locator-1.0.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 34654 Mar 10 16:42 paranamer-2.8.jar</span><br><span class="line">-rw-r--r-- 1 root root 2701171 Mar 10 16:42 poi-3.17.jar</span><br><span class="line">-rw-r--r-- 1 root root 1479023 Mar 10 16:42 poi-ooxml-3.17.jar</span><br><span class="line">-rw-r--r-- 1 root root 5924600 Mar 10 16:42 poi-ooxml-schemas-3.17.jar</span><br><span class="line">-rw-r--r-- 1 root root 533455 Mar 10 16:42 protobuf-java-2.5.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 129763 Mar 10 16:42 reflections-0.9.10.jar</span><br><span class="line">-rw-r--r-- 1 root root 86725 Mar 10 16:42 ribbon-2.2.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 83270 Mar 10 16:42 ribbon-core-2.2.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 17688 Mar 10 16:42 ribbon-eureka-2.2.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 54587 Mar 10 16:42 ribbon-httpclient-2.2.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 161793 Mar 10 16:42 ribbon-loadbalancer-2.2.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 40769 Mar 10 16:42 ribbon-transport-2.2.5.jar</span><br><span class="line">-rw-r--r-- 1 root root 1127547 Mar 10 16:42 rxjava-1.2.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 402675 Mar 10 16:42 rxnetty-0.4.9.jar</span><br><span class="line">-rw-r--r-- 1 root root 53987 Mar 10 16:42 rxnetty-contexts-0.4.9.jar</span><br><span class="line">-rw-r--r-- 1 root root 29155 Mar 10 16:42 rxnetty-servo-0.4.9.jar</span><br><span class="line">-rw-r--r-- 1 root root 15487351 Mar 10 16:42 scala-compiler-2.11.8.jar</span><br><span class="line">-rw-r--r-- 1 root root 5744974 Mar 10 16:43 scala-library-2.11.8.jar</span><br><span class="line">-rw-r--r-- 1 root root 802818 Mar 10 16:43 scalap-2.11.8.jar</span><br><span class="line">-rw-r--r-- 1 root root 423753 Mar 10 16:43 scala-parser-combinators_2.11-1.0.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 4573750 Mar 10 16:43 scala-reflect-2.11.8.jar</span><br><span class="line">-rw-r--r-- 1 root root 647891 Mar 10 16:43 scala-xml_2.11-1.0.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 224619 Mar 10 16:43 servo-core-0.10.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 174154 Mar 10 16:43 servo-internal-0.10.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 32127 Mar 10 16:43 slf4j-api-1.7.12.jar</span><br><span class="line">-rw-r--r-- 1 root root 297518 Mar 10 16:43 snakeyaml-1.19.jar</span><br><span class="line">-rw-r--r-- 1 root root 366340 Mar 10 16:43 spring-aop-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 660545 Mar 10 16:43 spring-beans-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 930680 Mar 10 16:43 spring-boot-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 461472 Mar 10 16:43 spring-boot-actuator-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 379277 Mar 10 16:43 spring-boot-actuator-autoconfigure-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 1162436 Mar 10 16:43 spring-boot-autoconfigure-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 592 Mar 10 16:43 spring-boot-starter-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 612 Mar 10 16:43 spring-boot-starter-actuator-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 599 Mar 10 16:43 spring-boot-starter-aop-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 590 Mar 10 16:43 spring-boot-starter-jetty-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 645 Mar 10 16:43 spring-boot-starter-json-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 596 Mar 10 16:43 spring-boot-starter-log4j2-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 588 Mar 10 16:43 spring-boot-starter-web-2.0.3.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 13138 Mar 10 16:43 spring-bridge-2.4.0-b34.jar</span><br><span class="line">-rw-r--r-- 1 root root 128382 Mar 10 16:43 spring-cloud-commons-2.0.0.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 35195 Mar 10 16:43 spring-cloud-config-client-2.0.0.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 104246 Mar 10 16:43 spring-cloud-context-2.0.0.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 429798 Mar 10 16:43 spring-cloud-netflix-core-1.4.4.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 93697 Mar 10 16:43 spring-cloud-netflix-eureka-client-1.4.4.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 2227 Mar 10 16:43 spring-cloud-starter-2.0.0.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 2315 Mar 10 16:43 spring-cloud-starter-config-2.0.0.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 3870 Mar 10 16:43 spring-cloud-starter-eureka-1.4.4.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 2429 Mar 10 16:43 spring-cloud-starter-netflix-archaius-1.4.4.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 2491 Mar 10 16:43 spring-cloud-starter-netflix-eureka-client-1.4.4.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 2359 Mar 10 16:43 spring-cloud-starter-netflix-ribbon-1.4.4.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 1090739 Mar 10 16:43 spring-context-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 178021 Mar 10 16:43 spring-context-support-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 1226075 Mar 10 16:43 spring-core-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 280032 Mar 10 16:43 spring-expression-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 21703 Mar 10 16:43 spring-jcl-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 72378 Mar 10 16:43 spring-security-crypto-5.0.6.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 19745 Mar 10 16:43 spring-security-rsa-1.0.5.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 1254656 Mar 10 16:43 spring-web-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 789866 Mar 10 16:43 spring-webmvc-5.0.7.RELEASE.jar</span><br><span class="line">-rw-r--r-- 1 root root 161867 Mar 10 16:43 stax2-api-3.1.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 26514 Mar 10 16:43 stax-api-1.0.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 23346 Mar 10 16:43 stax-api-1.0-2.jar</span><br><span class="line">-rw-r--r-- 1 root root 148627 Mar 10 16:43 stringtemplate-3.2.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 17506 Mar 10 16:43 tiger-types-1.4.jar</span><br><span class="line">-rw-r--r-- 1 root root 63777 Mar 10 16:43 validation-api-1.1.0.Final.jar</span><br><span class="line">-rw-r--r-- 1 root root 42354 Mar 10 16:43 websocket-api-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 35643 Mar 10 16:43 websocket-client-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 196553 Mar 10 16:43 websocket-common-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 35307 Mar 10 16:43 websocket-server-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 21327 Mar 10 16:43 websocket-servlet-9.4.11.v20180605.jar</span><br><span class="line">-rw-r--r-- 1 root root 486013 Mar 10 16:43 woodstox-core-asl-4.4.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 2009630 Mar 10 16:43 xerces2-xsd11-2.11.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 1229125 Mar 10 16:43 xercesImpl-2.9.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 51073 Mar 10 16:43 xlsx-streamer-1.2.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 220536 Mar 10 16:43 xml-apis-1.4.01.jar</span><br><span class="line">-rw-r--r-- 1 root root 2666695 Mar 10 16:43 xmlbeans-2.3.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 15010 Mar 10 16:43 xmlenc-0.52.jar</span><br><span class="line">-rw-r--r-- 1 root root 7188 Mar 10 16:43 xmlpull-1.1.3.1.jar</span><br><span class="line">-rw-r--r-- 1 root root 84091 Mar 10 16:43 xml-resolver-1.2.jar</span><br><span class="line">-rw-r--r-- 1 root root 24956 Mar 10 16:43 xpp3_min-1.1.4c.jar</span><br><span class="line">-rw-r--r-- 1 root root 589803 Mar 10 16:43 xstream-1.4.10.jar</span><br><span class="line">-rw-r--r-- 1 root root 94672 Mar 10 16:43 xz-1.0.jar</span><br><span class="line">-rw-r--r-- 1 root root 792964 Mar 10 16:43 zookeeper-3.4.6.jar</span><br></pre></td></tr></table></figure>

<p>重启DSS服务</p>
<h5 id="修改网关端口为9002后需确认"><a href="#修改网关端口为9002后需确认" class="headerlink" title="修改网关端口为9002后需确认"></a>修改网关端口为9002后需确认</h5><p>网关修改后需要确认dss_application表中各组件配置的端口号，要与配置信息一致。<br><img src="/2021/03/17/%E5%BE%AE%E4%BC%97%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/image2021-3-16_14-2-59.png" alt></p>
<h4 id="微众平台文档连接"><a href="#微众平台文档连接" class="headerlink" title="微众平台文档连接"></a>微众平台文档连接</h4><h5 id="DSS-github-地址"><a href="#DSS-github-地址" class="headerlink" title="DSS github 地址"></a>DSS github 地址</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio</a></p>
<h5 id="Linkis-github-地址"><a href="#Linkis-github-地址" class="headerlink" title="Linkis github 地址"></a>Linkis github 地址</h5><p><a href="https://github.com/WeBankFinTech/Linkis" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Linkis</a></p>
<h5 id="Scriptis-github-地址"><a href="#Scriptis-github-地址" class="headerlink" title="Scriptis github 地址"></a>Scriptis github 地址</h5><p><a href="https://github.com/WeBankFinTech/Scriptis" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Scriptis</a></p>
<h5 id="Visualis-github-地址"><a href="#Visualis-github-地址" class="headerlink" title="Visualis github 地址"></a>Visualis github 地址</h5><p><a href="https://github.com/WeBankFinTech/Visualis" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Visualis</a></p>
<h5 id="Qualitis-github-地址"><a href="#Qualitis-github-地址" class="headerlink" title="Qualitis github 地址"></a>Qualitis github 地址</h5><p><a href="https://github.com/WeBankFinTech/Qualitis" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Qualitis</a></p>
<h5 id="Azkaban-github-地址"><a href="#Azkaban-github-地址" class="headerlink" title="Azkaban github 地址"></a>Azkaban github 地址</h5><p><a href="https://azkaban.github.io/" target="_blank" rel="noopener">https://azkaban.github.io/</a></p>
<h5 id="部署文档（仅供参考）"><a href="#部署文档（仅供参考）" class="headerlink" title="部署文档（仅供参考）"></a>部署文档（仅供参考）</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch2/DSS_LINKIS_Quick_Install.md" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch2/DSS_LINKIS_Quick_Install.md</a></p>
<h5 id="DSS-用户使用手册"><a href="#DSS-用户使用手册" class="headerlink" title="DSS 用户使用手册"></a>DSS 用户使用手册</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio/blob/b7ed704228a86b9235936f84f119cb33907e5f5f/docs/zh_CN/ch3/DSS_User_Manual.md" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio/blob/b7ed704228a86b9235936f84f119cb33907e5f5f/docs/zh_CN/ch3/DSS_User_Manual.md</a></p>
<h5 id="DSS-快速使用手册"><a href="#DSS-快速使用手册" class="headerlink" title="DSS 快速使用手册"></a>DSS 快速使用手册</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio/blob/b7ed704228a86b9235936f84f119cb33907e5f5f/docs/zh_CN/ch3/DataSphere_Studio_QuickStart.md" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio/blob/b7ed704228a86b9235936f84f119cb33907e5f5f/docs/zh_CN/ch3/DataSphere_Studio_QuickStart.md</a></p>
<h5 id="Scriptis-SparkSQL-demo"><a href="#Scriptis-SparkSQL-demo" class="headerlink" title="Scriptis SparkSQL demo"></a>Scriptis SparkSQL demo</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio/blob/b7ed704228a86b9235936f84f119cb33907e5f5f/docs/zh_CN/ch3/DSS_User_Tests3_SparkSQL.md" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio/blob/b7ed704228a86b9235936f84f119cb33907e5f5f/docs/zh_CN/ch3/DSS_User_Tests3_SparkSQL.md</a></p>
<h5 id="DSS-接入第三方系统指南"><a href="#DSS-接入第三方系统指南" class="headerlink" title="DSS 接入第三方系统指南"></a>DSS 接入第三方系统指南</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch4/第三方系统接入DSS指南.md" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch4/%E7%AC%AC%E4%B8%89%E6%96%B9%E7%B3%BB%E7%BB%9F%E6%8E%A5%E5%85%A5DSS%E6%8C%87%E5%8D%97.md</a></p>
<h5 id="DSS-手动接入Azkaban-说明文档"><a href="#DSS-手动接入Azkaban-说明文档" class="headerlink" title="DSS 手动接入Azkaban 说明文档"></a>DSS 手动接入Azkaban 说明文档</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio/blob/b7ed704228a86b9235936f84f119cb33907e5f5f/docs/zh_CN/ch4/如何接入调度系统Azkaban.md" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio/blob/b7ed704228a86b9235936f84f119cb33907e5f5f/docs/zh_CN/ch4/%E5%A6%82%E4%BD%95%E6%8E%A5%E5%85%A5%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9FAzkaban.md</a></p>
<h5 id="JobType-Azkaban-说明文档"><a href="#JobType-Azkaban-说明文档" class="headerlink" title="JobType Azkaban 说明文档"></a>JobType Azkaban 说明文档</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch2/Azkaban_LinkisJobType_Deployment_Manual.md" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch2/Azkaban_LinkisJobType_Deployment_Manual.md</a></p>
<h5 id="DSS-安装常见问题（官方提供）"><a href="#DSS-安装常见问题（官方提供）" class="headerlink" title="DSS 安装常见问题（官方提供）"></a>DSS 安装常见问题（官方提供）</h5><p><a href="https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch1/DSS安装常见问题列表.md" target="_blank" rel="noopener">https://github.com/WeBankFinTech/DataSphereStudio/blob/master/docs/zh_CN/ch1/DSS%E5%AE%89%E8%A3%85%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%88%97%E8%A1%A8.md</a></p>
<h5 id="Linkis-安装常见问题（官方提供）"><a href="#Linkis-安装常见问题（官方提供）" class="headerlink" title="Linkis 安装常见问题（官方提供）"></a>Linkis 安装常见问题（官方提供）</h5><p><a href="https://github.com/WeBankFinTech/Linkis/wiki/Q&A-问题总结" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Linkis/wiki/Q&amp;A-%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93</a></p>
<h5 id="官方Q-amp-A-总结"><a href="#官方Q-amp-A-总结" class="headerlink" title="官方Q&amp;A 总结"></a>官方Q&amp;A 总结</h5><p><a href="https://github.com/WeBankFinTech/Linkis/wiki/Q&A-问题总结" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Linkis/wiki/Q&amp;A-%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93</a></p>
]]></content>
      <categories>
        <category>Webank</category>
      </categories>
      <tags>
        <tag>Webank</tag>
      </tags>
  </entry>
</search>
